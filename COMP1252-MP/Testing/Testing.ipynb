{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Essential imports\n",
    "    import re\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import glob\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import logging\n",
    "    import rasterio\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import geopandas as gpd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    from datetime import datetime\n",
    "    from pathlib import Path\n",
    "    from scipy import stats\n",
    "    from typing import Dict, Tuple, Optional\n",
    "    from collections import defaultdict\n",
    "    from rasterio.mask import mask\n",
    "    from shapely.ops import unary_union\n",
    "    from shapely.wkt import dumps, loads\n",
    "    from shapely.geometry import mapping, box, Polygon, MultiPolygon\n",
    "    from rasterio.windows import from_bounds\n",
    "    from s2cloudless import S2PixelCloudDetector\n",
    "    from tqdm import tqdm\n",
    "    from tqdm.notebook import tqdm\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project directory structure\n",
    "project_dir = Path(\"../Solutions/Land_Change_Monitoring\")\n",
    "subdirs = [\n",
    "    \"../Datasets/Sentinel-2/\",           # Original GeoJSON and Sentinel-2 data\n",
    "    \"../Datasets/Testing/Processed\",     # Processed and grouped events\n",
    "    \"../Datasets/Testing/Samples\",       # Our sampled datasets\n",
    "    \"../Datasets/Testing/Tiles\",         # Generated image tiles\n",
    "    \"../Docs/Diagrams\",                             # Results and visualizations\n",
    "    \"../Models\",                                    # Trained models\n",
    "    \"../Docs/Logs\"                                  # Processing logs\n",
    "]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for subdir in subdirs:\n",
    "    Path(subdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=Path(\"../Docs/Logs/processing.log\"),\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Print created directory structure for verification\n",
    "print(\"Created directory structure:\")\n",
    "for subdir in subdirs:\n",
    "    if Path(subdir).exists():\n",
    "        print(f\"✓ {subdir}\")\n",
    "    else:\n",
    "        print(f\"✗ {subdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check if running in Google Colab\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(\"GPU not available in Colab, consider enabling a GPU runtime.\")\n",
    "# Running on a local machine\n",
    "else:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        print(f\"Is Apple MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n",
    "        print(f\"Is Apple MPS available? {torch.backends.mps.is_available()}\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "\n",
    "# TODO: Add support for AMD ROCm GPU if needed\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the GeoJSON file\n",
    "base_path = Path('../Datasets/Sentinel-2')\n",
    "geojson_path = Path('../Datasets/BoundingBox/deforestation.geojson')\n",
    "safe_dirs = list(base_path.glob(\"*/*.SAFE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GeoJSON file\n",
    "gdf = gpd.read_file(geojson_path)\n",
    "\n",
    "# Display GeoJSON information\n",
    "print(\"GeoJSON Information:\", len(gdf))\n",
    "print(\"-\" * 50)\n",
    "print(gdf.info())\n",
    "print(\"\\nFirst few records:\")\n",
    "print(gdf.head())\n",
    "\n",
    "# Get the bounding box coordinates\n",
    "bbox = gdf.total_bounds\n",
    "print(\"\\nBounding Box (minx, miny, maxx, maxy):\")\n",
    "print(bbox)\n",
    "\n",
    "# Display basic information about the GeoJSON\n",
    "print(\"\\nGeoJSON CRS:\", gdf.crs)\n",
    "print(\"Number of features:\", len(gdf))\n",
    "print(\"Columns:\", gdf.columns.tolist())\n",
    "print(\"First geometry type:\", gdf.geometry.iloc[0].geom_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'img_date' is in datetime format if it exists\n",
    "if 'img_date' in gdf.columns:\n",
    "    gdf['img_date'] = pd.to_datetime(gdf['img_date'], errors='coerce')\n",
    "\n",
    "    # Drop rows with invalid dates if any\n",
    "    gdf = gdf.dropna(subset=['img_date'])\n",
    "\n",
    "    # Get unique dates and sort them\n",
    "    unique_dates = gdf['img_date'].dt.date.unique()\n",
    "    unique_dates.sort()\n",
    "\n",
    "    date_counts = gdf['img_date'].dt.date.value_counts().sort_index()\n",
    "    print(\"Occurrences of each 'img_date':\")\n",
    "    print(date_counts)\n",
    "else:\n",
    "    print(\"'img_date' column not found in the GeoDataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reduce geometry precision\n",
    "def reduce_precision(geometry, decimal_places=5):\n",
    "    return loads(dumps(geometry, rounding_precision=decimal_places))\n",
    "\n",
    "# Apply precision reduction to all geometries\n",
    "gdf['geometry'] = gdf['geometry'].apply(lambda geom: reduce_precision(geom))\n",
    "\n",
    "# Now check for duplicates again\n",
    "duplicate_geometries = gdf[gdf.geometry.duplicated(keep=False)]\n",
    "print(\"Duplicate geometries after reducing precision:\")\n",
    "print(duplicate_geometries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on geometry\n",
    "# gdf = gdf.drop_duplicates(subset='geometry')\n",
    "\n",
    "# Save the cleaned GeoDataFrame to a new GeoJSON file\n",
    "# gdf.to_file(\"deforestation_unique.geojson\", driver='GeoJSON')\n",
    "\n",
    "print(f\"Number of geometries present in gdf: {len(gdf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the geospatial data\n",
    "gdf.plot()\n",
    "plt.title('Deforestation Areas (Ukraine)')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal distribution\n",
    "gdf['year'] = gdf['img_date'].dt.year\n",
    "gdf['month'] = gdf['img_date'].dt.month\n",
    "\n",
    "# Create year-month summary\n",
    "temporal_dist = gdf.groupby(['year', 'month']).size().unstack(fill_value=0)\n",
    "print(\"Deforestation events by year and month:\")\n",
    "print(temporal_dist)\n",
    "\n",
    "# Distribution by tile\n",
    "print(\"\\nDeforestation events by tile:\")\n",
    "print(gdf['tile'].value_counts())\n",
    "\n",
    "# Create a monthly summary plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "temporal_dist.T.plot(kind='bar', stacked=True)\n",
    "plt.title('Deforestation Events by Month and Year')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Events')\n",
    "plt.legend(title='Year')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate area of each polygon in square meters\n",
    "# Converting to UTM projection for accurate area and perimeter calculation\n",
    "gdf['area'] = gdf.geometry.to_crs({'proj':'utm', 'zone':36, 'ellps':'WGS84'}).area\n",
    "gdf['perimeter'] = gdf.geometry.to_crs({'proj':'utm', 'zone':36, 'ellps':'WGS84'}).length\n",
    "\n",
    "# Basic statistics of polygon sizes\n",
    "print(\"Polygon area statistics (square meters):\")\n",
    "print(gdf['area'].describe())\n",
    "\n",
    "# Create histogram of polygon sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(gdf['area'], bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Deforestation Polygon Sizes')\n",
    "plt.xlabel('Area (square meters)')\n",
    "plt.ylabel('Count')\n",
    "plt.yscale('log')  # Using log scale for better visualization\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate basic shape metrics\n",
    "gdf['complexity'] = gdf['perimeter'] / (4 * np.sqrt(gdf['area']))\n",
    "\n",
    "print(\"Shape complexity statistics (1.0 = perfect square):\")\n",
    "print(gdf['complexity'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject to UTM Zone 36N\n",
    "deforestation_data_utm = gdf.to_crs(epsg=32636)\n",
    "\n",
    "# Recalculate statistics in UTM projection\n",
    "polygon_areas = deforestation_data_utm.geometry.area  # now in square meters\n",
    "polygon_bounds = deforestation_data_utm.geometry.bounds\n",
    "\n",
    "# Calculate statistics\n",
    "max_width = (polygon_bounds.maxx - polygon_bounds.minx).max()\n",
    "max_height = (polygon_bounds.maxy - polygon_bounds.miny).max()\n",
    "mean_area = polygon_areas.mean()\n",
    "median_area = polygon_areas.median()\n",
    "\n",
    "print(f\"Polygon Statistics:\")\n",
    "print(f\"Mean area: {mean_area/10000:.2f} hectares\")\n",
    "print(f\"Median area: {median_area/10000:.2f} hectares\")\n",
    "print(f\"Max width: {max_width:.2f} meters\")\n",
    "print(f\"Max height: {max_height:.2f} meters\")\n",
    "\n",
    "# Calculate optimal mesh size based on polygon distribution\n",
    "hist, bins = np.histogram(polygon_areas, bins=20)\n",
    "print(\"\\nArea Distribution (hectares):\")\n",
    "for i in range(len(hist)):\n",
    "    if hist[i] > 0:\n",
    "        print(f\"{bins[i]/10000:.2f} - {bins[i+1]/10000:.2f}: {hist[i]} polygons\")\n",
    "\n",
    "# Additional useful statistics\n",
    "print(\"\\nAdditional Statistics:\")\n",
    "print(f\"Total number of polygons: {len(deforestation_data_utm)}\")\n",
    "print(f\"Total area of deforestation: {polygon_areas.sum()/10000:.2f} hectares\")\n",
    "print(f\"95th percentile area: {np.percentile(polygon_areas, 95)/10000:.2f} hectares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_jp2_files(base_dir):\n",
    "    jp2_files = []  # Create empty list to store paths\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jp2\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                jp2_files.append(full_path)\n",
    "    return jp2_files\n",
    "\n",
    "\n",
    "jp2_files = collect_jp2_files(base_path)\n",
    "\n",
    "# Print count\n",
    "print(f\"Total files found: {len(jp2_files)}\\n\")\n",
    "\n",
    "# Print all paths\n",
    "print(\"Found .jp2 files:\")\n",
    "for path in jp2_files:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_parser(file_path):\n",
    "    # Extract filename and directory structure\n",
    "    path_parts = file_path.split(os.sep)\n",
    "    \n",
    "    # Get product name from the first directory in the path containing S2\n",
    "    product_name = next(part for part in path_parts if part.startswith('S2'))\n",
    "    \n",
    "    # Get band information from the filename (last part)\n",
    "    filename = path_parts[-1]\n",
    "    band = filename.split('_')[-1].replace('.jp2', '').replace('B', '')\n",
    "    \n",
    "    # Split the product name into components\n",
    "    parts = product_name.split('_')\n",
    "    \n",
    "    details = {\n",
    "        'mission': parts[0],           # S2A or S2B satellite\n",
    "        'product_level': parts[1],     # Processing level (MSIL1C)\n",
    "        'sensing_date': parts[2][:8],  # YYYYMMDD\n",
    "        'sensing_time': parts[2][9:],  # HHMMSS\n",
    "        'processing_number': parts[3],  # Processing baseline number\n",
    "        'orbit_number': parts[4],      # Relative orbit number\n",
    "        'tile_number': parts[5][1:],   # Tile identifier\n",
    "        'product_date': parts[6][:8],  # Product generation date\n",
    "        'product_time': parts[6][9:],  # Product generation time\n",
    "        'band': band,                  # Band number\n",
    "        'file_path': file_path           # Full file path\n",
    "    }\n",
    "    \n",
    "    # Pretty print the results\n",
    "    print(f\"Mission           : {details['mission']}\")\n",
    "    print(f\"Product Level     : {details['product_level']}\")\n",
    "    print(f\"Sensing Date      : {details['sensing_date']}\")\n",
    "    print(f\"Sensing Time      : {details['sensing_time']}\")\n",
    "    print(f\"Processing Number : {details['processing_number']}\")\n",
    "    print(f\"Orbit Number      : {details['orbit_number']}\")\n",
    "    print(f\"Tile Number       : {details['tile_number']}\")\n",
    "    print(f\"Product Date      : {details['product_date']}\")\n",
    "    print(f\"Product Time      : {details['product_time']}\")\n",
    "    print(f\"Band Number       : {details['band']}\")\n",
    "    print(f\"File Path         : {details['file_path']}\")\n",
    "\n",
    "    return details\n",
    "\n",
    "# Example usage:\n",
    "file_path = jp2_files[70]\n",
    "result = data_parser(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentinel_structure(base_path):\n",
    "    \"\"\"\n",
    "    Analyzes Sentinel-2 dataset structure and returns key information\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): Path to the Sentinel-2 dataset directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing image metadata\n",
    "    \"\"\"\n",
    "    # Initialize lists to store metadata\n",
    "    metadata = []\n",
    "    \n",
    "    # Convert to Path object\n",
    "    base = Path(base_path)\n",
    "    # Pattern for date extraction\n",
    "    date_pattern = r'(\\d{8}T\\d{6})'\n",
    "    \n",
    "    # Iterate through all .SAFE directories\n",
    "    for safe_dir in base.glob('*/*.SAFE'):\n",
    "        # Extract metadata from directory name\n",
    "        dir_name = safe_dir.parent.name\n",
    "        \n",
    "        # Extract date using regex\n",
    "        date_match = re.search(date_pattern, dir_name)\n",
    "        if date_match:\n",
    "            acquisition_date = datetime.strptime(date_match.group(1), '%Y%m%dT%H%M%S')\n",
    "        else:\n",
    "            acquisition_date = None\n",
    "            \n",
    "        # Get satellite (S2A or S2B)\n",
    "        satellite = dir_name[:3]\n",
    "        \n",
    "        # Get tile ID\n",
    "        tile_match = re.search(r'T(\\d{2}[A-Z]{3})', dir_name)\n",
    "        tile_id = tile_match.group(1) if tile_match else None\n",
    "        \n",
    "        # Count number of bands\n",
    "        bands = list(safe_dir.glob('GRANULE/*/IMG_DATA/*.jp2'))\n",
    "        num_bands = len([b for b in bands if not b.name.endswith('TCI.jp2')])\n",
    "        \n",
    "        metadata.append({\n",
    "            'satellite': satellite,\n",
    "            'acquisition_date': acquisition_date,\n",
    "            'tile_id': tile_id,\n",
    "            'num_bands': num_bands,\n",
    "            'path': safe_dir\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(metadata)\n",
    "    df = df.sort_values('acquisition_date')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Usage example:\n",
    "df = analyze_sentinel_structure(base_path)\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"Dataset Summary:\")\n",
    "print(f\"Total number of images: {len(df)}\")\n",
    "print(\"\\nAcquisitions by satellite:\")\n",
    "print(df['satellite'].value_counts())\n",
    "print(\"\\nDate range:\")\n",
    "print(f\"First acquisition: {df['acquisition_date'].min()}\")\n",
    "print(f\"Last acquisition: {df['acquisition_date'].max()}\")\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_band_statistics(image_path):\n",
    "    \"\"\"Get statistics for specific bands in an image\"\"\"\n",
    "    band_paths = Path(image_path).glob('GRANULE/*/IMG_DATA/*.jp2')\n",
    "    bands = {}\n",
    "    for band_path in band_paths:\n",
    "        band_name = re.search(r'B\\d{2}|B8A', band_path.name)\n",
    "        if band_name:\n",
    "            bands[band_name.group(0)] = str(band_path)\n",
    "    return dict(sorted(bands.items()))\n",
    "\n",
    "def get_quality_masks(image_path):\n",
    "    \"\"\"Get list of quality masks for an image\"\"\"\n",
    "    mask_paths = Path(image_path).glob('GRANULE/*/QI_DATA/*.gml')\n",
    "    return [p.name for p in mask_paths]\n",
    "\n",
    "# Example usage:\n",
    "image_path = df.iloc[0]['path']\n",
    "print(\"Band files in first image:\")\n",
    "for band, path in get_band_statistics(image_path).items():\n",
    "    print(f\"{band}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first image directory to analyze bands\n",
    "first_image = safe_dirs[0]\n",
    "img_data_path = list((first_image / \"GRANULE\").glob(\"*\"))[0] / \"IMG_DATA\"\n",
    "band_files = list(img_data_path.glob(\"*.jp2\"))\n",
    "\n",
    "# Extract band information\n",
    "band_info = []\n",
    "for band_file in band_files:\n",
    "    band_name = band_file.name.split('_')[-1].split('.')[0]\n",
    "    \n",
    "    # Open the band file to get metadata\n",
    "    with rasterio.open(band_file) as src:\n",
    "        band_info.append({\n",
    "            'band': band_name,\n",
    "            'width': src.width,\n",
    "            'height': src.height,\n",
    "            'dtype': src.dtypes[0],\n",
    "            'resolution': src.res[0]  # pixel size in meters\n",
    "        })\n",
    "\n",
    "# Create DataFrame with band information\n",
    "df_bands = pd.DataFrame(band_info)\n",
    "print(\"Band Information:\")\n",
    "print(\"-\" * 50)\n",
    "print(df_bands.sort_values('band'))\n",
    "\n",
    "# Count number of files per image\n",
    "print(\"\\nNumber of files per image:\")\n",
    "print(len(band_files))\n",
    "\n",
    "# Print list of unique bands\n",
    "print(\"\\nAvailable bands:\")\n",
    "unique_bands = sorted(list(df_bands['band'].unique()))\n",
    "print(unique_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first key\n",
    "example = jp2_files[9]\n",
    "\n",
    "def visualize_band(jp2_path):\n",
    "    with rasterio.open(jp2_path) as src:\n",
    "        band = src.read(1)  # Read the first band\n",
    "        plt.imshow(band, cmap=\"gray\")\n",
    "        plt.title(jp2_path)\n",
    "        plt.show()\n",
    "\n",
    "visualize_band(example)\n",
    "\n",
    "# Open the .jp2 file\n",
    "with rasterio.open(example) as dataset:\n",
    "    # Read the dataset's data as a numpy array\n",
    "    band_data = dataset.read(1)\n",
    "    # Access metadata\n",
    "    metadata = dataset.meta\n",
    "\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store metadata\n",
    "metadata_list = []\n",
    "\n",
    "for safe_dir in safe_dirs:\n",
    "    # Parse directory name\n",
    "    dir_parts = safe_dir.name.split('_')\n",
    "    \n",
    "    metadata = {\n",
    "        'satellite': dir_parts[0],  # S2A or S2B\n",
    "        'processing_level': dir_parts[1],  # MSIL1C\n",
    "        'timestamp': datetime.strptime(dir_parts[2], '%Y%m%dT%H%M%S'),\n",
    "        'relative_orbit': dir_parts[4],  # R064\n",
    "        'tile_id': dir_parts[5],  # T36UYA\n",
    "        'path': safe_dir\n",
    "    }\n",
    "    metadata_list.append(metadata)\n",
    "\n",
    "# Create DataFrame\n",
    "df_metadata = pd.DataFrame(metadata_list)\n",
    "\n",
    "# Basic analysis\n",
    "print(\"Dataset Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Date range: {df_metadata['timestamp'].min()} to {df_metadata['timestamp'].max()}\")\n",
    "print(f\"Number of unique tiles: {df_metadata['tile_id'].nunique()}\")\n",
    "print(f\"Number of satellites: {df_metadata['satellite'].nunique()}\")\n",
    "print(\"\\nSatellite distribution:\")\n",
    "print(df_metadata['satellite'].value_counts())\n",
    "print(\"\\nTile distribution:\")\n",
    "print(df_metadata['tile_id'].value_counts())\n",
    "\n",
    "# Visualize temporal distribution\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.hist(df_metadata['timestamp'], bins=20, edgecolor='black')\n",
    "plt.title('Temporal Distribution of Satellite Images')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Monthly distribution\n",
    "df_metadata['month'] = df_metadata['timestamp'].dt.month\n",
    "monthly_counts = df_metadata['month'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "monthly_counts.plot(kind='bar')\n",
    "plt.title('Monthly Distribution of Images')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size calculation with confidence level\n",
    "def calculate_strata_sizes(total_events=480, confidence_level=0.95, margin_error=0.1):\n",
    "    z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "    \n",
    "    # Define strata bounds and proportions\n",
    "    strata_bounds = [0, 3.37, 10.10, np.inf]  # hectares\n",
    "    strata_props = [0.8, 0.15, 0.05]  # given proportions\n",
    "    \n",
    "    # Calculate sample sizes for each stratum\n",
    "    sample_sizes = []\n",
    "    for prop in strata_props:\n",
    "        stratum_n = int(np.ceil((z_score**2 * prop * (1-prop)) / margin_error**2))\n",
    "        sample_sizes.append(min(stratum_n, int(total_events * prop)))\n",
    "    \n",
    "    return sample_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_deforestation_events(gdf, spatial_thresh, temporal_thresh):\n",
    "    # Create time-based groups\n",
    "    gdf['temporal_group'] = pd.to_datetime(gdf['date']).dt.to_period('D')\n",
    "    temporal_groups = gdf.groupby(pd.Grouper(key='temporal_group', freq=f'{temporal_thresh}D'))\n",
    "    \n",
    "    grouped_events = []\n",
    "    for _, time_group in temporal_groups:\n",
    "        if len(time_group) > 0:\n",
    "            # Buffer and merge nearby polygons\n",
    "            buffered = time_group.geometry.buffer(spatial_thresh)\n",
    "            merged = unary_union(buffered)\n",
    "            grouped_events.append(merged)\n",
    "            \n",
    "    return gpd.GeoDataFrame(geometry=grouped_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Define temporal window\n",
    "temporal_window = [\n",
    "    datetime.datetime(2018, 7, 15),\n",
    "    datetime.datetime(2018, 9, 15)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_size_distribution(gdf):\n",
    "    \"\"\"Analyze the size distribution of events before sampling\"\"\"\n",
    "    size_stats = {\n",
    "        'small': len(gdf[gdf['area_ha'] <= 3.37]),\n",
    "        'medium': len(gdf[(gdf['area_ha'] > 3.37) & (gdf['area_ha'] <= 10.10)]),\n",
    "        'large': len(gdf[gdf['area_ha'] > 10.10])\n",
    "    }\n",
    "    return size_stats\n",
    "\n",
    "def sample_deforestation_events(geojson_path, output_dir, temporal_window):\n",
    "    # Read and initial processing\n",
    "    gdf = gpd.read_file(geojson_path)\n",
    "    \n",
    "    # Log initial dataset size\n",
    "    logging.info(f\"Total events in dataset: {len(gdf)}\")\n",
    "    \n",
    "    # Convert dates and filter for our temporal window\n",
    "    gdf['img_date'] = pd.to_datetime(gdf['img_date'])\n",
    "    mask = (gdf['img_date'] >= temporal_window[0]) & (gdf['img_date'] <= temporal_window[1])\n",
    "    filtered_gdf = gdf[mask].copy()\n",
    "    \n",
    "    logging.info(f\"Events in temporal window: {len(filtered_gdf)}\")\n",
    "    \n",
    "    # Project to UTM and calculate area in hectares\n",
    "    filtered_gdf = filtered_gdf.to_crs('EPSG:32736')  # UTM zone 36N for Ukraine\n",
    "    filtered_gdf['area_ha'] = filtered_gdf.geometry.area / 10000\n",
    "    \n",
    "    # Analyze size distribution before sampling\n",
    "    initial_distribution = analyze_size_distribution(filtered_gdf)\n",
    "    logging.info(\"Initial size distribution:\")\n",
    "    logging.info(initial_distribution)\n",
    "    \n",
    "    # Define strata with adjusted sample sizes based on availability\n",
    "    filtered_gdf['size_category'] = pd.cut(\n",
    "        filtered_gdf['area_ha'],\n",
    "        bins=[0, 3.37, 10.10, float('inf')],\n",
    "        labels=['small', 'medium', 'large']\n",
    "    )\n",
    "    \n",
    "    # Adjust sample sizes based on availability\n",
    "    target_sizes = {'small': 50, 'medium': 15, 'large': 7}\n",
    "    actual_sizes = {}\n",
    "    \n",
    "    for category, target in target_sizes.items():\n",
    "        available = len(filtered_gdf[filtered_gdf['size_category'] == category])\n",
    "        actual_sizes[category] = min(target, available)\n",
    "        logging.info(f\"{category}: Target={target}, Available={available}, Will sample={actual_sizes[category]}\")\n",
    "    \n",
    "    # Stratified sampling with adjusted sizes\n",
    "    sampled_events = pd.DataFrame()\n",
    "    \n",
    "    for category, size in actual_sizes.items():\n",
    "        stratum = filtered_gdf[filtered_gdf['size_category'] == category]\n",
    "        if len(stratum) > 0:\n",
    "            sampled = stratum.sample(\n",
    "                n=size, \n",
    "                random_state=42\n",
    "            )\n",
    "            sampled_events = pd.concat([sampled_events, sampled])\n",
    "    \n",
    "    # Convert back to GeoDataFrame and process\n",
    "    sampled_events = gpd.GeoDataFrame(sampled_events, geometry='geometry')\n",
    "    sampled_events = sampled_events.to_crs(4326)\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = Path(output_dir).resolve()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = output_dir / f\"sampled_events_{timestamp}.geojson\"\n",
    "    \n",
    "    sampled_events.to_file(output_path, driver='GeoJSON')\n",
    "    \n",
    "    return sampled_events, output_path, initial_distribution\n",
    "\n",
    "# Execute sampling\n",
    "sampled_events, saved_path, initial_dist = sample_deforestation_events(\n",
    "    geojson_path=geojson_path,\n",
    "    output_dir=Path(\"../Datasets/Testing/Samples\").resolve(),\n",
    "    temporal_window=temporal_window\n",
    ")\n",
    "\n",
    "# Display comprehensive results\n",
    "print(\"\\nInitial Distribution in Temporal Window:\")\n",
    "for category, count in initial_dist.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "\n",
    "print(\"\\nFinal Sample Distribution:\")\n",
    "print(sampled_events['size_category'].value_counts())\n",
    "\n",
    "print(f\"\\nSpatial Distribution:\")\n",
    "print(sampled_events.groupby('tile')['size_category'].count())\n",
    "\n",
    "print(f\"\\nTemporal Distribution:\")\n",
    "print(sampled_events.groupby(sampled_events['img_date'].dt.strftime('%Y-%m-%d'))['size_category'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's analyze the raw data to understand the area distribution\n",
    "def analyze_raw_distribution(geojson_path):\n",
    "    gdf = gpd.read_file(geojson_path)\n",
    "    gdf = gdf.to_crs('EPSG:32736')  # UTM zone 36N for Ukraine\n",
    "    gdf['area_ha'] = gdf.geometry.area / 10000\n",
    "    \n",
    "    print(\"Area Statistics (hectares):\")\n",
    "    print(gdf['area_ha'].describe())\n",
    "    \n",
    "    print(\"\\nQuantile Distribution:\")\n",
    "    quantiles = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "    print(gdf['area_ha'].quantile(quantiles))\n",
    "    \n",
    "    # Count events by tile\n",
    "    print(\"\\nEvents per tile:\")\n",
    "    print(gdf['tile'].value_counts())\n",
    "    \n",
    "    # Temporal distribution by month\n",
    "    gdf['month'] = pd.to_datetime(gdf['img_date']).dt.strftime('%Y-%m')\n",
    "    print(\"\\nEvents per month:\")\n",
    "    print(gdf['month'].value_counts().sort_index())\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# Analyze full dataset\n",
    "raw_data = analyze_raw_distribution(geojson_path)\n",
    "\n",
    "# Let's adjust our size categories based on the actual distribution\n",
    "def recalculate_size_thresholds(gdf, num_categories=3):\n",
    "    quantiles = np.linspace(0, 1, num_categories + 1)[1:-1]\n",
    "    thresholds = gdf['area_ha'].quantile(quantiles)\n",
    "    \n",
    "    print(\"\\nProposed new size thresholds:\")\n",
    "    print(f\"Small: <= {thresholds.iloc[0]:.2f} ha\")\n",
    "    print(f\"Medium: {thresholds.iloc[0]:.2f} - {thresholds.iloc[1]:.2f} ha\")\n",
    "    print(f\"Large: > {thresholds.iloc[1]:.2f} ha\")\n",
    "    \n",
    "    return thresholds.tolist()\n",
    "\n",
    "# Calculate new thresholds\n",
    "new_thresholds = recalculate_size_thresholds(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_deforestation_events(geojson_path, output_dir, temporal_window):\n",
    "\n",
    "    # Read and process data\n",
    "    gdf = gpd.read_file(geojson_path)\n",
    "    gdf['img_date'] = pd.to_datetime(gdf['img_date'])\n",
    "\n",
    "    # Filter temporal window\n",
    "    mask = (gdf['img_date'] >= temporal_window[0]) & (gdf['img_date'] <= temporal_window[1])\n",
    "    filtered_gdf = gdf[mask].copy()\n",
    "\n",
    "    # Calculate areas\n",
    "    filtered_gdf = filtered_gdf.to_crs('EPSG:32736')\n",
    "    filtered_gdf['area_ha'] = filtered_gdf.geometry.area / 10000\n",
    "\n",
    "    # Define size categories\n",
    "    filtered_gdf['size_category'] = pd.cut(\n",
    "        filtered_gdf['area_ha'],\n",
    "        bins=[0, 0.60, 2.35, float('inf')],\n",
    "        labels=['small', 'medium', 'large']\n",
    "    )\n",
    "\n",
    "    # Calculate available events per category\n",
    "    available = filtered_gdf['size_category'].value_counts()\n",
    "    print(\"Available events per category:\")\n",
    "    print(available)\n",
    "\n",
    "    # Define target samples (adjusted based on availability)\n",
    "    target_samples = {\n",
    "        'small': min(10, available.get('small', 0)),\n",
    "        'medium': min(15, available.get('medium', 0)),\n",
    "        'large': min(0, available.get('large', 0))\n",
    "    }\n",
    "    print(\"\\nTarget samples per category:\")\n",
    "    print(target_samples)\n",
    "\n",
    "    # Sample from each category\n",
    "    sampled_events = pd.DataFrame()\n",
    "    for category, target in target_samples.items():\n",
    "        if target > 0:\n",
    "            stratum = filtered_gdf[filtered_gdf['size_category'] == category]\n",
    "            # Ensure even temporal distribution within each category\n",
    "            sampled = stratum.groupby(stratum['img_date'].dt.to_period('M')).apply(\n",
    "                lambda x: x.sample(\n",
    "                    n=min(\n",
    "                        max(1, target // len(stratum['img_date'].dt.to_period('M').unique())),\n",
    "                        len(x)\n",
    "                    ),\n",
    "                    random_state=42\n",
    "                )\n",
    "            ).reset_index(drop=True)\n",
    "            # If we still need more samples, take them randomly from the remaining events\n",
    "            if len(sampled) < target:\n",
    "                remaining = stratum[~stratum.index.isin(sampled.index)]\n",
    "                if len(remaining) > 0:\n",
    "                    additional = remaining.sample(\n",
    "                        n=min(target - len(sampled), len(remaining)),\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    sampled = pd.concat([sampled, additional])\n",
    "            sampled_events = pd.concat([sampled_events, sampled])\n",
    "\n",
    "    # Convert to GeoDataFrame and prepare for saving\n",
    "    sampled_events = gpd.GeoDataFrame(sampled_events, geometry='geometry')\n",
    "    sampled_events = sampled_events.to_crs(4326)\n",
    "\n",
    "    # Assign 'name' property\n",
    "    sampled_events['name'] = ['PLOT-{0:05d}'.format(i) for i in range(1, len(sampled_events) + 1)]\n",
    "\n",
    "    # Convert 'img_date' back to string format 'YYYY-MM-DD' before saving\n",
    "    sampled_events['img_date'] = sampled_events['img_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Save results\n",
    "    output_dir = Path(output_dir).resolve()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = output_dir / f\"sampled_events_{timestamp}.geojson\"\n",
    "    sampled_events.to_file(output_path, driver='GeoJSON')\n",
    "\n",
    "    # Print detailed statistics\n",
    "    print(f\"\\nTotal number of sampled events: {len(sampled_events)}\")\n",
    "    print(\"\\nFinal sample distribution:\")\n",
    "    print(\"\\nBy size category:\")\n",
    "    print(sampled_events['size_category'].value_counts())\n",
    "    print(\"\\nTemporal distribution:\")\n",
    "    print(sampled_events.groupby([\n",
    "        sampled_events['img_date'].str.slice(0, 7),  # Get 'YYYY-MM' from 'YYYY-MM-DD'\n",
    "        'size_category'\n",
    "    ], observed=True).size().unstack(fill_value=0))\n",
    "\n",
    "    return sampled_events, output_path\n",
    "\n",
    "# Execute sampling with new parameters\n",
    "sampled_events, saved_path = sample_deforestation_events(\n",
    "    geojson_path=geojson_path,\n",
    "    output_dir=Path(\"../Datasets/Testing/Samples\").resolve(),\n",
    "    temporal_window=temporal_window\n",
    ")\n",
    "\n",
    "# Additional analysis of the results\n",
    "print(\"\\nArea statistics of sampled events:\")\n",
    "print(sampled_events['area_ha'].describe())\n",
    "\n",
    "print(\"\\nDate distribution:\")\n",
    "print(sampled_events['img_date'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample_distribution(sampled_events):\n",
    "    # Create a figure with 2x2 subplots, but only use 3\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Adjust the layout to use only 3 plots\n",
    "    gs = plt.GridSpec(2, 2)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])  # top-left\n",
    "    ax2 = fig.add_subplot(gs[0, 1])  # top-right\n",
    "    ax3 = fig.add_subplot(gs[1, :])  # bottom, spanning both columns\n",
    "    \n",
    "    # Size category distribution\n",
    "    sns.countplot(data=sampled_events, x='size_category', ax=ax1)\n",
    "    ax1.set_title('Distribution by Size Category')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    # Temporal distribution\n",
    "    sampled_events['date'] = pd.to_datetime(sampled_events['img_date']).dt.date\n",
    "    sns.histplot(data=sampled_events, x='date', ax=ax2)\n",
    "    ax2.set_title('Temporal Distribution')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Area distribution - make this plot wider\n",
    "    sns.boxplot(data=sampled_events, y='area_ha', x='size_category', ax=ax3)\n",
    "    ax3.set_title('Area Distribution by Category')\n",
    "    ax3.set_ylabel('Area (hectares)')\n",
    "    \n",
    "    # Save metadata\n",
    "    sample_metadata = {\n",
    "        'total_samples': len(sampled_events),\n",
    "        'temporal_range': {\n",
    "            'start': str(sampled_events['img_date'].min()),\n",
    "            'end': str(sampled_events['img_date'].max())\n",
    "        },\n",
    "        'size_distribution': sampled_events['size_category'].value_counts().to_dict(),\n",
    "        'area_statistics': {\n",
    "            'mean': float(sampled_events['area_ha'].mean()),\n",
    "            'median': float(sampled_events['area_ha'].median()),\n",
    "            'std': float(sampled_events['area_ha'].std())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_dir = Path(\"../Docs/Diagrams/\")\n",
    "    with open(output_dir / 'sample_metadata.json', 'w') as f:\n",
    "        json.dump(sample_metadata, f, indent=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'sample_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "# Generate visualizations\n",
    "visualize_sample_distribution(sampled_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentinelPatchProcessor:\n",
    "    def __init__(self, patch_size=224):\n",
    "        \"\"\"\n",
    "        Initialize the Sentinel-2 patch processor.\n",
    "        \n",
    "        Args:\n",
    "            patch_size (int): Size of the output patches (default: 224)\n",
    "        \"\"\"\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "    def get_tile_id(self, sentinel_path):\n",
    "        \"\"\"Extract tile ID from Sentinel path\"\"\"\n",
    "        match = re.search(r'T\\d{2}[A-Z]{3}', str(sentinel_path))\n",
    "        return match.group(0) if match else None\n",
    "        \n",
    "    def get_tile_bounds(self, sentinel_path):\n",
    "        \"\"\"Get the geographical bounds of a Sentinel tile\"\"\"\n",
    "        sample_band = next(Path(sentinel_path).glob('GRANULE/*/IMG_DATA/*B02.jp2'))\n",
    "        with rasterio.open(sample_band) as src:\n",
    "            bounds = box(*src.bounds)\n",
    "        return bounds\n",
    "        \n",
    "    def group_geometries_by_tile(self, geojson_path, sentinel_path):\n",
    "        \"\"\"Group geometries based on which tile they intersect with\"\"\"\n",
    "        gdf = gpd.read_file(geojson_path)\n",
    "        tile_bounds = self.get_tile_bounds(sentinel_path)\n",
    "        tile_id = self.get_tile_id(sentinel_path)\n",
    "        \n",
    "        # Transform geometries to tile CRS if needed\n",
    "        with rasterio.open(next(Path(sentinel_path).glob('GRANULE/*/IMG_DATA/*B02.jp2'))) as src:\n",
    "            if gdf.crs != src.crs:\n",
    "                gdf = gdf.to_crs(src.crs)\n",
    "        \n",
    "        # Filter geometries that intersect with this tile\n",
    "        mask = gdf.geometry.intersects(tile_bounds)\n",
    "        tile_geometries = gdf[mask].copy()\n",
    "        \n",
    "        # Clip geometries to tile bounds\n",
    "        tile_geometries['geometry'] = tile_geometries.geometry.intersection(tile_bounds)\n",
    "        \n",
    "        return tile_geometries if not tile_geometries.empty else None\n",
    "\n",
    "    def load_bands(self, sentinel_path):\n",
    "        \"\"\"Load and stack Sentinel-2 bands\"\"\"\n",
    "        band_paths = list(Path(sentinel_path).glob('GRANULE/*/IMG_DATA/*.jp2'))\n",
    "        band_data = {}\n",
    "        required_bands = ['B02', 'B03', 'B04', 'B08', 'B8A', 'B11', 'B12']\n",
    "        \n",
    "        for band_path in band_paths:\n",
    "            band_name = re.search(r'B\\d{2}|B8A', band_path.name)\n",
    "            if band_name:\n",
    "                band_name = band_name.group(0)\n",
    "                if band_name in required_bands:\n",
    "                    try:\n",
    "                        with rasterio.open(band_path) as src:\n",
    "                            band_data[band_name] = src.read(1)\n",
    "                            if band_name == 'B02':\n",
    "                                self.meta = src.meta.copy()\n",
    "                            logging.info(f\"Loaded band {band_name} from {self.get_tile_id(sentinel_path)}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error loading {band_name} from {sentinel_path}: {str(e)}\")\n",
    "        \n",
    "        if len(band_data) != len(required_bands):\n",
    "            missing_bands = set(required_bands) - set(band_data.keys())\n",
    "            logging.error(f\"Missing required bands for {self.get_tile_id(sentinel_path)}: {missing_bands}\")\n",
    "            return None\n",
    "            \n",
    "        return band_data\n",
    "\n",
    "    def validate_bands(self, band_data, required_bands):\n",
    "        \"\"\"Validate that all required bands are present\"\"\"\n",
    "        missing_bands = [band for band in required_bands if band not in band_data]\n",
    "        if missing_bands:\n",
    "            raise ValueError(f\"Missing required bands: {missing_bands}\")\n",
    "\n",
    "    def resample_to_10m(self, band_data):\n",
    "        \"\"\"Resample all bands to 10m resolution\"\"\"\n",
    "        try:\n",
    "            # Get shape from a 10m band (B02)\n",
    "            target_shape = band_data['B02'].shape\n",
    "            logging.debug(f\"Target shape for resampling: {target_shape}\")\n",
    "            \n",
    "            # Bands that need resampling (20m bands)\n",
    "            bands_to_resample = ['B8A', 'B11', 'B12']\n",
    "            \n",
    "            for band in bands_to_resample:\n",
    "                if band in band_data and band_data[band].shape != target_shape:\n",
    "                    logging.info(f\"Resampling {band} to 10m resolution\")\n",
    "                    band_data[band] = self._resample_array(\n",
    "                        band_data[band],\n",
    "                        target_shape\n",
    "                    )\n",
    "                    logging.debug(f\"Resampled {band} shape: {band_data[band].shape}\")\n",
    "                    \n",
    "            return band_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in resample_to_10m: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _resample_array(self, array, target_shape):\n",
    "        \"\"\"Helper function to resample arrays using bilinear interpolation\"\"\"\n",
    "        try:\n",
    "            # Convert array to PIL Image for resampling\n",
    "            img = Image.fromarray(array)\n",
    "            \n",
    "            # Resize to target shape (note the order: width, height)\n",
    "            resized = img.resize(\n",
    "                (target_shape[1], target_shape[0]),  # PIL uses (width, height)\n",
    "                resample=Image.BILINEAR\n",
    "            )\n",
    "            \n",
    "            # Convert back to numpy array\n",
    "            return np.array(resized)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in _resample_array: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def compute_indices(self, band_data):\n",
    "        \"\"\"\n",
    "        Compute NDVI and NDMI indices from Sentinel-2 bands with safe division\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate required bands\n",
    "            required_bands = ['B04', 'B08', 'B8A', 'B11']\n",
    "            self.validate_bands(band_data, required_bands)\n",
    "\n",
    "            # Calculate NDVI safely\n",
    "            nir_red_sum = band_data['B08'] + band_data['B04']\n",
    "            nir_red_diff = band_data['B08'] - band_data['B04']\n",
    "\n",
    "            # Use np.divide with where condition to handle zeros\n",
    "            ndvi = np.divide(\n",
    "                nir_red_diff, \n",
    "                nir_red_sum, \n",
    "                out=np.zeros_like(nir_red_diff, dtype=np.float32),\n",
    "                where=nir_red_sum != 0\n",
    "            )\n",
    "\n",
    "            # Calculate NDMI safely\n",
    "            nir_swir_sum = band_data['B8A'] + band_data['B11']\n",
    "            nir_swir_diff = band_data['B8A'] - band_data['B11']\n",
    "\n",
    "            # Use np.divide with where condition to handle zeros\n",
    "            ndmi = np.divide(\n",
    "                nir_swir_diff, \n",
    "                nir_swir_sum, \n",
    "                out=np.zeros_like(nir_swir_diff, dtype=np.float32),\n",
    "                where=nir_swir_sum != 0\n",
    "            )\n",
    "            \n",
    "            # Add bounds to prevent extreme values\n",
    "            ndvi = np.clip(ndvi, -1, 1)\n",
    "            ndmi = np.clip(ndmi, -1, 1)\n",
    "\n",
    "            # Replace NaN values with 0\n",
    "            ndvi = np.nan_to_num(ndvi, nan=0.0)\n",
    "            ndmi = np.nan_to_num(ndmi, nan=0.0)\n",
    "\n",
    "            logging.info(f\"Successfully computed NDVI and NDMI indices\")\n",
    "            logging.debug(f\"NDVI range: [{ndvi.min():.3f}, {ndvi.max():.3f}]\")\n",
    "            logging.debug(f\"NDMI range: [{ndmi.min():.3f}, {ndmi.max():.3f}]\")\n",
    "            \n",
    "            return ndvi, ndmi\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error computing indices: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_patches(self, stacked_bands, geometries, output_dir):\n",
    "        \"\"\"\n",
    "        Create and save image patches for each geometry using the 'name' property\n",
    "        for patch naming, with _P{number} suffix for multiple patches.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Create a dictionary to track patch counts for each name\n",
    "            patch_counts = {}\n",
    "            \n",
    "            for idx, geometry in geometries.iterrows():\n",
    "                try:\n",
    "                    # Get the name property from the GeoJSON feature\n",
    "                    plot_name = geometry['name']\n",
    "                    \n",
    "                    # Initialize patch count for this name if not exists\n",
    "                    if plot_name not in patch_counts:\n",
    "                        patch_counts[plot_name] = 1\n",
    "                    else:\n",
    "                        patch_counts[plot_name] += 1\n",
    "                    \n",
    "                    # Create patch name with _P{number} suffix if multiple patches exist\n",
    "                    patch_name = f\"{plot_name}_P{patch_counts[plot_name]}\"\n",
    "                    \n",
    "                    bounds = geometry.geometry.bounds\n",
    "                    window = from_bounds(*bounds, transform=self.meta['transform'])\n",
    "\n",
    "                    patch = stacked_bands[\n",
    "                        :,\n",
    "                        int(window.row_off):int(window.row_off + self.patch_size),\n",
    "                        int(window.col_off):int(window.col_off + self.patch_size)\n",
    "                    ]\n",
    "\n",
    "                    if patch.shape[1:] == (self.patch_size, self.patch_size):\n",
    "                        output_path = Path(output_dir) / f\"{patch_name}.npy\"\n",
    "                        np.save(output_path, patch)\n",
    "                        logging.info(f\"Saved patch {patch_name} to {output_path}\")\n",
    "                    else:\n",
    "                        logging.warning(f\"Skipping patch {patch_name} due to incorrect size: {patch.shape[1:]}\")\n",
    "\n",
    "                except KeyError as ke:\n",
    "                    logging.error(f\"'name' property not found in geometry at index {idx}: {str(ke)}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing patch for geometry at index {idx}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in create_patches: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process_imagery(self, sentinel_path, geojson_path, output_dir):\n",
    "        \"\"\"Process imagery considering tile boundaries\"\"\"\n",
    "        try:\n",
    "            tile_id = self.get_tile_id(sentinel_path)\n",
    "            if not tile_id:\n",
    "                logging.error(f\"Could not determine tile ID for {sentinel_path}\")\n",
    "                return\n",
    "\n",
    "            # Group geometries by tile\n",
    "            tile_geometries = self.group_geometries_by_tile(geojson_path, sentinel_path)\n",
    "            if tile_geometries is None:\n",
    "                logging.info(f\"No geometries intersect with tile {tile_id}\")\n",
    "                return\n",
    "\n",
    "            # Create output directory\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Load and process bands\n",
    "            band_data = self.load_bands(sentinel_path)\n",
    "            if band_data is None:\n",
    "                return\n",
    "\n",
    "            band_data = self.resample_to_10m(band_data)\n",
    "            ndvi, ndmi = self.compute_indices(band_data)\n",
    "            \n",
    "            stacked_bands = np.stack([\n",
    "                band_data['B02'], band_data['B03'], band_data['B04'],\n",
    "                band_data['B08'], band_data['B8A'], band_data['B11'],\n",
    "                band_data['B12'], ndvi, ndmi\n",
    "            ])\n",
    "\n",
    "            self.create_patches(stacked_bands, tile_geometries, output_dir)\n",
    "            logging.info(f\"Successfully processed tile {tile_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing tile {tile_id}: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "processor = SentinelPatchProcessor(patch_size=224)\n",
    "\n",
    "# Get all .SAFE directories\n",
    "base_path = Path(\"../Datasets/Sentinel-2\")\n",
    "safe_dirs = list(base_path.glob(\"*/*.SAFE\"))\n",
    "\n",
    "# Process each Sentinel-2 scene\n",
    "for safe_dir in tqdm(safe_dirs, desc=\"Processing Sentinel-2 images\"):\n",
    "    try:\n",
    "        processor.process_imagery(\n",
    "            sentinel_path=str(safe_dir),\n",
    "            geojson_path=saved_path,\n",
    "            output_dir=f\"../Datasets/Testing/Tiles/{safe_dir.parent.name}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process {safe_dir}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_patch(patch_path):\n",
    "    \"\"\"Visualize a saved patch with all its bands and indices\"\"\"\n",
    "    # Load the patch\n",
    "    patch = np.load(patch_path)\n",
    "    \n",
    "    # Define band names for labeling\n",
    "    band_names = ['B02', 'B03', 'B04', 'B08', 'B8A', 'B11', 'B12', 'NDVI', 'NDMI']\n",
    "    \n",
    "    # Create a figure with subplots for each band/index\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    fig.suptitle(f'Patch Visualization: {patch_path.name}', fontsize=16)\n",
    "    \n",
    "    # Plot each band/index\n",
    "    for idx, (ax, name) in enumerate(zip(axes.flat, band_names)):\n",
    "        im = ax.imshow(patch[idx], cmap='viridis')\n",
    "        ax.set_title(name)\n",
    "        plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage:\n",
    "tiles_dir = Path(\"../Datasets/Testing/Tiles\")\n",
    "patch_files = list(tiles_dir.glob(\"**/*.npy\"))\n",
    "\n",
    "# Visualize first few patches\n",
    "for patch_file in patch_files[:3]:  # Adjust number of patches to display\n",
    "    fig = visualize_patch(patch_file)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rgb_composite(patch_path):\n",
    "    \"\"\"Show true color RGB composite\"\"\"\n",
    "    patch = np.load(patch_path)\n",
    "    \n",
    "    # Use B04 (Red), B03 (Green), B02 (Blue) for true color\n",
    "    rgb = np.stack([patch[2], patch[1], patch[0]], axis=-1)\n",
    "    \n",
    "    # Normalize for display\n",
    "    rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(rgb)\n",
    "    plt.title(f'RGB Composite: {patch_path.name}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "show_rgb_composite(patch_files[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_temporal_patches(tiles_dir, plot_identifier):\n",
    "    \"\"\"Visualize the same patch location across different dates using the plot number.\"\"\"\n",
    "    # Determine if input is a number or a string starting with 'PLOT-'\n",
    "    if isinstance(plot_identifier, int) or plot_identifier.isdigit():\n",
    "        plot_name = f\"PLOT-{int(plot_identifier):05d}\"\n",
    "    elif str(plot_identifier).startswith('PLOT-'):\n",
    "        plot_name = plot_identifier\n",
    "    else:\n",
    "        print(f\"Invalid plot identifier: {plot_identifier}\")\n",
    "        return\n",
    "\n",
    "    # Find all patch files matching the plot name (including multiple patches)\n",
    "    patch_files = list(tiles_dir.glob(f\"**/{plot_name}*.npy\"))\n",
    "    \n",
    "    # Check if any patches are found\n",
    "    if not patch_files:\n",
    "        print(f\"No patches found for plot {plot_name}\")\n",
    "        return\n",
    "    \n",
    "    # Extract date and sequence number from the filename\n",
    "    def get_capture_date(filepath):\n",
    "        # Assuming the date is included in the directory name or filepath\n",
    "        match = re.search(r'_(\\d{8})T', str(filepath))\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            return f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}\"\n",
    "        else:\n",
    "            return \"Unknown Date\"\n",
    "        \n",
    "    def get_patch_sequence(filepath):\n",
    "        # Extract the patch sequence number (_P1, _P2, etc.)\n",
    "        match = re.search(r'_P(\\d+)\\.npy$', filepath.name)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        else:\n",
    "            # If no sequence number, assume it's the first patch\n",
    "            return 1\n",
    "\n",
    "    # Sort the files based on capture date and patch sequence\n",
    "    patch_files.sort(key=lambda x: (get_capture_date(x), get_patch_sequence(x)))\n",
    "    \n",
    "    n_patches = len(patch_files)\n",
    "    fig, axes = plt.subplots(n_patches, 1, figsize=(15, 5*n_patches))\n",
    "    \n",
    "    # Ensure axes is iterable\n",
    "    if n_patches == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, patch_file in enumerate(patch_files):\n",
    "        patch = np.load(patch_file)\n",
    "        rgb = np.stack([patch[2], patch[1], patch[0]], axis=-1)\n",
    "        \n",
    "        # Robust normalization\n",
    "        rgb_min = rgb.min()\n",
    "        rgb_max = rgb.max()\n",
    "        if rgb_max > rgb_min:\n",
    "            rgb = (rgb - rgb_min) / (rgb_max - rgb_min)\n",
    "        else:\n",
    "            rgb = np.zeros_like(rgb)\n",
    "        \n",
    "        # Get and display the actual capture date\n",
    "        file_name = patch_file.name\n",
    "        capture_date = get_capture_date(patch_file)\n",
    "        patch_seq = get_patch_sequence(patch_file)\n",
    "        \n",
    "        # Build the plot title\n",
    "        title = f'Plot: {plot_name}\\nFile: {file_name}\\nDate: {capture_date}'\n",
    "        if patch_seq > 1:\n",
    "            title += f'\\nPatch Sequence: {patch_seq}'\n",
    "        \n",
    "        # Set the title and plot the image\n",
    "        axes[idx].imshow(rgb)\n",
    "        axes[idx].set_title(title)\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Display temporal sequence\n",
    "tiles_dir = Path(\"../Datasets/Testing/Tiles\")\n",
    "visualize_temporal_patches(tiles_dir, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from tqdm import tqdm\n",
    "from s2cloudless import S2PixelCloudDetector\n",
    "\n",
    "class CloudProcessor:\n",
    "    def __init__(self, base_path: str, cloud_threshold: float = 0.1):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.cloud_threshold = cloud_threshold\n",
    "        # Corrected band list and order\n",
    "        self.s2cloudless_bands = ['B02', 'B03', 'B04', 'B05', 'B08', 'B8A', 'B11', 'B12']\n",
    "        self.cloud_detector = S2PixelCloudDetector(threshold=0.4)\n",
    "        \n",
    "        # Verify base path exists\n",
    "        if not self.base_path.exists():\n",
    "            raise ValueError(f\"Base path does not exist: {self.base_path}\")\n",
    "\n",
    "    def get_safe_dir(self, patch_file: Path) -> Path:\n",
    "        \"\"\"Get corresponding SAFE directory for a patch\"\"\"\n",
    "        # Extract product name from patch path\n",
    "        # Updated to account for the new directory structure\n",
    "        product_name = patch_file.parent.name\n",
    "\n",
    "        # Search for matching SAFE directory\n",
    "        safe_pattern = f\"{product_name}.SAFE\"\n",
    "        safe_dirs = list(self.base_path.glob(safe_pattern))\n",
    "        \n",
    "        if not safe_dirs:\n",
    "            # Try searching one level deeper\n",
    "            safe_dirs = list(self.base_path.glob(f\"*/{safe_pattern}\"))\n",
    "            \n",
    "        if not safe_dirs:\n",
    "            raise ValueError(f\"No SAFE directory found for product: {product_name}\")\n",
    "            \n",
    "        return safe_dirs[0]\n",
    "\n",
    "    def load_bands_for_patch(self, safe_dir: Path) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Load required bands for cloud detection with proper alignment\"\"\"\n",
    "        bands = {}\n",
    "        # Use B02 as reference band\n",
    "        ref_band = 'B02'\n",
    "        ref_files = list(safe_dir.glob(f'GRANULE/*/IMG_DATA/*{ref_band}*.jp2'))\n",
    "        if not ref_files:\n",
    "            raise FileNotFoundError(f\"Reference band {ref_band} not found in {safe_dir}\")\n",
    "        ref_band_file = ref_files[0]\n",
    "        with rasterio.open(ref_band_file) as ref_src:\n",
    "            ref_data = ref_src.read(1)\n",
    "            ref_transform = ref_src.transform\n",
    "            ref_crs = ref_src.crs\n",
    "            ref_shape = ref_src.shape\n",
    "\n",
    "        for band in self.s2cloudless_bands:\n",
    "            band_files = list(safe_dir.glob(f'GRANULE/*/IMG_DATA/*{band}*.jp2'))\n",
    "            if not band_files:\n",
    "                raise FileNotFoundError(f\"Band {band} not found in {safe_dir}\")\n",
    "            band_file = band_files[0]\n",
    "            with rasterio.open(band_file) as src:\n",
    "                if src.crs != ref_crs:\n",
    "                    raise ValueError(f\"Band {band} CRS does not match reference CRS\")\n",
    "                # Resample band to reference resolution and shape\n",
    "                band_data = src.read(\n",
    "                    out_shape=(1, ref_shape[0], ref_shape[1]),\n",
    "                    resampling=Resampling.bilinear\n",
    "                )[0]\n",
    "            bands[band] = band_data\n",
    "        return bands\n",
    "\n",
    "    def detect_clouds(self, bands: Dict[str, np.ndarray]) -> Tuple[bool, float]:\n",
    "        \"\"\"Run cloud detection on band data\"\"\"\n",
    "        # Stack bands in required order\n",
    "        stacked = np.stack([bands[b] for b in self.s2cloudless_bands], axis=2) / 10000.0  # Shape: (H, W, C)\n",
    "        data = np.expand_dims(stacked, axis=0)  # Shape: (1, H, W, C)\n",
    "        \n",
    "        # Get cloud probabilities\n",
    "        cloud_probs = self.cloud_detector.get_cloud_probability_maps(data)\n",
    "        cloud_mask = cloud_probs > self.cloud_detector.threshold\n",
    "        cloud_percentage = float(np.mean(cloud_mask))\n",
    "        \n",
    "        return cloud_percentage <= self.cloud_threshold, cloud_percentage\n",
    "\n",
    "    def process_patches(self, input_dir: str, output_dir: str):\n",
    "        \"\"\"Process all patches\"\"\"\n",
    "        input_path = Path(input_dir)\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # Get all patches\n",
    "        patches = list(input_path.rglob(\"*.npy\"))\n",
    "        \n",
    "        results = {\n",
    "            'total': len(patches),\n",
    "            'processed': 0,\n",
    "            'kept': 0,\n",
    "            'errors': 0\n",
    "        }\n",
    "        \n",
    "        for patch_file in tqdm(patches, desc=\"Processing patches\"):\n",
    "            try:\n",
    "                # Get original SAFE directory\n",
    "                safe_dir = self.get_safe_dir(patch_file)\n",
    "                \n",
    "                # Load bands and detect clouds\n",
    "                bands = self.load_bands_for_patch(safe_dir)\n",
    "                keep_patch, cloud_pct = self.detect_clouds(bands)\n",
    "                \n",
    "                results['processed'] += 1\n",
    "                \n",
    "                if keep_patch:\n",
    "                    # Create output directory structure\n",
    "                    out_file = output_path / patch_file.relative_to(input_path)\n",
    "                    out_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "                    \n",
    "                    # Copy patch to output\n",
    "                    patch_data = np.load(patch_file)\n",
    "                    np.save(out_file, patch_data)\n",
    "                    results['kept'] += 1\n",
    "                    \n",
    "                logging.info(f\"Processed {patch_file.name}: {cloud_pct:.1%} clouds, kept: {keep_patch}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                results['errors'] += 1\n",
    "                logging.error(f\"Error processing {patch_file}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Print summary\n",
    "        logging.info(f\"\\nProcessing Summary:\")\n",
    "        logging.info(f\"Total patches: {results['total']}\")\n",
    "        logging.info(f\"Successfully processed: {results['processed']}\")\n",
    "        logging.info(f\"Kept (low clouds): {results['kept']}\")\n",
    "        logging.info(f\"Errors: {results['errors']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample patch\n",
    "patch_path = Path(\"../Datasets/Testing/Tiles/S2A_MSIL1C_20190815T083601_N0208_R064_T36UYA_20190815T123742/T36UYA/patch_9.npy\")\n",
    "patch_data = np.load(patch_path)\n",
    "\n",
    "# Verify shape and channels\n",
    "print(f\"Patch shape: {patch_data.shape}\")  # Should be (9, 224, 224)\n",
    "print(f\"Channel stats:\")\n",
    "for i, channel_name in enumerate(['B02', 'B03', 'B04', 'B08', 'B8A', 'B11', 'B12', 'NDVI', 'NDMI']):\n",
    "    print(f\"{channel_name}: min={patch_data[i].min():.3f}, max={patch_data[i].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_patch(patch_data):\n",
    "    # True color composite (B04, B03, B02)\n",
    "    rgb = np.stack([patch_data[2], patch_data[1], patch_data[0]], axis=-1)\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    rgb = np.clip(rgb / 3000, 0, 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # RGB\n",
    "    axes[0].imshow(rgb)\n",
    "    axes[0].set_title(\"RGB\")\n",
    "    \n",
    "    # NDVI\n",
    "    axes[1].imshow(patch_data[7], cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "    axes[1].set_title(\"NDVI\")\n",
    "    \n",
    "    # NDMI\n",
    "    axes[2].imshow(patch_data[8], cmap='RdYlBu', vmin=-1, vmax=1)\n",
    "    axes[2].set_title(\"NDMI\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_patch(patch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the example SAFE directory\n",
    "example_path = \"../Datasets/Sentinel-2/S2A_MSIL1C_20190815T083601_N0208_R064_T36UYA_20190815T123742/S2A_MSIL1C_20190815T083601_N0208_R064_T36UYA_20190815T123742.SAFE\"\n",
    "\n",
    "# Find the index of the folder in the safe_dirs list\n",
    "folder_index = safe_dirs.index(Path(example_path))\n",
    "\n",
    "# Print the index\n",
    "print(f\"The index of the folder is: {folder_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio import plot\n",
    "from rasterio.windows import from_bounds, bounds as window_bounds\n",
    "from shapely.geometry import box\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def verify_patch_coordinates(sentinel_path, geojson_path, patch_path):\n",
    "    try:\n",
    "        # Load the GeoJSON geometry\n",
    "        gdf = gpd.read_file(geojson_path)\n",
    "        \n",
    "        # Find B02 band path safely\n",
    "        b02_paths = list(Path(sentinel_path).glob('GRANULE/*/IMG_DATA/*B02.jp2'))\n",
    "        if not b02_paths:\n",
    "            raise FileNotFoundError(\"B02 band not found\")\n",
    "        b02_path = b02_paths[0]\n",
    "        \n",
    "        with rasterio.open(b02_path) as src:\n",
    "            # Verify and transform CRS\n",
    "            if not gdf.crs:\n",
    "                raise ValueError(\"GeoJSON CRS is missing\")\n",
    "            if not src.crs:\n",
    "                raise ValueError(\"Raster CRS is missing\")\n",
    "            \n",
    "            if gdf.crs != src.crs:\n",
    "                gdf = gdf.to_crs(src.crs)\n",
    "            \n",
    "            # Extract patch index and get geometry\n",
    "            patch_idx = int(Path(patch_path).stem.split('_')[1])\n",
    "            if patch_idx >= len(gdf):\n",
    "                raise IndexError(f\"Patch index {patch_idx} exceeds GeoJSON features\")\n",
    "\n",
    "            geometry = gdf.iloc[patch_idx].geometry\n",
    "            geom_bounds = geometry.bounds\n",
    "            window = from_bounds(*geom_bounds, transform=src.transform)\n",
    "\n",
    "            # Get the spatial bounds of the window (patch)\n",
    "            patch_bounds = window_bounds(window, transform=src.transform)\n",
    "            \n",
    "            # Create a shapely Polygon from patch_bounds\n",
    "            patch_polygon = box(*patch_bounds)\n",
    "            \n",
    "            # Check if geometry is within the patch polygon\n",
    "            intersects = geometry.intersects(patch_polygon)\n",
    "            is_within = geometry.within(patch_polygon)\n",
    "            \n",
    "            print(f\"Geometry bounds: {geom_bounds}\")\n",
    "            print(f\"Patch bounds: {patch_bounds}\")\n",
    "            print(f\"Does geometry intersect patch bounds: {intersects}\")\n",
    "            print(f\"Is geometry entirely within patch bounds: {is_within}\")\n",
    "            \n",
    "            # Load and validate patch\n",
    "            patch = np.load(patch_path)\n",
    "\n",
    "            # Expected dimensions from window\n",
    "            expected_height = int(window.height)\n",
    "            expected_width = int(window.width)\n",
    "\n",
    "            print(f\"Patch shape: {patch.shape}\")\n",
    "            \n",
    "            # Visualize the patch with geometry\n",
    "            visualize_patch_with_geometry(sentinel_path, geometry, window)\n",
    "            \n",
    "            return {\n",
    "                'geometry_bounds': geom_bounds,\n",
    "                'patch_bounds': patch_bounds,\n",
    "                'intersects': intersects,\n",
    "                'is_within': is_within\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def visualize_patch_with_geometry(sentinel_path, geometry, window):\n",
    "    b02_paths = list(Path(sentinel_path).glob('GRANULE/*/IMG_DATA/*B02.jp2'))\n",
    "    if not b02_paths:\n",
    "        raise FileNotFoundError(\"B02 band not found\")\n",
    "    b02_path = b02_paths[0]\n",
    "    \n",
    "    with rasterio.open(b02_path) as src:\n",
    "        patch_data = src.read(1, window=window)\n",
    "        patch_transform = src.window_transform(window)\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    rasterio.plot.show(patch_data, transform=patch_transform, ax=ax, cmap='gray')\n",
    "    \n",
    "    # Plot the geometry using geom_type instead of type\n",
    "    if geometry.geom_type == 'Polygon':\n",
    "        x, y = geometry.exterior.xy\n",
    "        ax.plot(x, y, color='red', linewidth=2)\n",
    "    elif geometry.geom_type == 'MultiPolygon':\n",
    "        # Use geoms to iterate through MultiPolygon\n",
    "        for polygon in geometry.geoms:\n",
    "            x, y = polygon.exterior.xy\n",
    "            ax.plot(x, y, color='red', linewidth=2)\n",
    "    else:\n",
    "        print(f\"Unsupported geometry type: {geometry.geom_type}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "result = verify_patch_coordinates(\n",
    "    sentinel_path=safe_dirs[folder_index],\n",
    "    geojson_path=saved_path,\n",
    "    patch_path=patch_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_all_patches(sentinel_path, geojson_path, patches_dir):\n",
    "    try:\n",
    "        # Load the GeoJSON geometries\n",
    "        gdf = gpd.read_file(geojson_path)\n",
    "        \n",
    "        # Locate the B02 band image\n",
    "        b02_paths = list(Path(sentinel_path).glob('GRANULE/*/IMG_DATA/*B02.jp2'))\n",
    "        if not b02_paths:\n",
    "            raise FileNotFoundError(\"B02 band not found in the specified Sentinel-2 data directory.\")\n",
    "        b02_path = b02_paths[0]\n",
    "        \n",
    "        with rasterio.open(b02_path) as src:\n",
    "            # Ensure CRS alignment between geometries and raster\n",
    "            if not gdf.crs:\n",
    "                raise ValueError(\"CRS is missing in the GeoJSON file.\")\n",
    "            if not src.crs:\n",
    "                raise ValueError(\"CRS is missing in the raster data.\")\n",
    "            \n",
    "            if gdf.crs != src.crs:\n",
    "                gdf = gdf.to_crs(src.crs)\n",
    "            \n",
    "            total_patches = 0\n",
    "            intersects_count = 0\n",
    "            within_count = 0\n",
    "            \n",
    "            # List all patch files in the directory\n",
    "            patch_paths = sorted(Path(patches_dir).glob('patch_*.npy'))\n",
    "            \n",
    "            for patch_path in patch_paths:\n",
    "                total_patches += 1\n",
    "                \n",
    "                # Extract the patch index from the file name\n",
    "                patch_idx = int(Path(patch_path).stem.split('_')[1])\n",
    "                if patch_idx >= len(gdf):\n",
    "                    print(f\"Warning: Patch index {patch_idx} exceeds the number of geometries in the GeoJSON file.\")\n",
    "                    continue\n",
    "\n",
    "                geometry = gdf.iloc[patch_idx].geometry\n",
    "                geom_bounds = geometry.bounds\n",
    "                window = from_bounds(*geom_bounds, transform=src.transform)\n",
    "\n",
    "                # Calculate the spatial bounds of the patch\n",
    "                patch_bounds = window_bounds(window, transform=src.transform)\n",
    "                patch_polygon = box(*patch_bounds)\n",
    "                \n",
    "                # Check spatial relationships\n",
    "                intersects = geometry.intersects(patch_polygon)\n",
    "                is_within = geometry.within(patch_polygon)\n",
    "                \n",
    "                if intersects:\n",
    "                    intersects_count += 1\n",
    "                if is_within:\n",
    "                    within_count += 1\n",
    "\n",
    "            # Output the summary\n",
    "            print(f\"Total patches processed: {total_patches}\")\n",
    "            print(f\"Patches where geometry intersects patch bounds: {intersects_count}\")\n",
    "            print(f\"Patches where geometry is entirely within patch bounds: {within_count}\")\n",
    "            \n",
    "            return {\n",
    "                'total_patches': total_patches,\n",
    "                'intersects_count': intersects_count,\n",
    "                'within_count': within_count\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these with your actual paths\n",
    "sentinel_path = safe_dirs[folder_index]\n",
    "tile_id = re.search(r'T\\d{2}[A-Z]{3}', str(safe_dir)).group(0)\n",
    "scene_name = safe_dir.parent.name\n",
    "geojson_path = saved_path\n",
    "patches_dir = f\"../Datasets/Testing/Tiles/{scene_name}/{tile_id}\"\n",
    "\n",
    "# Call the function\n",
    "result = verify_all_patches(sentinel_path, geojson_path, patches_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def load_geojson_dates(print_loading=False):\n",
    "    # Load the most recent sampled events file\n",
    "    sample_files = glob('../Datasets/Testing/Samples/*.geojson')\n",
    "    if not sample_files:\n",
    "        raise FileNotFoundError(\"No .geojson files found in Testing/Samples/\")\n",
    "    latest_file = max(sample_files, key=os.path.getctime)\n",
    "    if print_loading == True:\n",
    "        print(f\"Loading events from {latest_file}\")\n",
    "\n",
    "    with open(latest_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract dates and convert to datetime objects\n",
    "    event_dates = []\n",
    "    for feature in data['features']:\n",
    "        date_str = feature['properties']['img_date']\n",
    "        try:\n",
    "            event_date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "            event_dates.append(event_date)\n",
    "        except ValueError:\n",
    "            print(f\"Date format error in {date_str}\")\n",
    "            # You can choose to skip or handle the error as needed\n",
    "            continue\n",
    "\n",
    "    return sorted(event_dates)\n",
    "\n",
    "def get_tile_date(patch_file_path):\n",
    "    # Extract date from Sentinel-2 tile path\n",
    "    tile_name = Path(patch_file_path).parent.parent.name\n",
    "    parts = tile_name.split('_')\n",
    "    if len(parts) < 3:\n",
    "        raise ValueError(f\"Unexpected tile name format: {tile_name}\")\n",
    "    date_str = parts[2][:8]\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%Y%m%d')\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Invalid date format in tile name: {date_str}\")\n",
    "\n",
    "def load_and_sort_patches():\n",
    "    patches = []\n",
    "    patch_dates = []\n",
    "\n",
    "    # Use pathlib for better path handling\n",
    "    base_path = Path('../Datasets/Testing/Tiles')\n",
    "    tile_dirs = list(base_path.glob('S2*'))\n",
    "    \n",
    "    if not tile_dirs:\n",
    "        raise FileNotFoundError(f\"No tile directories found in {base_path}\")\n",
    "\n",
    "    # Print total number of patches expected\n",
    "    print(f\"Found {len(tile_dirs)} tile directories\")\n",
    "\n",
    "    for tile_dir in tile_dirs:\n",
    "        # Find all patch files (0-65) in each tile directory\n",
    "        patch_files = []\n",
    "        for patch_num in range(66):  # 0 to 65\n",
    "            patch_files.extend(list(tile_dir.rglob(f'T*/patch_{patch_num}.npy')))\n",
    "\n",
    "        if not patch_files:\n",
    "            print(f\"No patches found in {tile_dir}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Found {len(patch_files)} patches in {tile_dir}\")\n",
    "\n",
    "        for idx, patch_file in enumerate(sorted(patch_files)):\n",
    "            try:\n",
    "                patch = np.load(patch_file)\n",
    "                patch_date = get_tile_date(patch_file)\n",
    "                patch_dates.append(patch_date)\n",
    "                patches.append(patch)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {patch_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if not patches:\n",
    "        raise RuntimeError(\"No patches loaded successfully.\")\n",
    "\n",
    "    # Convert patch_dates to numpy datetime64 for sorting\n",
    "    patch_dates_np = np.array(patch_dates, dtype='datetime64')\n",
    "    patches_np = np.array(patches)\n",
    "\n",
    "    # Sort patches by date\n",
    "    sorted_indices = np.argsort(patch_dates_np)\n",
    "    patches_sorted = patches_np[sorted_indices]\n",
    "    patch_dates_sorted = patch_dates_np[sorted_indices]\n",
    "\n",
    "    print(f\"\\nTotal patches loaded: {len(patches_sorted)}\")\n",
    "    return patches_sorted, patch_dates_sorted\n",
    "\n",
    "event_dates = load_geojson_dates(print_loading=True)\n",
    "patches, patch_dates = load_and_sort_patches()\n",
    "# Example processing: print the number of patches and first few dates\n",
    "print(\"First 5 patch dates:\")\n",
    "for date in patch_dates[:5]:\n",
    "    print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def analyze_patch_distribution():\n",
    "    base_path = Path('../Datasets/Testing/Tiles')\n",
    "    patch_counts = Counter()\n",
    "    \n",
    "    # Count patches\n",
    "    for tile_dir in base_path.glob('S2*'):\n",
    "        for patch_num in range(66):\n",
    "            count = len(list(tile_dir.rglob(f'T*/patch_{patch_num}.npy')))\n",
    "            patch_counts[patch_num] = patch_counts[patch_num] + count\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    patch_numbers = list(range(66))\n",
    "    frequencies = [patch_counts[i] for i in patch_numbers]\n",
    "    \n",
    "    # Create histogram\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.bar(patch_numbers, frequencies)\n",
    "    plt.title('Distribution of Patch Numbers')\n",
    "    plt.xlabel('Patch Number')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Total patches: {sum(frequencies)}\")\n",
    "    print(f\"Most common patch numbers:\")\n",
    "    for num, freq in sorted(patch_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"Patch {num}: {freq} occurrences\")\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    return patch_counts\n",
    "\n",
    "# Run analysis\n",
    "distribution = analyze_patch_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import h5py\n",
    "\n",
    "# Create a directory for intermediate storage\n",
    "os.makedirs('../Datasets/Testing/Processed/HDF5-TemporalPairs', exist_ok=True)\n",
    "\n",
    "# Process in smaller chunks and save to HDF5\n",
    "with h5py.File('../Datasets/Testing/Processed/HDF5-TemporalPairs/pairs.h5', 'w') as f:\n",
    "    # Create datasets\n",
    "    f.create_dataset('pairs', shape=(0, 2, *patches[0].shape), maxshape=(None, 2, *patches[0].shape), chunks=True)\n",
    "    f.create_dataset('labels', shape=(0,), maxshape=(None,), dtype=bool)\n",
    "    \n",
    "    pair_count = 0\n",
    "    chunk_size = 10  # Process 10 pairs at a time\n",
    "    \n",
    "    for i in range(0, len(patches)-1, chunk_size):\n",
    "        chunk_pairs = []\n",
    "        chunk_labels = []\n",
    "        \n",
    "        chunk_end = min(i + chunk_size, len(patches)-1)\n",
    "        for j in range(i, chunk_end):\n",
    "            date1 = patch_dates[j]\n",
    "            for k in range(j+1, len(patches)):\n",
    "                date2 = patch_dates[k]\n",
    "                if (date2 - date1).astype('timedelta64[D]').astype(int) <= 30:\n",
    "                    chunk_pairs.append([patches[j], patches[k]])\n",
    "                    has_event = any(date1 <= event_date <= date2 for event_date in load_geojson_dates())\n",
    "                    chunk_labels.append(has_event)\n",
    "        \n",
    "        if chunk_pairs:\n",
    "            # Resize datasets\n",
    "            new_size = pair_count + len(chunk_pairs)\n",
    "            f['pairs'].resize(new_size, axis=0)\n",
    "            f['labels'].resize(new_size, axis=0)\n",
    "            \n",
    "            # Store chunk\n",
    "            f['pairs'][pair_count:new_size] = chunk_pairs\n",
    "            f['labels'][pair_count:new_size] = chunk_labels\n",
    "            \n",
    "            pair_count = new_size\n",
    "\n",
    "print(f\"Total pairs saved: {pair_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create temporal pairs\n",
    "temporal_pairs = []\n",
    "labels = []\n",
    "max_time_diff = 30  # Maximum days between image pairs\n",
    "\n",
    "for i, (date1, patch1) in enumerate(zip(patch_dates[:-1], patches[:-1])):\n",
    "    for j, (date2, patch2) in enumerate(zip(patch_dates[i+1:], patches[i+1:]), i+1):\n",
    "        time_diff = (date2 - date1).astype('timedelta64[D]').astype(int)\n",
    "        if time_diff <= max_time_diff:\n",
    "            temporal_pairs.append((patch1, patch2))\n",
    "            # Binary label: 1 if deforestation event exists between dates\n",
    "            has_event = any(date1 <= event_date <= date2 for event_date in load_geojson_dates())\n",
    "            labels.append(has_event)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(temporal_pairs)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split data: 70% train, 15% validation, 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training pairs: {len(X_train)}\")\n",
    "print(f\"Validation pairs: {len(X_val)}\")\n",
    "print(f\"Testing pairs: {len(X_test)}\")\n",
    "print(f\"Positive samples: {sum(y)}/{len(y)} ({sum(y)/len(y)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "   def __init__(self, in_ch, out_ch):\n",
    "       super().__init__()\n",
    "       self.conv = nn.Sequential(\n",
    "           nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "           nn.BatchNorm2d(out_ch),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "           nn.BatchNorm2d(out_ch),\n",
    "           nn.ReLU(inplace=True)\n",
    "       )\n",
    "\n",
    "   def forward(self, x):\n",
    "       return self.conv(x)\n",
    "\n",
    "class UNetCH(nn.Module):\n",
    "   def __init__(self, in_channels=27): # 9 channels each for img1, img2, diff\n",
    "       super().__init__()\n",
    "       \n",
    "       # Encoder\n",
    "       self.enc1 = ConvBlock(in_channels, 64)\n",
    "       self.enc2 = ConvBlock(64, 128)\n",
    "       self.enc3 = ConvBlock(128, 256)\n",
    "       \n",
    "       # Decoder\n",
    "       self.dec3 = ConvBlock(256 + 128, 128)\n",
    "       self.dec2 = ConvBlock(128 + 64, 64)\n",
    "       self.dec1 = ConvBlock(64, 32)\n",
    "       \n",
    "       # Classification Head\n",
    "       self.cls_head = nn.Sequential(\n",
    "           nn.AdaptiveMaxPool2d(1),\n",
    "           nn.Flatten(),\n",
    "           nn.Linear(256, 1),\n",
    "           nn.Sigmoid()\n",
    "       )\n",
    "       \n",
    "       # Final Conv\n",
    "       self.final = nn.Conv2d(32, 1, kernel_size=1)\n",
    "       \n",
    "       # Pooling and Upsampling\n",
    "       self.pool = nn.MaxPool2d(2)\n",
    "       self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "       \n",
    "   def forward(self, x):\n",
    "       # Encoder\n",
    "       e1 = self.enc1(x)\n",
    "       e2 = self.enc2(self.pool(e1))\n",
    "       e3 = self.enc3(self.pool(e2))\n",
    "       \n",
    "       # Classification Branch\n",
    "       cls_output = self.cls_head(e3)\n",
    "       \n",
    "       # Decoder with Skip Connections\n",
    "       d3 = self.dec3(torch.cat([self.up(e3), e2], dim=1))\n",
    "       d2 = self.dec2(torch.cat([self.up(d3), e1], dim=1))\n",
    "       d1 = self.dec1(d2)\n",
    "       \n",
    "       # Final Output\n",
    "       seg_output = torch.sigmoid(self.final(d1))\n",
    "       \n",
    "       return seg_output, cls_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP1252-MP(GPU)",
   "language": "python",
   "name": "comp1252-mp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
