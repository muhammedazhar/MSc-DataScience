{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Essential imports\n",
    "    import re\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import glob\n",
    "    import h5py\n",
    "    import torch\n",
    "    import shutil\n",
    "    import logging\n",
    "    import rasterio\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    import geopandas as gpd\n",
    "    from pprint import pprint\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from PIL import Image\n",
    "    from datetime import datetime\n",
    "    from pathlib import Path\n",
    "    from scipy import stats\n",
    "    from typing import Dict, Tuple, Optional\n",
    "    from collections import defaultdict\n",
    "    from rasterio.mask import mask\n",
    "    from shapely.ops import unary_union\n",
    "    from shapely.wkt import dumps, loads\n",
    "    from shapely.geometry import mapping, box, Polygon, MultiPolygon\n",
    "    from rasterio.windows import from_bounds\n",
    "    from s2cloudless import S2PixelCloudDetector\n",
    "    from tqdm import tqdm\n",
    "    from tqdm.notebook import tqdm\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project directory structure\n",
    "project_dir = Path(\"../Solutions/Land_Change_Monitoring\")\n",
    "subdirs = [\n",
    "    \"../Datasets/Sentinel-2/\",           # Original GeoJSON and Sentinel-2 data\n",
    "    \"../Datasets/Testing/Processed\",     # Processed and grouped events\n",
    "    \"../Datasets/Testing/Samples\",       # Our sampled datasets\n",
    "    \"../Datasets/Testing/Tiles\",         # Generated image tiles\n",
    "    \"../Docs/Diagrams\",                             # Results and visualizations\n",
    "    \"../Models\",                                    # Trained models\n",
    "    \"../Docs/Logs\"                                  # Processing logs\n",
    "]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for subdir in subdirs:\n",
    "    Path(subdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=Path(\"../Docs/Logs/processing.log\"),\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Print created directory structure for verification\n",
    "print(\"Created directory structure:\")\n",
    "for subdir in subdirs:\n",
    "    if Path(subdir).exists():\n",
    "        print(f\"✓ {subdir}\")\n",
    "    else:\n",
    "        print(f\"✗ {subdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check if running in Google Colab\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(\"GPU not available in Colab, consider enabling a GPU runtime.\")\n",
    "# Running on a local machine\n",
    "else:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        print(f\"Is Apple MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n",
    "        print(f\"Is Apple MPS available? {torch.backends.mps.is_available()}\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "\n",
    "# TODO: Add support for AMD ROCm GPU if needed\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the GeoJSON file\n",
    "base_path = Path('../Datasets/Sentinel-2')\n",
    "geojson_path = Path('../Datasets/BoundingBox/deforestation.geojson')\n",
    "safe_dirs = list(base_path.glob(\"*/*.SAFE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GeoJSON file\n",
    "gdf = gpd.read_file(geojson_path)\n",
    "\n",
    "# Display GeoJSON information\n",
    "print(\"GeoJSON Information:\", len(gdf))\n",
    "print(\"-\" * 50)\n",
    "print(gdf.info())\n",
    "print(\"\\nFirst few records:\")\n",
    "print(gdf.head())\n",
    "\n",
    "# Get the bounding box coordinates\n",
    "bbox = gdf.total_bounds\n",
    "print(\"\\nBounding Box (minx, miny, maxx, maxy):\")\n",
    "print(bbox)\n",
    "\n",
    "# Display basic information about the GeoJSON\n",
    "print(\"\\nGeoJSON CRS:\", gdf.crs)\n",
    "print(\"Number of features:\", len(gdf))\n",
    "print(\"Columns:\", gdf.columns.tolist())\n",
    "print(\"First geometry type:\", gdf.geometry.iloc[0].geom_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'img_date' is in datetime format if it exists\n",
    "if 'img_date' in gdf.columns:\n",
    "    gdf['img_date'] = pd.to_datetime(gdf['img_date'], errors='coerce')\n",
    "\n",
    "    # Drop rows with invalid dates if any\n",
    "    gdf = gdf.dropna(subset=['img_date'])\n",
    "\n",
    "    # Get unique dates and sort them\n",
    "    unique_dates = gdf['img_date'].dt.date.unique()\n",
    "    unique_dates.sort()\n",
    "\n",
    "    date_counts = gdf['img_date'].dt.date.value_counts().sort_index()\n",
    "    print(\"Occurrences of each 'img_date':\")\n",
    "    print(date_counts)\n",
    "else:\n",
    "    print(\"'img_date' column not found in the GeoDataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reduce geometry precision\n",
    "def reduce_precision(geometry, decimal_places=5):\n",
    "    return loads(dumps(geometry, rounding_precision=decimal_places))\n",
    "\n",
    "# Apply precision reduction to all geometries\n",
    "gdf['geometry'] = gdf['geometry'].apply(lambda geom: reduce_precision(geom))\n",
    "\n",
    "# Now check for duplicates again\n",
    "duplicate_geometries = gdf[gdf.geometry.duplicated(keep=False)]\n",
    "print(\"Duplicate geometries after reducing precision:\")\n",
    "print(duplicate_geometries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on geometry\n",
    "# gdf = gdf.drop_duplicates(subset='geometry')\n",
    "\n",
    "# Save the cleaned GeoDataFrame to a new GeoJSON file\n",
    "# gdf.to_file(\"deforestation_unique.geojson\", driver='GeoJSON')\n",
    "\n",
    "print(f\"Number of geometries present in gdf: {len(gdf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the geospatial data\n",
    "gdf.plot()\n",
    "plt.title('Deforestation Areas (Ukraine)')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal distribution\n",
    "gdf['year'] = gdf['img_date'].dt.year\n",
    "gdf['month'] = gdf['img_date'].dt.month\n",
    "\n",
    "# Create year-month summary\n",
    "temporal_dist = gdf.groupby(['year', 'month']).size().unstack(fill_value=0)\n",
    "print(\"Deforestation events by year and month:\")\n",
    "print(temporal_dist)\n",
    "\n",
    "# Distribution by tile\n",
    "print(\"\\nDeforestation events by tile:\")\n",
    "print(gdf['tile'].value_counts())\n",
    "\n",
    "# Create a monthly summary plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "temporal_dist.T.plot(kind='bar', stacked=True)\n",
    "plt.title('Deforestation Events by Month and Year')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Events')\n",
    "plt.legend(title='Year')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate area of each polygon in square meters\n",
    "# Converting to UTM projection for accurate area and perimeter calculation\n",
    "gdf['area'] = gdf.geometry.to_crs({'proj':'utm', 'zone':36, 'ellps':'WGS84'}).area\n",
    "gdf['perimeter'] = gdf.geometry.to_crs({'proj':'utm', 'zone':36, 'ellps':'WGS84'}).length\n",
    "\n",
    "# Basic statistics of polygon sizes\n",
    "print(\"Polygon area statistics (square meters):\")\n",
    "print(gdf['area'].describe())\n",
    "\n",
    "# Create histogram of polygon sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(gdf['area'], bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Deforestation Polygon Sizes')\n",
    "plt.xlabel('Area (square meters)')\n",
    "plt.ylabel('Count')\n",
    "plt.yscale('log')  # Using log scale for better visualization\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate basic shape metrics\n",
    "gdf['complexity'] = gdf['perimeter'] / (4 * np.sqrt(gdf['area']))\n",
    "\n",
    "print(\"Shape complexity statistics (1.0 = perfect square):\")\n",
    "print(gdf['complexity'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject to UTM Zone 36N\n",
    "deforestation_data_utm = gdf.to_crs(epsg=32636)\n",
    "\n",
    "# Recalculate statistics in UTM projection\n",
    "polygon_areas = deforestation_data_utm.geometry.area  # now in square meters\n",
    "polygon_bounds = deforestation_data_utm.geometry.bounds\n",
    "\n",
    "# Calculate statistics\n",
    "max_width = (polygon_bounds.maxx - polygon_bounds.minx).max()\n",
    "max_height = (polygon_bounds.maxy - polygon_bounds.miny).max()\n",
    "mean_area = polygon_areas.mean()\n",
    "median_area = polygon_areas.median()\n",
    "\n",
    "print(f\"Polygon Statistics:\")\n",
    "print(f\"Mean area: {mean_area/10000:.2f} hectares\")\n",
    "print(f\"Median area: {median_area/10000:.2f} hectares\")\n",
    "print(f\"Max width: {max_width:.2f} meters\")\n",
    "print(f\"Max height: {max_height:.2f} meters\")\n",
    "\n",
    "# Calculate optimal mesh size based on polygon distribution\n",
    "hist, bins = np.histogram(polygon_areas, bins=20)\n",
    "print(\"\\nArea Distribution (hectares):\")\n",
    "for i in range(len(hist)):\n",
    "    if hist[i] > 0:\n",
    "        print(f\"{bins[i]/10000:.2f} - {bins[i+1]/10000:.2f}: {hist[i]} polygons\")\n",
    "\n",
    "# Additional useful statistics\n",
    "print(\"\\nAdditional Statistics:\")\n",
    "print(f\"Total number of polygons: {len(deforestation_data_utm)}\")\n",
    "print(f\"Total area of deforestation: {polygon_areas.sum()/10000:.2f} hectares\")\n",
    "print(f\"95th percentile area: {np.percentile(polygon_areas, 95)/10000:.2f} hectares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_jp2_files(base_dir):\n",
    "    jp2_files = []  # Create empty list to store paths\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jp2\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                jp2_files.append(full_path)\n",
    "    return jp2_files\n",
    "\n",
    "\n",
    "jp2_files = collect_jp2_files(base_path)\n",
    "\n",
    "# Print count\n",
    "print(f\"Total files found: {len(jp2_files)}\\n\")\n",
    "\n",
    "# Print all paths\n",
    "print(\"Found .jp2 files:\")\n",
    "for path in jp2_files:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentinel_structure(base_path):\n",
    "    \"\"\"\n",
    "    Analyzes Sentinel-2 dataset structure and returns key information\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): Path to the Sentinel-2 dataset directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing image metadata\n",
    "    \"\"\"\n",
    "    # Initialize lists to store metadata\n",
    "    metadata = []\n",
    "    \n",
    "    # Convert to Path object\n",
    "    base = Path(base_path)\n",
    "    \n",
    "    # Pattern for date extraction\n",
    "    date_pattern = r'(\\d{8}T\\d{6})'\n",
    "    \n",
    "    # Iterate through all .SAFE directories\n",
    "    for safe_dir in base.glob('*/*.SAFE'):\n",
    "        # Extract metadata from directory name\n",
    "        dir_name = safe_dir.parent.name\n",
    "        \n",
    "        # Extract date using regex\n",
    "        date_match = re.search(date_pattern, dir_name)\n",
    "        if date_match:\n",
    "            acquisition_date = datetime.strptime(date_match.group(1), '%Y%m%dT%H%M%S')\n",
    "        else:\n",
    "            acquisition_date = None\n",
    "            \n",
    "        # Get satellite (S2A or S2B)\n",
    "        satellite = dir_name[:3]\n",
    "        \n",
    "        # Get tile ID\n",
    "        tile_match = re.search(r'T(\\d{2}[A-Z]{3})', dir_name)\n",
    "        tile_id = tile_match.group(1) if tile_match else None\n",
    "        \n",
    "        # Count number of bands\n",
    "        bands = list(safe_dir.glob('GRANULE/*/IMG_DATA/*.jp2'))\n",
    "        num_bands = len([b for b in bands if not b.name.endswith('TCI.jp2')])\n",
    "        \n",
    "        metadata.append({\n",
    "            'satellite': satellite,\n",
    "            'acquisition_date': acquisition_date,\n",
    "            'tile_id': tile_id,\n",
    "            'num_bands': num_bands,\n",
    "            'path': safe_dir\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(metadata)\n",
    "    df = df.sort_values('acquisition_date')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Usage example:\n",
    "df = analyze_sentinel_structure(base_path)\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"Dataset Summary:\")\n",
    "print(f\"Total number of images: {len(df)}\")\n",
    "print(\"\\nAcquisitions by satellite:\")\n",
    "print(df['satellite'].value_counts())\n",
    "print(\"\\nDate range:\")\n",
    "print(f\"First acquisition: {df['acquisition_date'].min()}\")\n",
    "print(f\"Last acquisition: {df['acquisition_date'].max()}\")\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_band_statistics(image_path):\n",
    "    \"\"\"Get statistics for specific bands in an image\"\"\"\n",
    "    band_paths = Path(image_path).glob('GRANULE/*/IMG_DATA/*.jp2')\n",
    "    bands = {}\n",
    "    for band_path in band_paths:\n",
    "        band_name = re.search(r'B\\d{2}|B8A', band_path.name)\n",
    "        if band_name:\n",
    "            bands[band_name.group(0)] = str(band_path)\n",
    "    return dict(sorted(bands.items()))\n",
    "\n",
    "def get_quality_masks(image_path):\n",
    "    \"\"\"Get list of quality masks for an image\"\"\"\n",
    "    mask_paths = Path(image_path).glob('GRANULE/*/QI_DATA/*.gml')\n",
    "    return [p.name for p in mask_paths]\n",
    "\n",
    "# Example usage:\n",
    "image_path = df.iloc[0]['path']\n",
    "print(\"Band files in first image:\")\n",
    "for band, path in get_band_statistics(image_path).items():\n",
    "    print(f\"{band}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first image directory to analyze bands\n",
    "first_image = safe_dirs[0]\n",
    "img_data_path = list((first_image / \"GRANULE\").glob(\"*\"))[0] / \"IMG_DATA\"\n",
    "band_files = list(img_data_path.glob(\"*.jp2\"))\n",
    "\n",
    "# Extract band information\n",
    "band_info = []\n",
    "for band_file in band_files:\n",
    "    band_name = band_file.name.split('_')[-1].split('.')[0]\n",
    "    \n",
    "    # Open the band file to get metadata\n",
    "    with rasterio.open(band_file) as src:\n",
    "        band_info.append({\n",
    "            'band': band_name,\n",
    "            'width': src.width,\n",
    "            'height': src.height,\n",
    "            'dtype': src.dtypes[0],\n",
    "            'resolution': src.res[0]  # pixel size in meters\n",
    "        })\n",
    "\n",
    "# Create DataFrame with band information\n",
    "df_bands = pd.DataFrame(band_info)\n",
    "print(\"Band Information:\")\n",
    "print(\"-\" * 50)\n",
    "print(df_bands.sort_values('band'))\n",
    "\n",
    "# Count number of files per image\n",
    "print(\"\\nNumber of files per image:\")\n",
    "print(len(band_files))\n",
    "\n",
    "# Print list of unique bands\n",
    "print(\"\\nAvailable bands:\")\n",
    "unique_bands = sorted(list(df_bands['band'].unique()))\n",
    "print(unique_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first key\n",
    "example = jp2_files[9]\n",
    "\n",
    "def visualize_band(jp2_path):\n",
    "    with rasterio.open(jp2_path) as src:\n",
    "        band = src.read(1)  # Read the first band\n",
    "        plt.imshow(band, cmap=\"gray\")\n",
    "        plt.title(jp2_path)\n",
    "        plt.show()\n",
    "\n",
    "visualize_band(example)\n",
    "\n",
    "# Open the .jp2 file\n",
    "with rasterio.open(example) as dataset:\n",
    "    # Read the dataset's data as a numpy array\n",
    "    band_data = dataset.read(1)\n",
    "    # Access metadata\n",
    "    metadata = dataset.meta\n",
    "\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store metadata\n",
    "metadata_list = []\n",
    "\n",
    "for safe_dir in safe_dirs:\n",
    "    # Parse directory name\n",
    "    dir_parts = safe_dir.name.split('_')\n",
    "    \n",
    "    metadata = {\n",
    "        'satellite': dir_parts[0],  # S2A or S2B\n",
    "        'processing_level': dir_parts[1],  # MSIL1C\n",
    "        'timestamp': datetime.strptime(dir_parts[2], '%Y%m%dT%H%M%S'),\n",
    "        'relative_orbit': dir_parts[4],  # R064\n",
    "        'tile_id': dir_parts[5],  # T36UYA\n",
    "        'path': safe_dir\n",
    "    }\n",
    "    metadata_list.append(metadata)\n",
    "\n",
    "# Create DataFrame\n",
    "df_metadata = pd.DataFrame(metadata_list)\n",
    "\n",
    "# Basic analysis\n",
    "print(\"Dataset Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Date range: {df_metadata['timestamp'].min()} to {df_metadata['timestamp'].max()}\")\n",
    "print(f\"Number of unique tiles: {df_metadata['tile_id'].nunique()}\")\n",
    "print(f\"Number of satellites: {df_metadata['satellite'].nunique()}\")\n",
    "print(\"\\nSatellite distribution:\")\n",
    "print(df_metadata['satellite'].value_counts())\n",
    "print(\"\\nTile distribution:\")\n",
    "print(df_metadata['tile_id'].value_counts())\n",
    "\n",
    "# Visualize temporal distribution\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.hist(df_metadata['timestamp'], bins=20, edgecolor='black')\n",
    "plt.title('Temporal Distribution of Satellite Images')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Monthly distribution\n",
    "df_metadata['month'] = df_metadata['timestamp'].dt.month\n",
    "monthly_counts = df_metadata['month'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "monthly_counts.plot(kind='bar')\n",
    "plt.title('Monthly Distribution of Images')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size calculation with confidence level\n",
    "def calculate_strata_sizes(total_events=480, confidence_level=0.95, margin_error=0.1):\n",
    "    z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "    \n",
    "    # Define strata bounds and proportions\n",
    "    strata_bounds = [0, 3.37, 10.10, np.inf]  # hectares\n",
    "    strata_props = [0.8, 0.15, 0.05]  # given proportions\n",
    "    \n",
    "    # Calculate sample sizes for each stratum\n",
    "    sample_sizes = []\n",
    "    for prop in strata_props:\n",
    "        stratum_n = int(np.ceil((z_score**2 * prop * (1-prop)) / margin_error**2))\n",
    "        sample_sizes.append(min(stratum_n, int(total_events * prop)))\n",
    "    \n",
    "    return sample_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_deforestation_events(gdf, spatial_thresh, temporal_thresh):\n",
    "    # Create time-based groups\n",
    "    gdf['temporal_group'] = pd.to_datetime(gdf['date']).dt.to_period('D')\n",
    "    temporal_groups = gdf.groupby(pd.Grouper(key='temporal_group', freq=f'{temporal_thresh}D'))\n",
    "    \n",
    "    grouped_events = []\n",
    "    for _, time_group in temporal_groups:\n",
    "        if len(time_group) > 0:\n",
    "            # Buffer and merge nearby polygons\n",
    "            buffered = time_group.geometry.buffer(spatial_thresh)\n",
    "            merged = unary_union(buffered)\n",
    "            grouped_events.append(merged)\n",
    "            \n",
    "    return gpd.GeoDataFrame(geometry=grouped_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Define temporal window\n",
    "temporal_window = [\n",
    "    datetime.datetime(2018, 7, 15),\n",
    "    datetime.datetime(2018, 9, 15)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_size_distribution(gdf):\n",
    "    \"\"\"Analyze the size distribution of events before sampling\"\"\"\n",
    "    size_stats = {\n",
    "        'small': len(gdf[gdf['area_ha'] <= 3.37]),\n",
    "        'medium': len(gdf[(gdf['area_ha'] > 3.37) & (gdf['area_ha'] <= 10.10)]),\n",
    "        'large': len(gdf[gdf['area_ha'] > 10.10])\n",
    "    }\n",
    "    return size_stats\n",
    "\n",
    "def sample_deforestation_events(geojson_path, output_dir, temporal_window):\n",
    "    # Read and initial processing\n",
    "    gdf = gpd.read_file(geojson_path)\n",
    "    \n",
    "    # Log initial dataset size\n",
    "    logging.info(f\"Total events in dataset: {len(gdf)}\")\n",
    "    \n",
    "    # Convert dates and filter for our temporal window\n",
    "    gdf['img_date'] = pd.to_datetime(gdf['img_date'])\n",
    "    mask = (gdf['img_date'] >= temporal_window[0]) & (gdf['img_date'] <= temporal_window[1])\n",
    "    filtered_gdf = gdf[mask].copy()\n",
    "    \n",
    "    logging.info(f\"Events in temporal window: {len(filtered_gdf)}\")\n",
    "    \n",
    "    # Project to UTM and calculate area in hectares\n",
    "    filtered_gdf = filtered_gdf.to_crs('EPSG:32736')  # UTM zone 36N for Ukraine\n",
    "    filtered_gdf['area_ha'] = filtered_gdf.geometry.area / 10000\n",
    "    \n",
    "    # Analyze size distribution before sampling\n",
    "    initial_distribution = analyze_size_distribution(filtered_gdf)\n",
    "    logging.info(\"Initial size distribution:\")\n",
    "    logging.info(initial_distribution)\n",
    "    \n",
    "    # Define strata with adjusted sample sizes based on availability\n",
    "    filtered_gdf['size_category'] = pd.cut(\n",
    "        filtered_gdf['area_ha'],\n",
    "        bins=[0, 3.37, 10.10, float('inf')],\n",
    "        labels=['small', 'medium', 'large']\n",
    "    )\n",
    "    \n",
    "    # Adjust sample sizes based on availability\n",
    "    target_sizes = {'small': 50, 'medium': 15, 'large': 7}\n",
    "    actual_sizes = {}\n",
    "    \n",
    "    for category, target in target_sizes.items():\n",
    "        available = len(filtered_gdf[filtered_gdf['size_category'] == category])\n",
    "        actual_sizes[category] = min(target, available)\n",
    "        logging.info(f\"{category}: Target={target}, Available={available}, Will sample={actual_sizes[category]}\")\n",
    "    \n",
    "    # Stratified sampling with adjusted sizes\n",
    "    sampled_events = pd.DataFrame()\n",
    "    \n",
    "    for category, size in actual_sizes.items():\n",
    "        stratum = filtered_gdf[filtered_gdf['size_category'] == category]\n",
    "        if len(stratum) > 0:\n",
    "            sampled = stratum.sample(\n",
    "                n=size, \n",
    "                random_state=42\n",
    "            )\n",
    "            sampled_events = pd.concat([sampled_events, sampled])\n",
    "    \n",
    "    # Convert back to GeoDataFrame and process\n",
    "    sampled_events = gpd.GeoDataFrame(sampled_events, geometry='geometry')\n",
    "    sampled_events = sampled_events.to_crs(4326)\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = Path(output_dir).resolve()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = output_dir / f\"sampled_events_{timestamp}.geojson\"\n",
    "    \n",
    "    sampled_events.to_file(output_path, driver='GeoJSON')\n",
    "    \n",
    "    return sampled_events, output_path, initial_distribution\n",
    "\n",
    "# Execute sampling\n",
    "sampled_events, saved_path, initial_dist = sample_deforestation_events(\n",
    "    geojson_path=geojson_path,\n",
    "    output_dir=Path(\"../Datasets/Testing/Samples\").resolve(),\n",
    "    temporal_window=temporal_window\n",
    ")\n",
    "\n",
    "# Display comprehensive results\n",
    "print(\"\\nInitial Distribution in Temporal Window:\")\n",
    "for category, count in initial_dist.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "\n",
    "print(\"\\nFinal Sample Distribution:\")\n",
    "print(sampled_events['size_category'].value_counts())\n",
    "\n",
    "print(f\"\\nSpatial Distribution:\")\n",
    "print(sampled_events.groupby('tile')['size_category'].count())\n",
    "\n",
    "print(f\"\\nTemporal Distribution:\")\n",
    "print(sampled_events.groupby(sampled_events['img_date'].dt.strftime('%Y-%m-%d'))['size_category'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's analyze the raw data to understand the area distribution\n",
    "def analyze_raw_distribution(geojson_path):\n",
    "    gdf = gpd.read_file(geojson_path)\n",
    "    gdf = gdf.to_crs('EPSG:32736')  # UTM zone 36N for Ukraine\n",
    "    gdf['area_ha'] = gdf.geometry.area / 10000\n",
    "    \n",
    "    print(\"Area Statistics (hectares):\")\n",
    "    print(gdf['area_ha'].describe())\n",
    "    \n",
    "    print(\"\\nQuantile Distribution:\")\n",
    "    quantiles = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "    print(gdf['area_ha'].quantile(quantiles))\n",
    "    \n",
    "    # Count events by tile\n",
    "    print(\"\\nEvents per tile:\")\n",
    "    print(gdf['tile'].value_counts())\n",
    "    \n",
    "    # Temporal distribution by month\n",
    "    gdf['month'] = pd.to_datetime(gdf['img_date']).dt.strftime('%Y-%m')\n",
    "    print(\"\\nEvents per month:\")\n",
    "    print(gdf['month'].value_counts().sort_index())\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# Analyze full dataset\n",
    "raw_data = analyze_raw_distribution(geojson_path)\n",
    "\n",
    "# Let's adjust our size categories based on the actual distribution\n",
    "def recalculate_size_thresholds(gdf, num_categories=3):\n",
    "    quantiles = np.linspace(0, 1, num_categories + 1)[1:-1]\n",
    "    thresholds = gdf['area_ha'].quantile(quantiles)\n",
    "    \n",
    "    print(\"\\nProposed new size thresholds:\")\n",
    "    print(f\"Small: <= {thresholds.iloc[0]:.2f} ha\")\n",
    "    print(f\"Medium: {thresholds.iloc[0]:.2f} - {thresholds.iloc[1]:.2f} ha\")\n",
    "    print(f\"Large: > {thresholds.iloc[1]:.2f} ha\")\n",
    "    \n",
    "    return thresholds.tolist()\n",
    "\n",
    "# Calculate new thresholds\n",
    "new_thresholds = recalculate_size_thresholds(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_deforestation_events_v3(geojson_path, output_dir, temporal_window):\n",
    "    # Read and process data\n",
    "    gdf = gpd.read_file(geojson_path)\n",
    "    gdf['img_date'] = pd.to_datetime(gdf['img_date'])\n",
    "    \n",
    "    # Filter temporal window\n",
    "    mask = (gdf['img_date'] >= temporal_window[0]) & (gdf['img_date'] <= temporal_window[1])\n",
    "    filtered_gdf = gdf[mask].copy()\n",
    "    \n",
    "    # Calculate areas\n",
    "    filtered_gdf = filtered_gdf.to_crs('EPSG:32736')\n",
    "    filtered_gdf['area_ha'] = filtered_gdf.geometry.area / 10000\n",
    "    \n",
    "    # Define size categories\n",
    "    filtered_gdf['size_category'] = pd.cut(\n",
    "        filtered_gdf['area_ha'],\n",
    "        bins=[0, 0.60, 2.35, float('inf')],\n",
    "        labels=['small', 'medium', 'large']\n",
    "    )\n",
    "    \n",
    "    # Calculate available events per category\n",
    "    available = filtered_gdf['size_category'].value_counts()\n",
    "    print(\"Available events per category:\")\n",
    "    print(available)\n",
    "    \n",
    "    # Define target samples (adjusted based on availability)\n",
    "    target_samples = {\n",
    "        'small': min(50, available['small']),\n",
    "        'medium': min(15, available['medium']),\n",
    "        'large': min(7, available.get('large', 0))  # Handle case if no large events\n",
    "    }\n",
    "    \n",
    "    print(\"\\nTarget samples per category:\")\n",
    "    print(target_samples)\n",
    "    \n",
    "    # Sample from each category\n",
    "    sampled_events = pd.DataFrame()\n",
    "    \n",
    "    for category, target in target_samples.items():\n",
    "        if target > 0:\n",
    "            stratum = filtered_gdf[filtered_gdf['size_category'] == category]\n",
    "            # Ensure even temporal distribution within each category\n",
    "            sampled = stratum.groupby(stratum['img_date'].dt.to_period('M')).apply(\n",
    "                lambda x: x.sample(\n",
    "                    n=min(max(1, target // len(stratum['img_date'].dt.to_period('M').unique())),\n",
    "                        len(x)),\n",
    "                    random_state=42\n",
    "                )\n",
    "            ).reset_index(drop=True)\n",
    "            \n",
    "            # If we still need more samples, take them randomly from the remaining events\n",
    "            if len(sampled) < target:\n",
    "                remaining = stratum[~stratum.index.isin(sampled.index)]\n",
    "                if len(remaining) > 0:\n",
    "                    additional = remaining.sample(\n",
    "                        n=min(target - len(sampled), len(remaining)),\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    sampled = pd.concat([sampled, additional])\n",
    "            \n",
    "            sampled_events = pd.concat([sampled_events, sampled])\n",
    "    \n",
    "    # Convert to GeoDataFrame and prepare for saving\n",
    "    sampled_events = gpd.GeoDataFrame(sampled_events, geometry='geometry')\n",
    "    sampled_events = sampled_events.to_crs(4326)\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = Path(output_dir).resolve()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = output_dir / f\"sampled_events_{timestamp}.geojson\"\n",
    "    sampled_events.to_file(output_path, driver='GeoJSON')\n",
    "\n",
    "    # Print detailed statistics\n",
    "    print(\"\\nFinal sample distribution:\")\n",
    "    print(\"\\nBy size category:\")\n",
    "    print(sampled_events['size_category'].value_counts())\n",
    "    \n",
    "    print(\"\\nTemporal distribution:\")\n",
    "    print(sampled_events.groupby([\n",
    "        sampled_events['img_date'].dt.strftime('%Y-%m'),\n",
    "        'size_category'\n",
    "    ], observed=True).size().unstack(fill_value=0))\n",
    "\n",
    "    return sampled_events, output_path\n",
    "\n",
    "# Execute sampling with new parameters\n",
    "sampled_events_v3, saved_path = sample_deforestation_events_v3(\n",
    "    geojson_path=geojson_path,\n",
    "    output_dir=Path(\"../Datasets/Testing/Samples\").resolve(),\n",
    "    temporal_window=temporal_window\n",
    ")\n",
    "\n",
    "# Additional analysis of the results\n",
    "print(\"\\nArea statistics of sampled events:\")\n",
    "print(sampled_events_v3['area_ha'].describe())\n",
    "\n",
    "print(\"\\nDate distribution:\")\n",
    "print(sampled_events_v3['img_date'].dt.strftime('%Y-%m-%d').value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample_distribution(sampled_events):\n",
    "    # Create a figure with 2x2 subplots, but only use 3\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Adjust the layout to use only 3 plots\n",
    "    gs = plt.GridSpec(2, 2)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])  # top-left\n",
    "    ax2 = fig.add_subplot(gs[0, 1])  # top-right\n",
    "    ax3 = fig.add_subplot(gs[1, :])  # bottom, spanning both columns\n",
    "    \n",
    "    # Size category distribution\n",
    "    sns.countplot(data=sampled_events, x='size_category', ax=ax1)\n",
    "    ax1.set_title('Distribution by Size Category')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    # Temporal distribution\n",
    "    sampled_events['date'] = pd.to_datetime(sampled_events['img_date']).dt.date\n",
    "    sns.histplot(data=sampled_events, x='date', ax=ax2)\n",
    "    ax2.set_title('Temporal Distribution')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Area distribution - make this plot wider\n",
    "    sns.boxplot(data=sampled_events, y='area_ha', x='size_category', ax=ax3)\n",
    "    ax3.set_title('Area Distribution by Category')\n",
    "    ax3.set_ylabel('Area (hectares)')\n",
    "    \n",
    "    # Save metadata\n",
    "    sample_metadata = {\n",
    "        'total_samples': len(sampled_events),\n",
    "        'temporal_range': {\n",
    "            'start': str(sampled_events['img_date'].min()),\n",
    "            'end': str(sampled_events['img_date'].max())\n",
    "        },\n",
    "        'size_distribution': sampled_events['size_category'].value_counts().to_dict(),\n",
    "        'area_statistics': {\n",
    "            'mean': float(sampled_events['area_ha'].mean()),\n",
    "            'median': float(sampled_events['area_ha'].median()),\n",
    "            'std': float(sampled_events['area_ha'].std())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_dir = Path(\"../Docs/Diagrams/\")\n",
    "    with open(output_dir / 'sample_metadata.json', 'w') as f:\n",
    "        json.dump(sample_metadata, f, indent=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'sample_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "# Generate visualizations\n",
    "visualize_sample_distribution(sampled_events_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentinelPatchProcessor:\n",
    "    def __init__(self, patch_size=224):\n",
    "        \"\"\"\n",
    "        Initialize the Sentinel-2 patch processor.\n",
    "        \n",
    "        Args:\n",
    "            patch_size (int): Size of the output patches (default: 224)\n",
    "        \"\"\"\n",
    "        self.patch_size = patch_size\n",
    "        # logging.basicConfig(\n",
    "        #     level=logging.INFO,\n",
    "        #     filename='processing.log',\n",
    "        #     format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        # )\n",
    "        \n",
    "    def get_tile_id(self, sentinel_path):\n",
    "        \"\"\"Extract tile ID from Sentinel path\"\"\"\n",
    "        match = re.search(r'T\\d{2}[A-Z]{3}', str(sentinel_path))\n",
    "        return match.group(0) if match else None\n",
    "        \n",
    "    def get_tile_bounds(self, sentinel_path):\n",
    "        \"\"\"Get the geographical bounds of a Sentinel tile\"\"\"\n",
    "        sample_band = next(Path(sentinel_path).glob('GRANULE/*/IMG_DATA/*B02.jp2'))\n",
    "        with rasterio.open(sample_band) as src:\n",
    "            bounds = box(*src.bounds)\n",
    "        return bounds\n",
    "        \n",
    "    def group_geometries_by_tile(self, geojson_path, sentinel_path):\n",
    "        \"\"\"Group geometries based on which tile they intersect with\"\"\"\n",
    "        gdf = gpd.read_file(geojson_path)\n",
    "        tile_bounds = self.get_tile_bounds(sentinel_path)\n",
    "        tile_id = self.get_tile_id(sentinel_path)\n",
    "        \n",
    "        # Transform geometries to tile CRS if needed\n",
    "        with rasterio.open(next(Path(sentinel_path).glob('GRANULE/*/IMG_DATA/*B02.jp2'))) as src:\n",
    "            if gdf.crs != src.crs:\n",
    "                gdf = gdf.to_crs(src.crs)\n",
    "        \n",
    "        # Filter geometries that intersect with this tile\n",
    "        mask = gdf.geometry.intersects(tile_bounds)\n",
    "        tile_geometries = gdf[mask].copy()\n",
    "        \n",
    "        # Clip geometries to tile bounds\n",
    "        tile_geometries['geometry'] = tile_geometries.geometry.intersection(tile_bounds)\n",
    "        \n",
    "        return tile_geometries if not tile_geometries.empty else None\n",
    "\n",
    "    def load_bands(self, sentinel_path):\n",
    "        \"\"\"Load and stack Sentinel-2 bands\"\"\"\n",
    "        band_paths = list(Path(sentinel_path).glob('GRANULE/*/IMG_DATA/*.jp2'))\n",
    "        band_data = {}\n",
    "        required_bands = ['B02', 'B03', 'B04', 'B08', 'B8A', 'B11', 'B12']\n",
    "        \n",
    "        for band_path in band_paths:\n",
    "            band_name = re.search(r'B\\d{2}|B8A', band_path.name)\n",
    "            if band_name:\n",
    "                band_name = band_name.group(0)\n",
    "                if band_name in required_bands:\n",
    "                    try:\n",
    "                        with rasterio.open(band_path) as src:\n",
    "                            band_data[band_name] = src.read(1)\n",
    "                            if band_name == 'B02':\n",
    "                                self.meta = src.meta.copy()\n",
    "                            logging.info(f\"Loaded band {band_name} from {self.get_tile_id(sentinel_path)}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error loading {band_name} from {sentinel_path}: {str(e)}\")\n",
    "        \n",
    "        if len(band_data) != len(required_bands):\n",
    "            missing_bands = set(required_bands) - set(band_data.keys())\n",
    "            logging.error(f\"Missing required bands for {self.get_tile_id(sentinel_path)}: {missing_bands}\")\n",
    "            return None\n",
    "            \n",
    "        return band_data\n",
    "\n",
    "    def validate_bands(self, band_data, required_bands):\n",
    "        \"\"\"Validate that all required bands are present\"\"\"\n",
    "        missing_bands = [band for band in required_bands if band not in band_data]\n",
    "        if missing_bands:\n",
    "            raise ValueError(f\"Missing required bands: {missing_bands}\")\n",
    "\n",
    "    def resample_to_10m(self, band_data):\n",
    "        \"\"\"Resample all bands to 10m resolution\"\"\"\n",
    "        try:\n",
    "            # Get shape from a 10m band (B02)\n",
    "            target_shape = band_data['B02'].shape\n",
    "            logging.debug(f\"Target shape for resampling: {target_shape}\")\n",
    "            \n",
    "            # Bands that need resampling (20m bands)\n",
    "            bands_to_resample = ['B8A', 'B11', 'B12']\n",
    "            \n",
    "            for band in bands_to_resample:\n",
    "                if band in band_data and band_data[band].shape != target_shape:\n",
    "                    logging.info(f\"Resampling {band} to 10m resolution\")\n",
    "                    band_data[band] = self._resample_array(\n",
    "                        band_data[band],\n",
    "                        target_shape\n",
    "                    )\n",
    "                    logging.debug(f\"Resampled {band} shape: {band_data[band].shape}\")\n",
    "                    \n",
    "            return band_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in resample_to_10m: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _resample_array(self, array, target_shape):\n",
    "        \"\"\"Helper function to resample arrays using bilinear interpolation\"\"\"\n",
    "        try:\n",
    "            # Convert array to PIL Image for resampling\n",
    "            img = Image.fromarray(array)\n",
    "            \n",
    "            # Resize to target shape (note the order: width, height)\n",
    "            resized = img.resize(\n",
    "                (target_shape[1], target_shape[0]),  # PIL uses (width, height)\n",
    "                resample=Image.BILINEAR\n",
    "            )\n",
    "            \n",
    "            # Convert back to numpy array\n",
    "            return np.array(resized)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in _resample_array: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def compute_indices(self, band_data):\n",
    "        \"\"\"\n",
    "        Compute NDVI and NDMI indices from Sentinel-2 bands with safe division\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate required bands\n",
    "            required_bands = ['B04', 'B08', 'B8A', 'B11']\n",
    "            self.validate_bands(band_data, required_bands)\n",
    "\n",
    "            # Calculate NDVI safely\n",
    "            nir_red_sum = band_data['B08'] + band_data['B04']\n",
    "            nir_red_diff = band_data['B08'] - band_data['B04']\n",
    "\n",
    "            # Use np.divide with where condition to handle zeros\n",
    "            ndvi = np.divide(\n",
    "                nir_red_diff, \n",
    "                nir_red_sum, \n",
    "                out=np.zeros_like(nir_red_diff, dtype=np.float32),\n",
    "                where=nir_red_sum != 0\n",
    "            )\n",
    "\n",
    "            # Calculate NDMI safely\n",
    "            nir_swir_sum = band_data['B8A'] + band_data['B11']\n",
    "            nir_swir_diff = band_data['B8A'] - band_data['B11']\n",
    "\n",
    "            # Use np.divide with where condition to handle zeros\n",
    "            ndmi = np.divide(\n",
    "                nir_swir_diff, \n",
    "                nir_swir_sum, \n",
    "                out=np.zeros_like(nir_swir_diff, dtype=np.float32),\n",
    "                where=nir_swir_sum != 0\n",
    "            )\n",
    "            \n",
    "            # Add bounds to prevent extreme values\n",
    "            ndvi = np.clip(ndvi, -1, 1)\n",
    "            ndmi = np.clip(ndmi, -1, 1)\n",
    "\n",
    "            # Replace NaN values with 0\n",
    "            ndvi = np.nan_to_num(ndvi, nan=0.0)\n",
    "            ndmi = np.nan_to_num(ndmi, nan=0.0)\n",
    "\n",
    "            logging.info(f\"Successfully computed NDVI and NDMI indices\")\n",
    "            logging.debug(f\"NDVI range: [{ndvi.min():.3f}, {ndvi.max():.3f}]\")\n",
    "            logging.debug(f\"NDMI range: [{ndmi.min():.3f}, {ndmi.max():.3f}]\")\n",
    "            \n",
    "            return ndvi, ndmi\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error computing indices: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_patches(self, stacked_bands, geometries, output_dir):\n",
    "        \"\"\"Create and save image patches for each geometry\"\"\"\n",
    "        try:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            for idx, geometry in geometries.iterrows():\n",
    "                try:\n",
    "                    bounds = geometry.geometry.bounds\n",
    "                    window = from_bounds(*bounds, transform=self.meta['transform'])\n",
    "\n",
    "                    patch = stacked_bands[\n",
    "                        :,\n",
    "                        int(window.row_off):int(window.row_off + self.patch_size),\n",
    "                        int(window.col_off):int(window.col_off + self.patch_size)\n",
    "                    ]\n",
    "\n",
    "                    if patch.shape[1:] == (self.patch_size, self.patch_size):\n",
    "                        output_path = Path(output_dir) / f\"patch_{idx}.npy\"\n",
    "                        np.save(output_path, patch)\n",
    "                        logging.info(f\"Saved patch {idx} to {output_path}\")\n",
    "                    else:\n",
    "                        logging.warning(f\"Skipping patch {idx} due to incorrect size: {patch.shape[1:]}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing patch {idx}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in create_patches: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process_imagery(self, sentinel_path, geojson_path, output_dir):\n",
    "        \"\"\"Process imagery considering tile boundaries\"\"\"\n",
    "        try:\n",
    "            tile_id = self.get_tile_id(sentinel_path)\n",
    "            if not tile_id:\n",
    "                logging.error(f\"Could not determine tile ID for {sentinel_path}\")\n",
    "                return\n",
    "\n",
    "            # Group geometries by tile\n",
    "            tile_geometries = self.group_geometries_by_tile(geojson_path, sentinel_path)\n",
    "            if tile_geometries is None:\n",
    "                logging.info(f\"No geometries intersect with tile {tile_id}\")\n",
    "                return\n",
    "\n",
    "            # Create output directory for this tile\n",
    "            tile_output_dir = Path(output_dir) / tile_id\n",
    "            os.makedirs(tile_output_dir, exist_ok=True)\n",
    "\n",
    "            # Load and process bands\n",
    "            band_data = self.load_bands(sentinel_path)\n",
    "            if band_data is None:\n",
    "                return\n",
    "\n",
    "            band_data = self.resample_to_10m(band_data)\n",
    "            ndvi, ndmi = self.compute_indices(band_data)\n",
    "            \n",
    "            stacked_bands = np.stack([\n",
    "                band_data['B02'], band_data['B03'], band_data['B04'],\n",
    "                band_data['B08'], band_data['B8A'], band_data['B11'],\n",
    "                band_data['B12'], ndvi, ndmi\n",
    "            ])\n",
    "\n",
    "            self.create_patches(stacked_bands, tile_geometries, tile_output_dir)\n",
    "            logging.info(f\"Successfully processed tile {tile_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing tile {tile_id}: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "processor = SentinelPatchProcessor(patch_size=224)\n",
    "\n",
    "# Get all .SAFE directories\n",
    "base_path = Path(\"../Datasets/Sentinel-2\")\n",
    "safe_dirs = list(base_path.glob(\"*/*.SAFE\"))\n",
    "\n",
    "# Process each Sentinel-2 scene\n",
    "for safe_dir in tqdm(safe_dirs, desc=\"Processing Sentinel-2 images\"):\n",
    "    try:\n",
    "        processor.process_imagery(\n",
    "            sentinel_path=str(safe_dir),\n",
    "            geojson_path=saved_path,\n",
    "            output_dir=f\"../Datasets/Testing/Tiles/{safe_dir.parent.name}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process {safe_dir}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_patch(patch_path):\n",
    "    \"\"\"Visualize a saved patch with all its bands and indices\"\"\"\n",
    "    # Load the patch\n",
    "    patch = np.load(patch_path)\n",
    "    \n",
    "    # Define band names for labeling\n",
    "    band_names = ['B02', 'B03', 'B04', 'B08', 'B8A', 'B11', 'B12', 'NDVI', 'NDMI']\n",
    "    \n",
    "    # Create a figure with subplots for each band/index\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    fig.suptitle(f'Patch Visualization: {patch_path.name}', fontsize=16)\n",
    "    \n",
    "    # Plot each band/index\n",
    "    for idx, (ax, name) in enumerate(zip(axes.flat, band_names)):\n",
    "        im = ax.imshow(patch[idx], cmap='viridis')\n",
    "        ax.set_title(name)\n",
    "        plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage:\n",
    "tiles_dir = Path(\"../Datasets/Testing/Tiles\")\n",
    "patch_files = list(tiles_dir.glob(\"**/patch_*.npy\"))\n",
    "\n",
    "# Visualize first few patches\n",
    "for patch_file in patch_files[:3]:  # Adjust number of patches to display\n",
    "    fig = visualize_patch(patch_file)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rgb_composite(patch_path):\n",
    "    \"\"\"Show true color RGB composite\"\"\"\n",
    "    patch = np.load(patch_path)\n",
    "    \n",
    "    # Use B04 (Red), B03 (Green), B02 (Blue) for true color\n",
    "    rgb = np.stack([patch[2], patch[1], patch[0]], axis=-1)\n",
    "    \n",
    "    # Normalize for display\n",
    "    rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(rgb)\n",
    "    plt.title(f'RGB Composite: {patch_path.name}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "show_rgb_composite(patch_files[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_temporal_patches(patch_number=9):\n",
    "    \"\"\"Visualize the same patch location across different dates\"\"\"\n",
    "    tiles_dir = Path(\"../Datasets/Testing/Tiles\")\n",
    "    patch_files = list(tiles_dir.glob(f\"**/patch_{patch_number}.npy\"))\n",
    "    \n",
    "    # Extract date using regex from the full directory path\n",
    "    def get_capture_date(filepath):\n",
    "        # Pattern matches YYYYMMDD from the directory name\n",
    "        match = re.search(r'_(\\d{8})T', str(filepath))\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            # Convert to readable format\n",
    "            return f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}\"\n",
    "        return None\n",
    "    \n",
    "    # Sort by capture date\n",
    "    patch_files.sort(key=get_capture_date)\n",
    "    \n",
    "    n_patches = len(patch_files)\n",
    "    fig, axes = plt.subplots(n_patches, 1, figsize=(15, 5*n_patches))\n",
    "    \n",
    "    for idx, patch_file in enumerate(patch_files):\n",
    "        patch = np.load(patch_file)\n",
    "        rgb = np.stack([patch[2], patch[1], patch[0]], axis=-1)\n",
    "        \n",
    "        # Robust normalization\n",
    "        rgb_min = rgb.min()\n",
    "        rgb_max = rgb.max()\n",
    "        if rgb_max > rgb_min:\n",
    "            rgb = (rgb - rgb_min) / (rgb_max - rgb_min)\n",
    "        else:\n",
    "            rgb = np.zeros_like(rgb)\n",
    "        \n",
    "        # Get and display the actual capture date\n",
    "        capture_date = get_capture_date(patch_file)\n",
    "        axes[idx].imshow(rgb)\n",
    "        axes[idx].set_title(f'Capture Date: {capture_date}')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display temporal sequence\n",
    "visualize_temporal_patches(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudProcessor:\n",
    "    def __init__(self, base_path: str, cloud_threshold: float = 0.7):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.cloud_threshold = cloud_threshold\n",
    "        self.s2cloudless_bands = ['B01', 'B02', 'B04', 'B05', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12']\n",
    "        self.cloud_detector = S2PixelCloudDetector(threshold=0.4)\n",
    "        \n",
    "        # Verify base path exists\n",
    "        if not self.base_path.exists():\n",
    "            raise ValueError(f\"Base path does not exist: {self.base_path}\")\n",
    "\n",
    "    def get_safe_dir(self, patch_file: Path) -> Path:\n",
    "        \"\"\"Get corresponding SAFE directory for a patch\"\"\"\n",
    "        # Extract product name from patch path\n",
    "        # Example: .../S2B_MSIL1C_20190611T083609_N0207_R064_T36UYA_20190611T122426/...\n",
    "        product_name = patch_file.parent.parent.name\n",
    "        \n",
    "        # Search for matching SAFE directory\n",
    "        safe_pattern = f\"{product_name}/*.SAFE\"\n",
    "        safe_dirs = list(self.base_path.glob(safe_pattern))\n",
    "        \n",
    "        if not safe_dirs:\n",
    "            # Try searching one level deeper\n",
    "            safe_dirs = list(self.base_path.glob(f\"*/{safe_pattern}\"))\n",
    "            \n",
    "        if not safe_dirs:\n",
    "            raise ValueError(f\"No SAFE directory found for product: {product_name}\")\n",
    "            \n",
    "        return safe_dirs[0]\n",
    "\n",
    "    def get_patch_window(self, patch_file: Path) -> rasterio.windows.Window:\n",
    "        \"\"\"Get window for a patch\"\"\"\n",
    "        # For now, return a fixed window size since patches are 224x224\n",
    "        return rasterio.windows.Window(0, 0, 224, 224)\n",
    "\n",
    "    def load_bands_for_window(self, safe_dir: Path, window: rasterio.windows.Window) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Load required bands for cloud detection\"\"\"\n",
    "        bands = {}\n",
    "        for band in self.s2cloudless_bands:\n",
    "            try:\n",
    "                # Search for band file\n",
    "                band_files = list(safe_dir.glob(f'GRANULE/*/IMG_DATA/*{band}.jp2'))\n",
    "                if not band_files:\n",
    "                    raise FileNotFoundError(f\"Band {band} not found in {safe_dir}\")\n",
    "                    \n",
    "                band_file = band_files[0]\n",
    "                \n",
    "                with rasterio.open(band_file) as src:\n",
    "                    # Read band data\n",
    "                    band_data = src.read(1, window=window)\n",
    "                    \n",
    "                    # Resample if needed\n",
    "                    if src.res[0] != 10:\n",
    "                        target_shape = (224, 224)\n",
    "                        band_data = self.resample_band(band_data, target_shape)\n",
    "                    \n",
    "                    bands[band] = band_data\n",
    "                    \n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Error loading band {band}: {str(e)}\")\n",
    "                \n",
    "        return bands\n",
    "\n",
    "    def resample_band(self, band: np.ndarray, target_shape: Tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"Resample band to target shape using PIL\"\"\"\n",
    "        from PIL import Image\n",
    "        img = Image.fromarray(band)\n",
    "        resized = img.resize(target_shape, Image.BILINEAR)\n",
    "        return np.array(resized)\n",
    "\n",
    "    def detect_clouds(self, bands: Dict[str, np.ndarray]) -> Tuple[bool, float]:\n",
    "        \"\"\"Run cloud detection on band data\"\"\"\n",
    "        # Stack bands in required order\n",
    "        stacked = np.stack([bands[b] for b in self.s2cloudless_bands]) / 10000.0\n",
    "        \n",
    "        # Reshape for s2cloudless\n",
    "        data = np.expand_dims(np.moveaxis(stacked, 0, -1), axis=0)\n",
    "        \n",
    "        # Get cloud probabilities\n",
    "        cloud_probs = self.cloud_detector.get_cloud_probability_maps(data)  # Correct method name\n",
    "        cloud_percentage = float(np.mean(cloud_probs > self.cloud_detector.threshold))\n",
    "        \n",
    "        return cloud_percentage <= self.cloud_threshold, cloud_percentage\n",
    "\n",
    "    def process_patches(self, input_dir: str, output_dir: str):\n",
    "        \"\"\"Process all patches\"\"\"\n",
    "        input_path = Path(input_dir)\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # Get all patches\n",
    "        patches = list(input_path.rglob(\"patch_*.npy\"))\n",
    "        \n",
    "        results = {\n",
    "            'total': len(patches),\n",
    "            'processed': 0,\n",
    "            'kept': 0,\n",
    "            'errors': 0\n",
    "        }\n",
    "        \n",
    "        for patch_file in tqdm(patches, desc=\"Processing patches\"):\n",
    "            try:\n",
    "                # Get original SAFE directory\n",
    "                safe_dir = self.get_safe_dir(patch_file)\n",
    "                \n",
    "                # Get window\n",
    "                window = self.get_patch_window(patch_file)\n",
    "                \n",
    "                # Load bands and detect clouds\n",
    "                bands = self.load_bands_for_window(safe_dir, window)\n",
    "                keep_patch, cloud_pct = self.detect_clouds(bands)\n",
    "                \n",
    "                results['processed'] += 1\n",
    "                \n",
    "                if keep_patch:\n",
    "                    # Create output directory structure\n",
    "                    out_file = output_path / patch_file.relative_to(input_path)\n",
    "                    out_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "                    \n",
    "                    # Copy patch to output\n",
    "                    patch_data = np.load(patch_file)\n",
    "                    np.save(out_file, patch_data)\n",
    "                    results['kept'] += 1\n",
    "                    \n",
    "                logging.info(f\"Processed {patch_file.name}: {cloud_pct:.1%} clouds, kept: {keep_patch}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                results['errors'] += 1\n",
    "                logging.error(f\"Error processing {patch_file}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Print summary\n",
    "        logging.info(f\"\\nProcessing Summary:\")\n",
    "        logging.info(f\"Total patches: {results['total']}\")\n",
    "        logging.info(f\"Successfully processed: {results['processed']}\")\n",
    "        logging.info(f\"Kept (low clouds): {results['kept']}\")\n",
    "        logging.info(f\"Errors: {results['errors']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set base path\n",
    "base_path = Path(\"../Datasets/Sentinel-2\")\n",
    "\n",
    "try:\n",
    "    # Initialize processor\n",
    "    processor = CloudProcessor(\n",
    "        base_path=base_path,\n",
    "        cloud_threshold=0.7\n",
    "    )\n",
    "\n",
    "    processor.process_patches(\n",
    "        input_dir=\"../Datasets/Testing/Tiles\",\n",
    "        output_dir=\"../Datasets/Testing/Processed\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logging.error(f\"Processing failed: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP1252-MP(GPU)",
   "language": "python",
   "name": "comp1252-mp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
