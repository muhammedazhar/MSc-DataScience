{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import h5py\n",
    "\n",
    "# Create a directory for intermediate storage\n",
    "os.makedirs('../Datasets/Testing/Processed/HDF5-TemporalPairs', exist_ok=True)\n",
    "\n",
    "# Process in smaller chunks and save to HDF5\n",
    "with h5py.File('../Datasets/Testing/Processed/HDF5-TemporalPairs/pairs.h5', 'w') as f:\n",
    "    # Create datasets\n",
    "    f.create_dataset('pairs', shape=(0, 2, *patches[0].shape), maxshape=(None, 2, *patches[0].shape), chunks=True)\n",
    "    f.create_dataset('labels', shape=(0,), maxshape=(None,), dtype=bool)\n",
    "    \n",
    "    pair_count = 0\n",
    "    chunk_size = 10  # Process 10 pairs at a time\n",
    "    \n",
    "    for i in range(0, len(patches)-1, chunk_size):\n",
    "        chunk_pairs = []\n",
    "        chunk_labels = []\n",
    "        \n",
    "        chunk_end = min(i + chunk_size, len(patches)-1)\n",
    "        for j in range(i, chunk_end):\n",
    "            date1 = patch_dates[j]\n",
    "            for k in range(j+1, len(patches)):\n",
    "                date2 = patch_dates[k]\n",
    "                if (date2 - date1).astype('timedelta64[D]').astype(int) <= 30:\n",
    "                    chunk_pairs.append([patches[j], patches[k]])\n",
    "                    has_event = any(date1 <= event_date <= date2 for event_date in load_geojson_dates())\n",
    "                    chunk_labels.append(has_event)\n",
    "        \n",
    "        if chunk_pairs:\n",
    "            # Resize datasets\n",
    "            new_size = pair_count + len(chunk_pairs)\n",
    "            f['pairs'].resize(new_size, axis=0)\n",
    "            f['labels'].resize(new_size, axis=0)\n",
    "            \n",
    "            # Store chunk\n",
    "            f['pairs'][pair_count:new_size] = chunk_pairs\n",
    "            f['labels'][pair_count:new_size] = chunk_labels\n",
    "            \n",
    "            pair_count = new_size\n",
    "\n",
    "print(f\"Total pairs saved: {pair_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create temporal pairs\n",
    "temporal_pairs = []\n",
    "labels = []\n",
    "max_time_diff = 30  # Maximum days between image pairs\n",
    "\n",
    "for i, (date1, patch1) in enumerate(zip(patch_dates[:-1], patches[:-1])):\n",
    "    for j, (date2, patch2) in enumerate(zip(patch_dates[i+1:], patches[i+1:]), i+1):\n",
    "        time_diff = (date2 - date1).astype('timedelta64[D]').astype(int)\n",
    "        if time_diff <= max_time_diff:\n",
    "            temporal_pairs.append((patch1, patch2))\n",
    "            # Binary label: 1 if deforestation event exists between dates\n",
    "            has_event = any(date1 <= event_date <= date2 for event_date in load_geojson_dates())\n",
    "            labels.append(has_event)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(temporal_pairs)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split data: 70% train, 15% validation, 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training pairs: {len(X_train)}\")\n",
    "print(f\"Validation pairs: {len(X_val)}\")\n",
    "print(f\"Testing pairs: {len(X_test)}\")\n",
    "print(f\"Positive samples: {sum(y)}/{len(y)} ({sum(y)/len(y)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def load_geojson_dates(print_loading=False):\n",
    "    # Load the most recent sampled events file\n",
    "    sample_files = glob('../Datasets/Testing/Samples/*.geojson')\n",
    "    if not sample_files:\n",
    "        raise FileNotFoundError(\"No .geojson files found in Testing/Samples/\")\n",
    "    latest_file = max(sample_files, key=os.path.getctime)\n",
    "    if print_loading == True:\n",
    "        print(f\"Loading events from {latest_file}\")\n",
    "\n",
    "    with open(latest_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract dates and convert to datetime objects\n",
    "    event_dates = []\n",
    "    for feature in data['features']:\n",
    "        date_str = feature['properties']['img_date']\n",
    "        try:\n",
    "            event_date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "            event_dates.append(event_date)\n",
    "        except ValueError:\n",
    "            print(f\"Date format error in {date_str}\")\n",
    "            # You can choose to skip or handle the error as needed\n",
    "            continue\n",
    "\n",
    "    return sorted(event_dates)\n",
    "\n",
    "def get_tile_date(patch_file_path):\n",
    "    # Extract date from Sentinel-2 tile path\n",
    "    tile_name = Path(patch_file_path).parent.parent.name\n",
    "    parts = tile_name.split('_')\n",
    "    if len(parts) < 3:\n",
    "        raise ValueError(f\"Unexpected tile name format: {tile_name}\")\n",
    "    date_str = parts[2][:8]\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%Y%m%d')\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Invalid date format in tile name: {date_str}\")\n",
    "\n",
    "\n",
    "def load_and_sort_patches():\n",
    "    patches = []\n",
    "    patch_dates = []\n",
    "\n",
    "    # Use pathlib for better path handling\n",
    "    base_path = Path('../Datasets/Testing/Tiles')\n",
    "    print(f\"Looking for tiles in: {base_path}\")\n",
    "    \n",
    "    tile_dirs = list(base_path.glob('S2*'))\n",
    "    \n",
    "    if not tile_dirs:\n",
    "        raise FileNotFoundError(f\"No tile directories found in {base_path}\")\n",
    "\n",
    "    print(f\"Found {len(tile_dirs)} tile directories\")\n",
    "\n",
    "    for tile_dir in tile_dirs:\n",
    "        print(f\"\\nProcessing directory: {tile_dir}\")\n",
    "        patch_files = []\n",
    "        \n",
    "        # Find both single and multi-patch files\n",
    "        # Pattern 1: PLOT-XXXXX.npy (single patches)\n",
    "        patch_files.extend(list(tile_dir.rglob('*.npy')))\n",
    "        \n",
    "        if not patch_files:\n",
    "            print(f\"No patches found in {tile_dir}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Found {len(patch_files)} patches in {tile_dir}\")\n",
    "\n",
    "        # Print first few file paths to verify\n",
    "        print(\"Sample file paths:\")\n",
    "        for f in patch_files[:3]:\n",
    "            print(f\"  {f}\")\n",
    "\n",
    "        # Custom sorting function for plot files\n",
    "        def sort_key(x):\n",
    "            filename = x.stem  # Get filename without extension\n",
    "            if '_P' in filename:\n",
    "                # For multi-patch files (PLOT-XXXXX_PX)\n",
    "                base_num = int(filename.split('-')[1].split('_')[0])\n",
    "                patch_num = int(filename.split('_P')[1])\n",
    "                return (base_num, patch_num)\n",
    "            else:\n",
    "                # For single patch files (PLOT-XXXXX)\n",
    "                return (int(filename.split('-')[1]), 0)\n",
    "\n",
    "        for patch_file in sorted(patch_files, key=sort_key):\n",
    "            try:\n",
    "                patch = np.load(patch_file)\n",
    "                patch_date = get_tile_date(patch_file)\n",
    "                print(f\"Loaded patch from {patch_file} with date {patch_date}\")\n",
    "                patch_dates.append(patch_date)\n",
    "                patches.append(patch)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {patch_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if not patches:\n",
    "        print(\"\\nDebug information:\")\n",
    "        print(f\"Total tile directories found: {len(tile_dirs)}\")\n",
    "        print(f\"Tile directory paths:\")\n",
    "        for td in tile_dirs:\n",
    "            print(f\"  {td}\")\n",
    "        raise RuntimeError(\"No patches loaded successfully.\")\n",
    "\n",
    "    # Convert patch_dates to numpy datetime64 for sorting\n",
    "    patch_dates_np = np.array(patch_dates, dtype='datetime64')\n",
    "    patches_np = np.array(patches)\n",
    "\n",
    "    # Sort patches by date\n",
    "    sorted_indices = np.argsort(patch_dates_np)\n",
    "    patches_sorted = patches_np[sorted_indices]\n",
    "    patch_dates_sorted = patch_dates_np[sorted_indices]\n",
    "\n",
    "    print(f\"\\nTotal patches loaded: {len(patches_sorted)}\")\n",
    "    return patches_sorted, patch_dates_sorted\n",
    "\n",
    "event_dates = load_geojson_dates(print_loading=True)\n",
    "base_path = Path('../Datasets/Testing/Tiles')\n",
    "tile_dirs = list(base_path.glob('S2*'))\n",
    "patches, patch_dates = load_and_sort_patches()\n",
    "# Example processing: print the number of patches and first few dates\n",
    "print(\"First 5 patch dates:\")\n",
    "for date in patch_dates[:5]:\n",
    "    print(date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP1252-MP(GPU)",
   "language": "python",
   "name": "comp1252-mp"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
