{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with NLP and Machine Learning Libraries\n",
    "\n",
    "This Jupyter notebook code snippet demonstrates the initial setup for a text classification project utilizing both Natural Language Processing (NLP) and Machine Learning (ML) libraries. The code is structured to import necessary libraries, prepare for data preprocessing, and set up a classification model. Below are the key components highlighted:\n",
    "\n",
    "## Importing Libraries and Initializing NLTK Resources\n",
    "\n",
    "### Overview\n",
    "This section focuses on setting up the environment by importing necessary libraries for NLP and machine learning tasks, specifically for text classification. It also includes a custom function to download NLTK resources if they're not already present, ensuring all dependencies are satisfied before proceeding with data processing and modeling.\n",
    "\n",
    "### Key Points\n",
    "- Libraries such as `numpy` NumPy, `pandas`, `matplotlib`, `re` (regular expression), `string`, `nltk` NLTK (Natural Language Toolkit), `sklearn` and `imbalanced-learn` are imported for various tasks including data manipulation, visualization, text processing, and handling class imbalance.\n",
    "- The custom function `download_nltk_resources` automates the downloading of essential NLTK resources like 'punkt' (for tokenizing), 'stopwords', and 'wordnet'.\n",
    "- Execution of `download_nltk_resources` ensures necessary resources are available for text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports necessary NLP and ML libraries for text classification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Importing necessary libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as IMBPipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Function to download necessary NLTK resources\n",
    "def download_nltk_resources():\n",
    "    resources = ['punkt', 'stopwords', 'wordnet']\n",
    "    for resource in resources:\n",
    "        try:\n",
    "            nltk.data.find(f'tokenizers/{resource}')\n",
    "        except LookupError:\n",
    "            nltk.download(resource)\n",
    "# This code is copied from my previous college project. It is called Lazy Loading NLTK Resources.\n",
    "\n",
    "# Call the function to download resources if not already present\n",
    "download_nltk_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Initial Data Check\n",
    "\n",
    "### Overview\n",
    "This segment deals with loading the dataset from a CSV file, followed by a basic check to confirm successful loading. It employs pandas for reading data and provides a quick overview of the dataset's size.\n",
    "\n",
    "### Key Points\n",
    "- Data is loaded from 'dataset.csv' using specific columns of interest.\n",
    "- Immediate feedback on the dataset's dimension is provided to confirm successful data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9347, 7) rows and columns loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv', usecols = ['par_id', 'paragraph', 'has_entity', 'lexicon_count', 'difficult_words', 'last_editor_gender', 'category'])\n",
    "size = df.shape\n",
    "if not df.empty: \n",
    "    print(size, \"rows and columns loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning: Handling Missing Values\n",
    "\n",
    "### Overview\n",
    "Focusing on data quality, this part involves checking for and removing any rows with missing values, ensuring the dataset's integrity for subsequent analysis.\n",
    "\n",
    "### Key Points\n",
    "- Calculates and prints the count of missing values per column.\n",
    "- Rows with missing values are removed to maintain data cleanliness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = df.shape\n",
    "print('Checking for missing values from', size)\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values, '\\n')\n",
    "\n",
    "# Remove rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "size = df.shape\n",
    "print('Removed missing values, Now it is', size)\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing Text Data\n",
    "\n",
    "### Overview\n",
    "Standardization efforts are concentrated on the 'category' and 'has_entity' columns. It involves converting text to lowercase and removing rows with specific unwanted values, enhancing consistency across textual data.\n",
    "\n",
    "### Key Points\n",
    "- Unique values in 'category' and 'has_entity' columns are inspected.\n",
    "- Text in the 'category' column is converted to lowercase to unify case usage.\n",
    "- Rows with 'data missing' in the 'has_entity' column are identified and excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for unique values in the category column\n",
      "['biographies' 'artificial intelligence' 'programming' 'philosophy'\n",
      " 'movies about artificial intelligence' 'Philosophy' nan 'Programming'\n",
      " 'Artificial intelligence' 'Biographies'\n",
      " 'Movies about artificial intelligence']\n",
      "\n",
      "Fixed the case of the category column\n",
      "['biographies' 'artificial intelligence' 'programming' 'philosophy'\n",
      " 'movies about artificial intelligence' nan]\n"
     ]
    }
   ],
   "source": [
    "print('Checking for unique values in the category column')\n",
    "category_counts = df['category'].unique()\n",
    "print(category_counts)\n",
    "# Convert the 'category' column to lowercase\n",
    "df['category'] = df['category'].str.lower()\n",
    "\n",
    "print('\\n' + 'Fixed the case of the category column')\n",
    "category_counts = df['category'].unique()\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for unique values in the has_entity column\n",
      "['ORG_YES_PRODUCT_NO_PERSON_YES_' 'ORG_YES_PRODUCT_NO_PERSON_NO_'\n",
      " 'ORG_NO_PRODUCT_YES_PERSON_NO_' 'ORG_YES_PRODUCT_YES_PERSON_YES_'\n",
      " 'ORG_NO_PRODUCT_NO_PERSON_NO_' 'ORG_NO_PRODUCT_YES_PERSON_YES_'\n",
      " 'ORG_NO_PRODUCT_NO_PERSON_YES_' 'ORG_YES_PRODUCT_YES_PERSON_NO_'\n",
      " 'data missing']\n",
      "\n",
      "Removed rows with \"data missing\" in the has_entity column\n",
      "['ORG_YES_PRODUCT_NO_PERSON_YES_' 'ORG_YES_PRODUCT_NO_PERSON_NO_'\n",
      " 'ORG_NO_PRODUCT_YES_PERSON_NO_' 'ORG_YES_PRODUCT_YES_PERSON_YES_'\n",
      " 'ORG_NO_PRODUCT_NO_PERSON_NO_' 'ORG_NO_PRODUCT_YES_PERSON_YES_'\n",
      " 'ORG_NO_PRODUCT_NO_PERSON_YES_' 'ORG_YES_PRODUCT_YES_PERSON_NO_']\n"
     ]
    }
   ],
   "source": [
    "print('Checking for unique values in the has_entity column')\n",
    "entity_counts = df['has_entity'].unique()\n",
    "print(entity_counts)\n",
    "\n",
    "# Removing rows with 'data missing' in the 'has_entity' column\n",
    "df = df[df['has_entity'] != 'data missing']\n",
    "\n",
    "print('\\n' + 'Removed rows with \"data missing\" in the has_entity column')\n",
    "entity_counts = df['has_entity'].unique()\n",
    "print(entity_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing for NLP\n",
    "\n",
    "### Overview\n",
    "Text data undergoes cleaning to remove punctuation, numbers, and extra whitespaces, preparing it for NLP tasks. The cleaned text replaces the original in a new column, preserving data integrity.\n",
    "\n",
    "### Key Points\n",
    "- The `clean_text` function is defined and applied to the 'paragraph' column, performing text cleaning operations.\n",
    "- Cleaned text is stored in a new column 'cleaned_paragraph', with an example shown for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original paragraph:\n",
      " Ramsay was born in Glasgow on 2 October 1852. He was a nephew of the geologist Sir Andrew Ramsay. His father, William, Sr., was a civil engineer. His mother was Catherine Robertson. He studied at Glasgow Academy, at the University of Glasgow and at University of Tubingen in Germany. \n",
      "\n",
      "Cleaned paragraph:\n",
      " Ramsay was born in Glasgow on October He was a nephew of the geologist Sir Andrew Ramsay His father William Sr was a civil engineer His mother was Catherine Robertson He studied at Glasgow Academy at the University of Glasgow and at University of Tubingen in Germany\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Remove punctuation marks and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply the 'clean_text' function to the 'paragraph' column\n",
    "unclean = df['paragraph']\n",
    "df['cleaned_paragraph'] = df['paragraph'].apply(clean_text)\n",
    "clean = df['cleaned_paragraph']\n",
    "print(\"Original paragraph:\\n\", unclean[0])\n",
    "print(\"\\nCleaned paragraph:\\n\", clean[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing Data and Feature Engineering\n",
    "\n",
    "### Overview\n",
    "This comprehensive section addresses data imbalance through resampling techniques and transforms textual data into numerical features using TF-IDF vectorization. It also visualizes category distributions before and after resampling.\n",
    "\n",
    "### Key Points\n",
    "- Imbalance in the category distribution is identified, and resampling (SMOTE for oversampling and RandomUnderSampler for undersampling) is applied within a pipeline.\n",
    "- The TF-IDF vectorizer is used to convert text data into a matrix of TF-IDF features.\n",
    "- Category distributions are compared visually using pie charts, showcasing the effect of resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the imbalanced category distribution\n",
    "print(\"Imbalanced Category Distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "\n",
    "# Encode the labels\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(df['category'])\n",
    "X_text = df['cleaned_paragraph'] # Assuming you have a column 'cleaned_paragraph' that contains the cleaned text\n",
    "\n",
    "# Vectorizing the text data\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X_vectorized = tfidf_vectorizer.fit_transform(X_text)\n",
    "\n",
    "# Setting up SMOTE and under-sampling within a pipeline\n",
    "resampling_pipeline = IMBPipeline([\n",
    "    ('smote', SMOTE(sampling_strategy='auto', random_state=42)),\n",
    "    ('under', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "X_resampled, y_resampled = resampling_pipeline.fit_resample(X_vectorized, y_encoded)\n",
    "\n",
    "# Decode the labels back to original category names\n",
    "y_resampled_decoded = encoder.inverse_transform(y_resampled)\n",
    "\n",
    "# Create a DataFrame to display the before and after category distribution\n",
    "resampled_df = pd.DataFrame(y_resampled_decoded, columns=['balanced_category'])\n",
    "\n",
    "print(\"\\nBalanced Category Distribution:\")\n",
    "print(resampled_df['balanced_category'].value_counts())\n",
    "\n",
    "# Count the occurrences of each category in the original and resampled data\n",
    "original_category_counts = df['category'].value_counts()\n",
    "resampled_category_counts = resampled_df['balanced_category'].value_counts()\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Create the first pie chart for original data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(original_category_counts.values, labels=original_category_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Distribution of Categories in Original Data')\n",
    "\n",
    "# Create the second pie chart for resampled data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(resampled_category_counts.values, labels=resampled_category_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Distribution of Categories in Resampled Data')\n",
    "\n",
    "plt.show()\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Feature Expansion\n",
    "\n",
    "### Overview\n",
    "Tokenization converts cleaned paragraphs into lists of tokens. Additionally, binary features are derived from the 'has_entity' column to indicate the presence of specific entity types.\n",
    "\n",
    "### Key Points\n",
    "- The 'tokenized_paragraph' column is created by applying word tokenization to the cleaned text.\n",
    "- Binary columns ('ORG', 'PRODUCT', 'PERSON') are introduced based on the 'has_entity' attribute, expanding the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each paragraph and store the tokens in a new column 'tokenized_paragraph'\n",
    "df['tokenized_paragraph'] = df['cleaned_paragraph'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Display the first few rows to verify the tokenization\n",
    "print(df[['cleaned_paragraph', 'tokenized_paragraph']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classify text paragraphs into specific topics based on the content and any mentioned entities such as persons, organizations, or products. The model should consider not just the textual content but also whether the paragraph mentions specific entities, enhancing its prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        has_entity  ORG  PRODUCT  PERSON\n",
      "0   ORG_YES_PRODUCT_NO_PERSON_YES_    1        0       1\n",
      "1    ORG_YES_PRODUCT_NO_PERSON_NO_    1        0       0\n",
      "2    ORG_YES_PRODUCT_NO_PERSON_NO_    1        0       0\n",
      "3    ORG_NO_PRODUCT_YES_PERSON_NO_    0        1       0\n",
      "4  ORG_YES_PRODUCT_YES_PERSON_YES_    1        1       1\n"
     ]
    }
   ],
   "source": [
    "# Splitting the \"has_entity\" column into three separate binary columns\n",
    "df['ORG'] = df['has_entity'].str.contains('ORG_YES').astype(int)\n",
    "df['PRODUCT'] = df['has_entity'].str.contains('PRODUCT_YES').astype(int)\n",
    "df['PERSON'] = df['has_entity'].str.contains('PERSON_YES').astype(int)\n",
    "\n",
    "# Displaying the first few rows to verify the changes\n",
    "print(df[['has_entity', 'ORG', 'PRODUCT', 'PERSON']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Modeling\n",
    "\n",
    "### Overview\n",
    "Data is prepared for the modeling stage, involving label encoding and splitting into training and testing sets. This setup is crucial for training and evaluating the model's performance.\n",
    "\n",
    "### Key Points\n",
    "- The 'category' labels are encoded into numerical format using LabelEncoder.\n",
    "- The dataset is split into training and test sets, ensuring stratification based on the category labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tokenized_paragraph', 'has_entity', 'ORG', 'PRODUCT', 'PERSON']]\n",
    "y =df['category']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "v = dict(zip(list(y), df['category'].to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Naive Bayes Classifier\n",
    "\n",
    "### Overview\n",
    "A text classification pipeline is constructed using a Naive Bayes classifier. This pipeline integrates CountVectorizer and TfidfTransformer for text feature extraction and transformation before model fitting.\n",
    "\n",
    "### Key Points\n",
    "- The pipeline (`text_clf`) includes steps for converting text into a matrix of token counts, transforming these counts with TF-IDF, and classifying using MultinomialNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer=\"word\", stop_words=\"english\")),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)), \n",
    "    ('clf', MultinomialNB(alpha=.01)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of tokens back into a single string per document\n",
    "x_train_processed = x_train['tokenized_paragraph'].apply(' '.join)\n",
    "\n",
    "# Now, fit your model\n",
    "text_clf.fit(x_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TEST = x_test['tokenized_paragraph'].to_list()\n",
    "Y_TEST = list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a pandas Series\n",
    "X_TEST_series = pd.Series(X_TEST)\n",
    "\n",
    "# Apply the lambda function to process the data\n",
    "X_TEST_processed = X_TEST_series.apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Now, use this processed data for prediction\n",
    "predicted = text_clf.predict(X_TEST_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the test datasets\n",
    "The purpose of this code is to print out the first two documents from the test dataset `(X_TEST)` along with their corresponding predicted categories `(predicted)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "\n",
    "for doc, category in zip(X_TEST, predicted):\n",
    "    \n",
    "    if c == 2:break\n",
    "    \n",
    "    print(\"-\"*55)\n",
    "    print(doc)\n",
    "    print(v[category])\n",
    "    print(\"-\"*55)\n",
    "\n",
    "    c = c + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Persistence\n",
    "\n",
    "### Overview\n",
    "This final section covers the prediction process on the test set, evaluation of the model's accuracy, and persistence of the trained model to disk using Pickle.\n",
    "\n",
    "### Key Points\n",
    "- Test data is preprocessed to match the training format before making predictions.\n",
    "- Model's performance is evaluated by comparing predictions against the true test labels.\n",
    "- The trained model is saved to a file (`model_cat_t1.pkl`) for future use or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extn = '.pkl'\n",
    "model_name = 'model_cat_t1' + model_extn\n",
    "\n",
    "import pickle\n",
    "with open(model_name,'wb') as f:\n",
    "    pickle.dump(text_clf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open(model_name, 'rb') as f:\n",
    "    clf2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = [\"History will place an asterisk next to A.I. as the film Stanley Kubrick might have directed. But let the record also show that Kubrick--after developing this project for some 15 years--wanted Steven Spielberg to helm this astonishing sci-fi rendition of Pinocchio, claiming (with good reason) that it veered closer to Spielberg's kinder, gentler sensibilities.\"]\n",
    "predicted = clf2.predict(docs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[predicted[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP1804-AML",
   "language": "python",
   "name": "comp1804-aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
