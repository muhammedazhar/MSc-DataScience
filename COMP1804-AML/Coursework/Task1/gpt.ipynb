{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\muhammedazhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\muhammedazhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\muhammedazhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\muhammedazhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources have been downloaded and verified.\n",
      "(9347, 7) rows and columns (without `text_clarity`) loaded successfully for taks 1.\n",
      "Data shape after removing missing values: (9268, 7)\n",
      "Removed rows with 'data missing' in the 'has_entity' column\n",
      "Skipped MultinomialNB due to negative values in the features.\n",
      "Applied SMOTE and Random Under-Sampling.\n",
      "--- SVM ---\n",
      "Accuracy: 0.8555976203353164\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.83      0.79       309\n",
      "           1       0.91      0.85      0.88       596\n",
      "           2       0.80      0.88      0.84        42\n",
      "           3       0.83      0.86      0.84       511\n",
      "           4       0.91      0.88      0.90       391\n",
      "\n",
      "    accuracy                           0.86      1849\n",
      "   macro avg       0.84      0.86      0.85      1849\n",
      "weighted avg       0.86      0.86      0.86      1849\n",
      "\n",
      "--- Random Forest ---\n",
      "Accuracy: 0.8312601406165495\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.75       309\n",
      "           1       0.85      0.86      0.85       596\n",
      "           2       0.96      0.64      0.77        42\n",
      "           3       0.80      0.85      0.82       511\n",
      "           4       0.89      0.87      0.88       391\n",
      "\n",
      "    accuracy                           0.83      1849\n",
      "   macro avg       0.85      0.79      0.81      1849\n",
      "weighted avg       0.83      0.83      0.83      1849\n",
      "\n",
      "--- MLP ---\n",
      "Accuracy: 0.8539751216873986\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80       309\n",
      "           1       0.89      0.86      0.87       596\n",
      "           2       0.92      0.83      0.88        42\n",
      "           3       0.83      0.85      0.84       511\n",
      "           4       0.87      0.91      0.89       391\n",
      "\n",
      "    accuracy                           0.85      1849\n",
      "   macro avg       0.86      0.85      0.85      1849\n",
      "weighted avg       0.85      0.85      0.85      1849\n",
      "\n",
      "\n",
      "--- Cross-Validation Scores ---\n",
      "SVM: CV average score = 0.8896757679180889\n",
      "Random Forest: CV average score = 0.8963310580204779\n",
      "MLP: CV average score = 0.9015358361774742\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Text Topic Classification\n",
    "# \n",
    "# This Jupyter notebook code snippet demonstrates the setup for a text classification project using NLP and ML libraries. The code imports libraries, preprocesses data, and sets up classification models.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import re\n",
    "import gensim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Optional: Setup for better visuals\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Function to download necessary NLTK resources\n",
    "def download_nltk_resources():\n",
    "    resources = ['punkt', 'stopwords', 'averaged_perceptron_tagger', 'wordnet']  # For tokenization, stopwords, POS tagging, and lemmatization\n",
    "    for resource in resources:\n",
    "        try:\n",
    "            nltk.data.find(resource)\n",
    "        except LookupError:\n",
    "            nltk.download(resource)\n",
    "    print('NLTK resources have been downloaded and verified.')\n",
    "\n",
    "# Call the function to download resources if not already present\n",
    "download_nltk_resources()\n",
    "\n",
    "# Load the dataset for Task 1\n",
    "def load_t1_df(filename):\n",
    "    df = pd.read_csv(filename, usecols=['par_id', 'paragraph', 'has_entity', 'lexicon_count', 'difficult_words', 'last_editor_gender', 'category'])\n",
    "    size = df.shape\n",
    "    if not df.empty:\n",
    "        print(f\"{size} rows and columns (without `text_clarity`) loaded successfully for taks 1.\")\n",
    "    else:\n",
    "        print(\"The dataset is empty.\")\n",
    "    return df\n",
    "\n",
    "# Load the dataset for Task 2\n",
    "def load_t2_df(filename):\n",
    "    df = pd.read_csv(filename, usecols=['par_id', 'paragraph', 'has_entity', 'lexicon_count', 'difficult_words', 'last_editor_gender', 'category', 'text_clarity'])\n",
    "    size = df.shape\n",
    "    if not df.empty:\n",
    "        print(f\"{size} rows and columns loaded successfully for task 2.\")\n",
    "    else:\n",
    "        print(\"The dataset is empty.\")\n",
    "    return df\n",
    "\n",
    "'''\n",
    "\t- Function usage: df = load_t1_df('filename_with_path')\n",
    "\t- Replace `filename_with_path` with your original value.\n",
    "'''\n",
    "\n",
    "df = load_t1_df('dataset.csv')\n",
    "\n",
    "# Clean the DataFrame\n",
    "df.dropna(inplace=True)\n",
    "print(f\"Data shape after removing missing values: {df.shape}\")\n",
    "\n",
    "# Process 'has_entity' column and remove rows with 'data missing'\n",
    "df = df[df['has_entity'] != 'data missing']\n",
    "print(\"Removed rows with 'data missing' in the 'has_entity' column\")\n",
    "\n",
    "# Splitting the \"has_entity\" column into three separate binary columns\n",
    "df['ORG'] = df['has_entity'].apply(lambda x: 'ORG_YES' in x).astype(int)\n",
    "df['PRODUCT'] = df['has_entity'].apply(lambda x: 'PRODUCT_YES' in x).astype(int)\n",
    "df['PERSON'] = df['has_entity'].apply(lambda x: 'PERSON_YES' in x).astype(int)\n",
    "\n",
    "# Load the Word2Vec model\n",
    "file_path = './WordEmbeddings/GoogleNews-vectors-negative300.bin'  # Adjust the file path accordingly\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "\n",
    "# Initialize the stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to map NLTK's POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if unmatched\n",
    "\n",
    "# Define the function to clean, stem, lemmatize, and preprocess text\n",
    "def process_text(text):\n",
    "    # Clean text\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    # POS tagging on stemmed tokens\n",
    "    pos_tags = nltk.pos_tag(stemmed_tokens)\n",
    "\n",
    "    # Lemmatization with POS tagging\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in pos_tags]\n",
    "\n",
    "    # Combine stemming and lemmatization effects by preferring lemmatized tokens\n",
    "    final_tokens = lemmatized_tokens  # Optionally, could use stemmed_tokens or a combination based on experimentation\n",
    "\n",
    "    # Vectorization using Word2Vec\n",
    "    valid_tokens = [word for word in final_tokens if word in word2vec_model]\n",
    "    if valid_tokens:\n",
    "        vector = np.mean([word2vec_model[token] for token in valid_tokens], axis=0)\n",
    "    else:\n",
    "        vector = np.zeros(300)  # Assuming Word2Vec vectors are of size 300\n",
    "\n",
    "    return vector\n",
    "\n",
    "# Preprocess the paragraphs to vectors\n",
    "df['vector'] = df['paragraph'].apply(process_text)\n",
    "\n",
    "# Combine all features into X\n",
    "X = np.hstack((np.array(df['vector'].tolist()), df[['ORG', 'PRODUCT', 'PERSON']].values))\n",
    "y = df['category'].str.lower()\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check if the features are non-negative for MultinomialNB suitability\n",
    "features_non_negative = (X_train >= 0).all()\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = {\n",
    "    \"SVM\": SVC(kernel='linear'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=500)\n",
    "}\n",
    "\n",
    "# Add MultinomialNB only if features are non-negative\n",
    "if features_non_negative:\n",
    "    classifiers[\"Multinomial Naive Bayes\"] = MultinomialNB()\n",
    "else:\n",
    "    print(\"Skipped MultinomialNB due to negative values in the features.\")\n",
    "\n",
    "# Prepare the pipeline for SMOTE and RandomUnderSampler\n",
    "pipeline = Pipeline([\n",
    "    ('o', SMOTE(sampling_strategy='auto')),\n",
    "    ('u', RandomUnderSampler(sampling_strategy='auto'))\n",
    "])\n",
    "\n",
    "# Apply SMOTE and Random Under-Sampling conditionally\n",
    "class_distribution = Counter(y_train)\n",
    "if min(class_distribution.values()) > 1:\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)\n",
    "    print(\"Applied SMOTE and Random Under-Sampling.\")\n",
    "else:\n",
    "    X_resampled, y_resampled = X_train, y_train\n",
    "    print(\"SMOTE not applied due to insufficient samples in a class.\")\n",
    "\n",
    "# Training and evaluating classifiers\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_resampled, y_resampled)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Cross-validation for each classifier\n",
    "print(\"\\n--- Cross-Validation Scores ---\")\n",
    "for name, clf in classifiers.items():\n",
    "    if name != \"Multinomial Naive Bayes\":  # Cross-validation directly on resampled data for other classifiers\n",
    "        cv_scores = cross_val_score(clf, X_resampled, y_resampled, cv=5)\n",
    "        print(f\"{name}: CV average score = {np.mean(cv_scores)}\")\n",
    "    else:\n",
    "        # For MultinomialNB, ensure cross-validation is done on non-negative features if included\n",
    "        if features_non_negative:\n",
    "            cv_scores = cross_val_score(clf, X_train, y_train, cv=5)  # Assuming original X_train has only non-negative features\n",
    "            print(f\"{name}: CV average score = {np.mean(cv_scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP1804-AML(GPU)",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
