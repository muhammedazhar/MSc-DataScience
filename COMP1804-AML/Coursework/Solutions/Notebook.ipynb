{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP1804 - Applied Machine Learning (Coursework)\n",
    "*The coursework is about a consulting scenario for a non-profit organization known as *NotTheRealWikipedia*, to explore if ML can assist in analyzing new content on their site. It comprises two main tasks:*\n",
    "\n",
    "1. **Topic Classification**: Utilizing ML to categorize paragraphs of text into one of five topics based on the content and the presence of references to a person, organization, and/or product. Success criteria include outperforming a trivial baseline, avoiding overfitting, and ensuring low misclassification rates for each class.\n",
    "2. **Text Clarity Classification Prototype**: Developing a prototype to automatically assess if a paragraph is written clearly enough, employing a subset of the data for training. This task also requires addressing ethical implications and suggesting improvements based on the prototype's performance.\n",
    "\n",
    "Here is the given dataset structure as follows,\n",
    "| FEATURE NAME       | BRIEF DESCRIPTION                                                                          |\n",
    "|--------------------|---------------------------------------------------------------------------------------------|\n",
    "| `par_id`             | Unique identifier for each paragraph to classify.                                           |\n",
    "| `paragraph`          | Text to classify.                                                                           |\n",
    "| `has_entity`         | Whether the text contains a reference to a product (yes/no), an organisation (yes/no), or a person (yes/no). |\n",
    "| `lexicon_count`      | The number of words in the text.                                                            |\n",
    "| `difficult_words`    | The number of difficult words in the text.                                                  |\n",
    "| `last_editor_gender` | The gender of the latest person to edit the text.                                           |\n",
    "| `category`           | The category into which the text should be classified.                                      |\n",
    "| `text_clarity`       | The clarity level of the text. Very few data points are labelled at first.                 |\n",
    "\n",
    "## Setup and Initial Imports\n",
    "\n",
    "### Prerequisite Packages\n",
    "Before proceeding with the actual code, it's crucial to ensure all necessary Python packages are installed. This is achieved by running a `pip install` command that references a [`requirements.txt`](../Docs/requirements.txt) file. This file must be located in a relative path from the notebook (`../Docs/requirements.txt`). The command `%pip install --user -r ../Docs/requirements.txt --quiet` takes care of installing these packages.\n",
    "\n",
    "The [`requirements.txt`](../Docs/requirements.txt) file contains necessary packages for running the Jupyter notebook.\n",
    "\n",
    "#### Usage:\n",
    "```python\n",
    "%pip install --user -r ../Docs/requirements.txt --quiet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --user -r ../Docs/requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "The following code block is responsible for importing a comprehensive set of libraries that are essential for data manipulation, visualization, natural language processing (NLP), and machine learning tasks. A `try` and `except` block is utilized to gracefully handle any errors that might occur during the import process. This approach ensures that any missing libraries or other issues are flagged immediately, facilitating troubleshooting.\n",
    "\n",
    "#### Key Libraries and Their Roles:\n",
    "- **Data Manipulation and Linear Algebra**: `pandas`, `numpy`\n",
    "- **NLP**: `spacy`, `nltk` (Natural Language Toolkit)\n",
    "- **Fast Function Application**: `swifter`\n",
    "- **Visualization**: `seaborn`, `matplotlib`\n",
    "- **Text Processing**: Regular expressions (`re`), `string`, `textblob`\n",
    "- **Machine Learning**: `sklearn` for model training, feature extraction, and evaluation; `imblearn` for handling imbalanced data.\n",
    "- **Progress Monitoring**: `tqdm` for progress bars during lengthy operations.\n",
    "- **Custom Transformers**: For creating pipelines that include custom data preprocessing steps.\n",
    "\n",
    "Additionally, the seaborn library's theme is set to `\"whitegrid\"` for better visual aesthetics in plots, and `tqdm` is enabled for progress applications on pandas series, enhancing feedback during data processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "try:\n",
    "    import json\n",
    "    import os\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import spacy\n",
    "    import swifter  # For applying functions in a fast and efficient way\n",
    "    import pickle\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # Visualization libraries\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Text processing libraries\n",
    "    import re\n",
    "    import string\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from tqdm import tqdm  # For progress bars\n",
    "    from textblob import TextBlob\n",
    "    from langdetect import detect\n",
    "    \n",
    "    # Machine learning libraries\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.pipeline import Pipeline as IMBPipeline\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from sklearn.model_selection import learning_curve, cross_val_score\n",
    "    \n",
    "    # Custom transformers for pipeline\n",
    "    from sklearn.preprocessing import FunctionTransformer\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin\n",
    "    from sklearn.pipeline import FeatureUnion\n",
    "    from sklearn.feature_extraction import DictVectorizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "    # Evaluation metrics\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "    # Setting seaborn theme for better visuals\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Enabling progress_apply for pandas series via tqdm\n",
    "    tqdm.pandas()\n",
    "\n",
    "# Catching and printing any exception that occurs during the import process\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading `NLTK` libraries\n",
    "The following `download_nltk_resources()` function will install the required NLTK resources if they are not already downloaded. This process is called **lazy loading** resources, which I used in my previous projects. It will download packages such as:\n",
    "\n",
    "- `punkt` for tokenzation.\n",
    "- `stopwords` for stopwords removal.\n",
    "- `averaged_perceptron_tagger` for POS tagging.\n",
    "- `wordnet` for lemmatization.\n",
    "\n",
    "The defined function will verify the downloads and suppresses the output to keep the log cleaner with the help of `quiet=True` argument.\n",
    "\n",
    "#### Usage:\n",
    "```python\n",
    "download_nltk_resources()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resource \"punkt\" downloaded.\n",
      "NLTK resource \"stopwords\" downloaded.\n",
      "NLTK resource \"averaged_perceptron_tagger\" downloaded.\n",
      "NLTK resource \"wordnet\" downloaded.\n",
      "All required NLTK resources are ready.\n"
     ]
    }
   ],
   "source": [
    "# Improved function to download and verify necessary NLTK resources\n",
    "def download_nltk_resources():\n",
    "    # Specifying the NLTK resources required for various NLP tasks\n",
    "    resources = ['punkt', 'stopwords', 'averaged_perceptron_tagger', 'wordnet']  # Needed for tokenization, stopwords removal, POS tagging, and lemmatization respectively\n",
    "\n",
    "    # Iterating over each resource to check its presence, and download if missing\n",
    "    for resource in resources:\n",
    "        try:\n",
    "            # Check if the resource is already downloaded to avoid re-downloading\n",
    "            nltk.data.find(f'tokenizers/punkt/{resource}.pickle')\n",
    "        except LookupError:\n",
    "            # Resource not found; proceed to download\n",
    "            nltk.download(resource, quiet=True)  # quiet=True suppresses the output to keep the log cleaner\n",
    "            print(f'NLTK resource \"{resource}\" downloaded.')\n",
    "\n",
    "    # Notification upon successful verification or download of all resources\n",
    "    print('All required NLTK resources are ready.')\n",
    "'''\n",
    "\t- This code is copied from my previous college project. It is called Lazy Loading NLTK Resources. \n",
    "\t- The idea behind it was that we don't need to download all the resources at once.\n",
    "\t- We can download them as and when needed.\n",
    "\t- This way, we can save time and space by downloading only the necessary resources.\n",
    "'''\n",
    "\n",
    "# Initiating the download and verification of NLTK resources\n",
    "download_nltk_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 1: Text Topic Classification\n",
    "*This task contains a machine learning approach for classifying text into five categories: **artificial intelligence**, **movies about artificial intelligence**, **programming**, **philosophy**, and **biographies**.*\n",
    "\n",
    "**Objectives**:\n",
    "1. Predict the topic `category` of a paragraph using `paragraph` and `has_entity` as features.\n",
    "2. Success criteria such as, **better than trivial baseline**, **avoids overfitting**, **misclassification below 10%** for unrelated categories.\n",
    "3. Identify the **most informative scalar performance metric**.\n",
    "\n",
    "## Loading the dataset\n",
    "\n",
    "These functions are designed to load and preprocess datasets for two distinct tasks, ensuring they are ready for further analysis or machine learning models. The following two functions `load_t1_df()` and `load_t2_df()` will efficiently load datasets for Task 1 and Task 2, respectively. Both functions enhance the dataset by adding an `'index'` column starting from 1.\n",
    "\n",
    "### Task 1: `load_t1_df`\n",
    "Loads a dataset specifically for Task 1 with a selected set of columns.\n",
    "\n",
    "#### Features:\n",
    "- **Columns Loaded**: `'paragraph'`, `'has_entity'`, `'category'`.\n",
    "- **Index Column**: Adds an `'index'` column starting from 1 for easier reference.\n",
    "- **Empty Check**: Verifies if the dataset is empty and prints the dataset's dimensions or an empty dataset message.\n",
    "\n",
    "#### Usage:\n",
    "```python\n",
    "df1 = load_t1_df('filename_with_path')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9347 rows and 6 columns (without 'text_clarity') loaded successfully for Task-1, including newly added 'index' column.\n"
     ]
    }
   ],
   "source": [
    "# Function to load and preprocess the dataset for Task 1\n",
    "def load_t1_df(filename):\n",
    "    # Define the columns to be loaded from the file\n",
    "    columns = ['paragraph', 'has_entity', 'category', 'lexicon_count', 'difficult_words']\n",
    "    # Load the dataset with specified columns\n",
    "    df = pd.read_csv(filename, usecols=columns)\n",
    "    # Add an 'index' column that starts from 1\n",
    "    df['index'] = df.index + 1\n",
    "    # Check if the DataFrame is empty and print a message accordingly\n",
    "    if not df.empty:\n",
    "        task_name = 'Task-1'\n",
    "        print(f\"{df.shape[0]} rows and {df.shape[1]} columns (without 'text_clarity') loaded successfully for {task_name}, including newly added 'index' column.\")\n",
    "    else:\n",
    "        print(\"The dataset is empty.\")\n",
    "    return df\n",
    "\n",
    "# Function to load and preprocess the dataset for Task 2\n",
    "def load_t2_df(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    # Add an 'index' column that starts from 1\n",
    "    df['index'] = df.index + 1\n",
    "    # Check if the DataFrame is empty and print a message accordingly\n",
    "    if not df.empty:\n",
    "        task_name = 'Task-2'\n",
    "        print(f\"{df.shape[0]} rows and {df.shape[1]} columns loaded successfully for {task_name}, including newly added 'index' column.\")\n",
    "    else:\n",
    "        print(\"The dataset is empty.\")\n",
    "    return df\n",
    "\n",
    "'''\n",
    "\t- Function usage: df = load_t1_df('filename_with_path')\n",
    "\t- Replace `filename_with_path` with your original value.\n",
    "'''\n",
    "\n",
    "df1 = load_t1_df('../Datasets/dataset.csv')\n",
    "task_name = 'Task-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Function: `clean_df`\n",
    "\n",
    "This function is designed for preprocessing a DataFrame by removing rows with any missing values, except those in the `'text_clarity'` column. It provides insights into the DataFrame's state before and after cleaning by printing the shape and the count of missing values.\n",
    "\n",
    "### Steps Performed:\n",
    "1. **Initial Check**: Prints the initial shape of the DataFrame and a count of missing values for each column.\n",
    "2. **Removing Missing Values**: Excludes rows with missing values across all columns except `'text_clarity'`. It also prints the columns from which rows are being removed due to missing values.\n",
    "3. **Verification**: After removal, prints the updated shape of the DataFrame and verifies that no missing values remain, except possibly in `'text_clarity'`.\n",
    "\n",
    "#### Usage:\n",
    "To clean your DataFrame `df`, call the function as follows:\n",
    "```python\n",
    "df1 = clean_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Initial shape: (9347, 6). Checking for missing values...\n",
      "paragraph           0\n",
      "has_entity          0\n",
      "lexicon_count       0\n",
      "difficult_words    18\n",
      "category           61\n",
      "index               0\n",
      "dtype: int64\n",
      "-------------------------------------------------------\n",
      "Removing rows with missing values...\n",
      "difficult_words    18\n",
      "category           61\n",
      "dtype: int64\n",
      "-------------------------------------------------------\n",
      "Updated shape: (9268, 6). Verifying no missing values remain...\n",
      "paragraph          0\n",
      "has_entity         0\n",
      "lexicon_count      0\n",
      "difficult_words    0\n",
      "category           0\n",
      "index              0\n",
      "dtype: int64\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def clean_df(df):\n",
    "    # Print initial shape and missing values count\n",
    "    print(\"-\" * 55)\n",
    "    print(f'Initial shape: {df.shape}. Checking for missing values...')\n",
    "    missing_values_initial = df.isnull().sum()\n",
    "    print(missing_values_initial)\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    # Remove rows with missing values\n",
    "    df_cleaned = df.dropna(subset=df.columns.difference(['text_clarity']))\n",
    "    print('Removing rows with missing values...')\n",
    "    print(missing_values_initial[missing_values_initial > 0])  # Print only columns with missing values\n",
    "\n",
    "    # Print shape after removal and verify no missing values\n",
    "    print(\"-\" * 55)\n",
    "    print(f'Updated shape: {df_cleaned.shape}. Verifying no missing values remain...')\n",
    "    missing_values_final = df_cleaned.isnull().sum()\n",
    "    print(missing_values_final)\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "'''\n",
    "\t- Function usage: df = clean_df(df)\n",
    "\t- Replace `df` with your original DataFrame.\n",
    "'''\n",
    "\n",
    "df1 = clean_df(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Processing Function: `process_df`\n",
    "\n",
    "This function performs specific preprocessing tasks on a DataFrame to ensure data consistency and integrity, focusing on the 'category' and 'has_entity' columns.\n",
    "\n",
    "### Steps Performed:\n",
    "1. **Standardize 'category' Column**:\n",
    "    - Initially prints unique values in the 'category' column.\n",
    "    - Converts all text in the 'category' column to lowercase to standardize the data.\n",
    "    - Prints the unique values in the 'category' column post-conversion for verification.\n",
    "\n",
    "2. **Clean 'has_entity' Column**:\n",
    "    - Prints unique values in the 'has_entity' column to show initial data state.\n",
    "    - Removes rows where the 'has_entity' column has the value 'data missing', ensuring data quality.\n",
    "    - Prints the unique values in the 'has_entity' column after removal for verification.\n",
    "\n",
    "#### Usage:\n",
    "To preprocess your DataFrame `df`, use the function as follows:\n",
    "```python\n",
    "df1 = process_df(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Checking for unique values in the \"category\" column:\n",
      "['biographies' 'artificial intelligence' 'programming' 'philosophy'\n",
      " 'movies about artificial intelligence' 'Philosophy' 'Programming'\n",
      " 'Artificial intelligence' 'Biographies'\n",
      " 'Movies about artificial intelligence']\n",
      "\n",
      "Fixed the case of the \"category\" column, unique values now:\n",
      "['biographies' 'artificial intelligence' 'programming' 'philosophy'\n",
      " 'movies about artificial intelligence']\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "Checking for unique values in the \"has_entity\" column:\n",
      "['ORG_YES_PRODUCT_NO_PERSON_YES_' 'ORG_YES_PRODUCT_NO_PERSON_NO_'\n",
      " 'ORG_NO_PRODUCT_YES_PERSON_NO_' 'ORG_YES_PRODUCT_YES_PERSON_YES_'\n",
      " 'ORG_NO_PRODUCT_NO_PERSON_NO_' 'ORG_NO_PRODUCT_YES_PERSON_YES_'\n",
      " 'ORG_NO_PRODUCT_NO_PERSON_YES_' 'ORG_YES_PRODUCT_YES_PERSON_NO_'\n",
      " 'data missing']\n",
      "\n",
      "Removed rows with \"data missing\" in the \"has_entity\" column, unique values now:\n",
      "['ORG_YES_PRODUCT_NO_PERSON_YES_' 'ORG_YES_PRODUCT_NO_PERSON_NO_'\n",
      " 'ORG_NO_PRODUCT_YES_PERSON_NO_' 'ORG_YES_PRODUCT_YES_PERSON_YES_'\n",
      " 'ORG_NO_PRODUCT_NO_PERSON_NO_' 'ORG_NO_PRODUCT_YES_PERSON_YES_'\n",
      " 'ORG_NO_PRODUCT_NO_PERSON_YES_' 'ORG_YES_PRODUCT_YES_PERSON_NO_']\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def process_df(df):\n",
    "    # Standardize 'category' column to lowercase\n",
    "    print(\"-\" * 55)\n",
    "    print('Checking for unique values in the \"category\" column:')\n",
    "    print(df['category'].unique())\n",
    "    df['category'] = df['category'].str.lower()\n",
    "    print('\\nFixed the case of the \"category\" column, unique values now:')\n",
    "    print(df['category'].unique())\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    # Remove rows where 'has_entity' column has 'data missing'\n",
    "    print(\"-\" * 55)\n",
    "    print('Checking for unique values in the \"has_entity\" column:')\n",
    "    print(df['has_entity'].unique())\n",
    "    df = df[df['has_entity'] != 'data missing']\n",
    "    print('\\nRemoved rows with \"data missing\" in the \"has_entity\" column, unique values now:')\n",
    "    print(df['has_entity'].unique())\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    return df\n",
    "\n",
    "'''\n",
    "\t- Function usage: df = process_df(df)\n",
    "\t- Replace `df` with your original DataFrame.\n",
    "'''\n",
    "\n",
    "df1 = process_df(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Column Transformation Function: `split_entity_column`\n",
    "\n",
    "This function enhances data representation by splitting the `'has_entity'` column of a DataFrame into three separate binary columns. Each new column represents the presence (1) or absence (0) of specific entity types: organizations, products, and persons.\n",
    "\n",
    "### Transformation Details:\n",
    "- **ORG Column**: Indicates the presence of organizations with `ORG_YES`.\n",
    "- **PRODUCT Column**: Flags the presence of products with `PRODUCT_YES`.\n",
    "- **PERSON Column**: Marks the presence of persons with `PERSON_YES`.\n",
    "  \n",
    "The function applies a lambda function to check for the existence of each entity type within the `'has_entity'` column, then converts these checks into binary columns.\n",
    "\n",
    "After the transformation, the function prints a preview of the DataFrame showing the original `'has_entity'` column alongside the newly created `'ORG'`, `'PRODUCT'`, and `'PERSON'` binary columns for verification.\n",
    "\n",
    "#### Usage:\n",
    "To apply this transformation to your DataFrame, use the function as follows:\n",
    "```python\n",
    "df1 = split_entity_column(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Preview of the 'has_entity' column and the new binary columns:\n",
      "                        has_entity  ORG  PRODUCT  PERSON\n",
      "0   ORG_YES_PRODUCT_NO_PERSON_YES_    1        0       1\n",
      "1    ORG_YES_PRODUCT_NO_PERSON_NO_    1        0       0\n",
      "2    ORG_YES_PRODUCT_NO_PERSON_NO_    1        0       0\n",
      "3    ORG_NO_PRODUCT_YES_PERSON_NO_    0        1       0\n",
      "4  ORG_YES_PRODUCT_YES_PERSON_YES_    1        1       1\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "Distribution of the 'has_entity' column:\n",
      "has_entity\n",
      "ORG_YES_PRODUCT_NO_PERSON_YES_     3029\n",
      "ORG_NO_PRODUCT_NO_PERSON_NO_       2851\n",
      "ORG_YES_PRODUCT_NO_PERSON_NO_      1462\n",
      "ORG_NO_PRODUCT_NO_PERSON_YES_      1373\n",
      "ORG_YES_PRODUCT_YES_PERSON_YES_     298\n",
      "ORG_YES_PRODUCT_YES_PERSON_NO_      125\n",
      "ORG_NO_PRODUCT_YES_PERSON_YES_       64\n",
      "ORG_NO_PRODUCT_YES_PERSON_NO_        42\n",
      "Name: count, dtype: int64\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Transform the \"has_entity\" column into three binary columns for entity types\n",
    "def split_entity_column(df):\n",
    "    # Create binary columns based on the presence of specific entity types in the 'has_entity' column\n",
    "    df['ORG'] = df['has_entity'].apply(lambda x: 'ORG_YES' in x).astype(int)\n",
    "    df['PRODUCT'] = df['has_entity'].apply(lambda x: 'PRODUCT_YES' in x).astype(int)\n",
    "    df['PERSON'] = df['has_entity'].apply(lambda x: 'PERSON_YES' in x).astype(int)\n",
    "\n",
    "    # Display the first few rows to verify the newly added binary columns alongside the 'has_entity' column\n",
    "    print(\"-\" * 55)\n",
    "    print(\"Preview of the 'has_entity' column and the new binary columns:\")\n",
    "    print(df[['has_entity', 'ORG', 'PRODUCT', 'PERSON']].head())\n",
    "    print(\"-\" * 55)\n",
    "    print(\"-\" * 55)\n",
    "    print(\"Distribution of the 'has_entity' column:\")\n",
    "    print(df['has_entity'].value_counts())\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    return df\n",
    "\n",
    "'''\n",
    "\t- Function usage: df = split_entity_column(df)\n",
    "\t- Replace `df` with your original DataFrame.\n",
    "'''\n",
    "\n",
    "df1 = split_entity_column(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning Functions: `clean_text` and `clean_column`\n",
    "\n",
    "These functions are designed to preprocess textual data within a DataFrame, removing unnecessary characters and whitespace, thereby standardizing the text for analysis or machine learning tasks.\n",
    "\n",
    "#### Text Cleaning Utility: `clean_text` \n",
    "- **Purpose**: Cleans a given text string by removing punctuation, numbers, and extra spaces.\n",
    "- **Implementation**:\n",
    "  - Uses regular expressions (`re.sub`) to strip out non-alphabetic characters and numbers.\n",
    "  - Condenses multiple whitespace characters down to a single space and trims leading/trailing whitespace.\n",
    "\n",
    "#### Apply Text Cleaning to DataFrame Column: `clean_column`\n",
    "- **Purpose**: Applies the `clean_text` function to a specified column in a DataFrame, creating a new \"_cleaned\" version of the column.\n",
    "- **Verification**: Prints the original and cleaned version of the first text entry in the specified column for a quick comparison and verification of the cleaning process.\n",
    "- **Error Handling**: Checks if the specified column exists in the DataFrame, printing an error message if it does not.\n",
    "\n",
    "#### Usage:\n",
    "To clean a text column in your DataFrame, use the `clean_column` function as follows:\n",
    "```python\n",
    "df1 = clean_column(df, 'column_name')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Original paragraph:\n",
      " Ramsay was born in Glasgow on 2 October 1852. He was a nephew of the geologist Sir Andrew Ramsay. His father, William, Sr., was a civil engineer. His mother was Catherine Robertson. He studied at Glasgow Academy, at the University of Glasgow and at University of Tubingen in Germany. \n",
      "\n",
      "Cleaned paragraph:\n",
      " Ramsay was born in Glasgow on October He was a nephew of the geologist Sir Andrew Ramsay His father William Sr was a civil engineer His mother was Catherine Robertson He studied at Glasgow Academy at the University of Glasgow and at University of Tubingen in Germany\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Remove punctuation marks and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_column(df, column_name):\n",
    "    # Ensure the column exists in the DataFrame\n",
    "    if column_name in df.columns:\n",
    "        unclean = df[column_name]\n",
    "        df[column_name + '_cleaned'] = df[column_name].apply(clean_text)\n",
    "        clean = df[column_name + '_cleaned']\n",
    "        print(\"-\"*55)\n",
    "        print(\"Original paragraph:\\n\", unclean.iloc[0])\n",
    "        print(\"\\nCleaned paragraph:\\n\", clean.iloc[0])\n",
    "        print(\"-\"*55)\n",
    "    else:\n",
    "        print(f\"The column '{column_name}' does not exist in the DataFrame.\")\n",
    "    return df\n",
    "\n",
    "'''\n",
    "\t- Function usage: df = clean_column(df, 'paragraph')\n",
    "\t- Replace `paragraph` with your original value.\n",
    "'''\n",
    "\n",
    "df1 = clean_column(df1, 'paragraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Apply the function to the 'paragraph_cleaned' column and store the result in a new column 'is_english'\n",
    "df1['is_english'] = df1['paragraph_cleaned'].swifter.apply(is_english)\n",
    "df1 = df1[df1['is_english'] != False]\n",
    "print(\"-\" * 55)\n",
    "print(\"Number of rows after removing non-English paragraphs:\\n\")\n",
    "print(df1['is_english'].value_counts())\n",
    "print(\"-\" * 55)\n",
    "\n",
    "print(\"Present columns:\\n\")\n",
    "column_names = df1.columns.tolist()\n",
    "for name in column_names:\n",
    "    print(name)\n",
    "print(\"-\" * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tokenization Function: `tokenize_column`\n",
    "\n",
    "This function is designed to tokenize textual data within a specified column of a DataFrame, converting text into a list of tokens (words) and storing the result in a new column.\n",
    "\n",
    "<u>Summary</u>: A **token** refers to a single unit of linguistic data. It's the result of taking a text or set of text and breaking it up into pieces such as words, keywords, phrases, symbols and other elements, which are then used as input for further processing.\n",
    "\n",
    "### Features:\n",
    "- **Column Verification**: Checks if the specified column exists in the DataFrame. If it does not, prints an error message.\n",
    "- **Tokenization**: Utilizes the `word_tokenize` function from the NLTK library to break down text into individual words or tokens.\n",
    "- **New Column Creation**: Appends a \"_tokenized\" suffix to the original column name and stores the tokenized lists in this new column.\n",
    "- **Preview**: Prints the first entry from both the original (cleaned) column and the new (tokenized) column to demonstrate the tokenization effect.\n",
    "\n",
    "#### Usage:\n",
    "To apply text tokenization to a column in your DataFrame, execute the function as follows:\n",
    "```python\n",
    "df1 = tokenize_column(df, 'column_name_cleaned')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Cleaned paragraph:\n",
      " Ramsay was born in Glasgow on October He was a nephew of the geologist Sir Andrew Ramsay His father William Sr was a civil engineer His mother was Catherine Robertson He studied at Glasgow Academy at the University of Glasgow and at University of Tubingen in Germany\n",
      "\n",
      "Tokenized paragraph:\n",
      " ['Ramsay', 'was', 'born', 'in', 'Glasgow', 'on', 'October', 'He', 'was', 'a', 'nephew', 'of', 'the', 'geologist', 'Sir', 'Andrew', 'Ramsay', 'His', 'father', 'William', 'Sr', 'was', 'a', 'civil', 'engineer', 'His', 'mother', 'was', 'Catherine', 'Robertson', 'He', 'studied', 'at', 'Glasgow', 'Academy', 'at', 'the', 'University', 'of', 'Glasgow', 'and', 'at', 'University', 'of', 'Tubingen', 'in', 'Germany']\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each paragraph and store the tokens in a new column 'tokenized_paragraph'\n",
    "def tokenize_column(df, column_name):\n",
    "    # Ensure the column exists in the DataFrame\n",
    "    if column_name in df.columns:\n",
    "        clean = df[column_name]\n",
    "        df[column_name + '_tokenized'] = df[column_name].apply(lambda x: word_tokenize(x))\n",
    "        tokenized = df[column_name + '_tokenized']\n",
    "        print(\"-\"*55)\n",
    "        print(\"Cleaned paragraph:\\n\", clean.iloc[0])\n",
    "        print(\"\\nTokenized paragraph:\\n\", tokenized.iloc[0])\n",
    "        print(\"-\"*55)\n",
    "    else:\n",
    "        print(f\"The column '{column_name}' does not exist in the DataFrame.\")\n",
    "    return df\n",
    "\n",
    "'''\n",
    "\t- Function usage: df = clean_column(df, 'paragraph')\n",
    "\t- Replace `paragraph` with your original value.\n",
    "'''\n",
    "\n",
    "df1 = tokenize_column(df1, 'paragraph_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;tfidf&#x27;,\n",
       "                                                  TfidfVectorizer(max_features=3000,\n",
       "                                                                  stop_words=&#x27;english&#x27;),\n",
       "                                                  &#x27;paragraph_cleaned&#x27;),\n",
       "                                                 (&#x27;ohe&#x27;, OneHotEncoder(),\n",
       "                                                  [&#x27;has_entity&#x27;])])),\n",
       "                (&#x27;resample&#x27;, SMOTE(random_state=42)),\n",
       "                (&#x27;classify&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;Pipeline<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;tfidf&#x27;,\n",
       "                                                  TfidfVectorizer(max_features=3000,\n",
       "                                                                  stop_words=&#x27;english&#x27;),\n",
       "                                                  &#x27;paragraph_cleaned&#x27;),\n",
       "                                                 (&#x27;ohe&#x27;, OneHotEncoder(),\n",
       "                                                  [&#x27;has_entity&#x27;])])),\n",
       "                (&#x27;resample&#x27;, SMOTE(random_state=42)),\n",
       "                (&#x27;classify&#x27;, MultinomialNB())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;preprocessing: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for preprocessing: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(transformers=[(&#x27;tfidf&#x27;,\n",
       "                                 TfidfVectorizer(max_features=3000,\n",
       "                                                 stop_words=&#x27;english&#x27;),\n",
       "                                 &#x27;paragraph_cleaned&#x27;),\n",
       "                                (&#x27;ohe&#x27;, OneHotEncoder(), [&#x27;has_entity&#x27;])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">tfidf</label><div class=\"sk-toggleable__content fitted\"><pre>paragraph_cleaned</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(max_features=3000, stop_words=&#x27;english&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">ohe</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;has_entity&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">SMOTE</label><div class=\"sk-toggleable__content fitted\"><pre>SMOTE(random_state=42)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MultinomialNB<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(transformers=[('tfidf',\n",
       "                                                  TfidfVectorizer(max_features=3000,\n",
       "                                                                  stop_words='english'),\n",
       "                                                  'paragraph_cleaned'),\n",
       "                                                 ('ohe', OneHotEncoder(),\n",
       "                                                  ['has_entity'])])),\n",
       "                ('resample', SMOTE(random_state=42)),\n",
       "                ('classify', MultinomialNB())])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)\n",
    "ohecoder = OneHotEncoder()\n",
    "\n",
    "# Prepare the data\n",
    "X = df1[['paragraph_cleaned', 'has_entity']]\n",
    "y = df1['category']\n",
    "\n",
    "# Label encoding for the target variable\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded)\n",
    "\n",
    "# Define a column transformer for preprocessing the features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tfidf', vectorizer, 'paragraph_cleaned'),\n",
    "        ('ohe', ohecoder, ['has_entity'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# TODO: Extended classifier choice\n",
    "chosen_clf = 2  # Update this to choose a classifier\n",
    "\n",
    "# Define the classifiers in a dictionary for easier selection\n",
    "classifiers = {\n",
    "    1: {'model': SVC(kernel='linear', probability=True, random_state=42), 'name': 'SVM'},\n",
    "    2: {'model': MultinomialNB(), 'name': 'MultinomialNB'},\n",
    "    3: {'model': LogisticRegression(random_state=42), 'name': 'LogisticRegression'},\n",
    "    4: {'model': RandomForestClassifier(random_state=42), 'name': 'RandomForest'},\n",
    "    5: {'model': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), 'name': 'XGBClassifier'},\n",
    "    # Add more classifiers here\n",
    "}\n",
    "\n",
    "# Select the classifier based on chosen_clf\n",
    "if chosen_clf in classifiers:\n",
    "    classifier = classifiers[chosen_clf]['model']\n",
    "    clf_name = classifiers[chosen_clf]['name']\n",
    "else:\n",
    "    raise ValueError(\"Invalid classifier choice.\")\n",
    "\n",
    "# Create a pipeline that includes preprocessing, resampling, and classifier\n",
    "model_pipeline = IMBPipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('resample', SMOTE(sampling_strategy='auto', random_state=42)), \n",
    "    ('classify', classifier)\n",
    "])\n",
    "\n",
    "# Train the model pipeline\n",
    "model_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training {}...'.format(clf_name))\n",
    "print(\"-\" * 55)\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model on the training set\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print(\"{} classifier trained on [{:.2f}] seconds.\".format(clf_name, end_time))\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Decode the predictions back to original category names for interpretability\n",
    "y_pred_decoded = label_encoder.inverse_transform(y_pred)\n",
    "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test_decoded, y_pred_decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just use X_test which already contains the required columns\n",
    "X_TEST = X_test  # X_test should be a DataFrame with the necessary columns\n",
    "\n",
    "# Use the model_pipeline to predict\n",
    "predicted = model_pipeline.predict(X_TEST)\n",
    "\n",
    "# Decoding the predicted labels back to original category names for interpretability\n",
    "predicted_decoded = label_encoder.inverse_transform(predicted)\n",
    "\n",
    "# Print the first two predictions\n",
    "c = 0\n",
    "for index, (doc, category) in enumerate(zip(X_TEST.itertuples(index=False), predicted_decoded)):\n",
    "    if c == 2:\n",
    "        break\n",
    "\n",
    "    print(\"-\"*55)\n",
    "    print(f\"Document: {doc.paragraph_cleaned}\")  # Assuming 'paragraph_cleaned' is the correct column name\n",
    "    print(f\"Entity Presence: [{doc.has_entity}]\\n\")\n",
    "    print(f\"Predicted Category: '{category}'\")\n",
    "    print(\"-\"*55)\n",
    "\n",
    "    c += 1\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = np.mean(predicted == y_test)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(model_pipeline, X, y, cv=5)\n",
    "\n",
    "# Calculate mean and standard deviation for training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Calculate mean and standard deviation for test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
    "\n",
    "# Draw bands\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that we are using a pipeline named `model_pipeline`\n",
    "scores = cross_val_score(model_pipeline, X, y, cv=5)\n",
    "\n",
    "print(\"-\" * 55)\n",
    "print(\"Cross-validation scores:\")\n",
    "print(f'{\", \".join([f\"{score:.2f}%\" for score in scores * 100])}\\n')\n",
    "print(f'Mean cross-validation score: {np.mean(scores) * 100:.2f}%')\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Fit the model\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix\n",
    "cm_df = pd.DataFrame(cm, index=model_pipeline.classes_, columns=model_pipeline.classes_)\n",
    "\n",
    "# Plot the confusion matrix using Seaborn\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_df, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Model saved successfully to '../Models/Task-1MultinomialNB.pkl' file.\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Save classifiers, transformers and encoders in a single object\n",
    "model_to_save = {\n",
    "    'model': model_pipeline,\n",
    "    'encoder': label_encoder,\n",
    "    'ohecoder': ohecoder,\n",
    "    'vectorizer': vectorizer\n",
    "}\n",
    "\n",
    "# Define the name and path for the model file\n",
    "model_extn = '.pkl' # Extension for the model file\n",
    "model_name = f'{task_name}{clf_name}{model_extn}' # Name of the model file\n",
    "dir_name = '../Models/' # Directory to save the model\n",
    "path_name = os.path.join(dir_name, model_name) # Complete path to save the model\n",
    "\n",
    "# Ensure that the 'Models' directory exists before trying to save the file\n",
    "if not os.path.exists(dir_name):\n",
    "    os.makedirs(dir_name)\n",
    "\n",
    "# Save the model to the specified path\n",
    "with open(path_name, 'wb') as file:\n",
    "    pickle.dump(model_to_save, file)\n",
    "\n",
    "# Notify the user about the successful saving of the model\n",
    "print(\"-\" * 55)\n",
    "print(f\"Model saved successfully to '{path_name}' file.\")\n",
    "print(\"-\" * 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model from the file\n",
    "loaded_model_path = path_name\n",
    "with open(loaded_model_path, 'rb') as file:\n",
    "    loaded_file = pickle.load(file)\n",
    "\n",
    "loaded_model = loaded_file['model']\n",
    "loaded_encoder = loaded_file['encoder']\n",
    "loaded_ohecoder = loaded_file['ohecoder']\n",
    "loaded_vectorizer = loaded_file['vectorizer']\n",
    "\n",
    "# Load the dataset\n",
    "df = load_t2_df('../Datasets/dataset.csv')\n",
    "\n",
    "# Clean the dataset\n",
    "df = clean_column(df, 'paragraph')\n",
    "\n",
    "# Identify rows with missing 'category' values AND 'has_entity' NOT 'data missing'\n",
    "missing_category_mask = df['category'].isnull() & (df['has_entity'] != 'data missing')\n",
    "\n",
    "# Prepare data for prediction (only 'paragraph' and 'has_entity' columns)\n",
    "data_to_predict = df.loc[missing_category_mask, ['paragraph_cleaned', 'has_entity']]\n",
    "\n",
    "# Predict missing categories\n",
    "predicted_categories = model_pipeline.predict(data_to_predict)\n",
    "\n",
    "decoded_predictions = loaded_encoder.inverse_transform(predicted_categories)\n",
    "\n",
    "# Fill missing values in the original dataframe\n",
    "df.loc[missing_category_mask, 'category'] = decoded_predictions\n",
    "df.drop(columns=['paragraph_cleaned', 'index'], inplace=True)\n",
    "\n",
    "df.to_csv('../Datasets/task1_dataset.csv', index=False)\n",
    "\n",
    "# Count how many categories were predicted and filled\n",
    "predicted_count = df.loc[missing_category_mask, 'category'].notnull().sum()\n",
    "\n",
    "print(f\"Total categories predicted and filled: {predicted_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9347 rows and 9 columns loaded successfully for Task-2, including newly added 'index' column.\n",
      "-------------------------------------------------------\n",
      "Original paragraph:\n",
      " Ramsay was born in Glasgow on 2 October 1852. He was a nephew of the geologist Sir Andrew Ramsay. His father, William, Sr., was a civil engineer. His mother was Catherine Robertson. He studied at Glasgow Academy, at the University of Glasgow and at University of Tubingen in Germany. \n",
      "\n",
      "Cleaned paragraph:\n",
      " Ramsay was born in Glasgow on October He was a nephew of the geologist Sir Andrew Ramsay His father William Sr was a civil engineer His mother was Catherine Robertson He studied at Glasgow Academy at the University of Glasgow and at University of Tubingen in Germany\n",
      "-------------------------------------------------------\n",
      "Total categories predicted and filled: 61\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model from the file\n",
    "with open('../Models/Task-1MultinomialNB.pkl', 'rb') as file:\n",
    "    loaded_file  = pickle.load(file)\n",
    "\n",
    "loaded_model = loaded_file['model']\n",
    "loaded_encoder = loaded_file['encoder']\n",
    "loaded_ohecoder = loaded_file['ohecoder']\n",
    "loaded_vectorizer = loaded_file['vectorizer']\n",
    "\n",
    "# Load the dataset\n",
    "df = load_t2_df('../Datasets/dataset.csv')\n",
    "\n",
    "# Clean the dataset\n",
    "df = clean_column(df, 'paragraph')\n",
    "\n",
    "# Identify rows with missing 'category' values AND 'has_entity' NOT 'data missing'\n",
    "missing_category_mask = df['category'].isnull() & (df['has_entity'] != 'data missing')\n",
    "\n",
    "# Prepare data for prediction (only 'paragraph' and 'has_entity' columns)\n",
    "data_to_predict = df.loc[missing_category_mask, ['paragraph_cleaned', 'has_entity']]\n",
    "\n",
    "# Predict missing categories\n",
    "predicted_categories = loaded_model.predict(data_to_predict)\n",
    "\n",
    "decoded_predictions = loaded_encoder.inverse_transform(predicted_categories)\n",
    "\n",
    "# Fill missing values in the original dataframe\n",
    "df.loc[missing_category_mask, 'category'] = decoded_predictions\n",
    "df.drop(columns=['paragraph_cleaned', 'index'], inplace=True)\n",
    "\n",
    "df.to_csv('../Datasets/task1_dataset.csv', index=False)\n",
    "\n",
    "# Count how many categories were predicted and filled\n",
    "predicted_count = df.loc[missing_category_mask, 'category'].notnull().sum()\n",
    "\n",
    "print(f\"Total categories predicted and filled: {predicted_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 2: Text Clarity Classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP1804-AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
