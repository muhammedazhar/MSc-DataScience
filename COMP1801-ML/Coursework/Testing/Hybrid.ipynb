{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Separate features for encoding\n",
    "onehot_features = ['microstructure', 'seedLocation', 'castType']\n",
    "label_features = ['partType']\n",
    "\n",
    "# Custom transformers\n",
    "label_encoder = LabelEncoder()\n",
    "df['partType'] = label_encoder.fit_transform(df['partType'])  # Apply label encoding directly\n",
    "\n",
    "# Create a ColumnTransformer for One-Hot Encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # To match the output of One-Hot Encoding with the Label Encoding\n",
    "        ('onehot', OneHotEncoder(dtype=int), onehot_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keeps all other features as they are\n",
    ")\n",
    "\n",
    "# Define pipeline with preprocessing and model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(\n",
    "        max_depth=15,\n",
    "        n_estimators=387,\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=3,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['Lifespan']), df['Lifespan'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "msle = mean_squared_log_error(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSLE: {msle:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Access the trained Random Forest model from the pipeline\n",
    "rf_model_imp = pipeline.named_steps['model']\n",
    "\n",
    "# Step 2: Retrieve the feature names from the preprocessor\n",
    "# Get the One-Hot Encoder categories and append label encoded features\n",
    "onehot_encoder = pipeline.named_steps['preprocessor'].named_transformers_['onehot']\n",
    "onehot_feature_names = onehot_encoder.get_feature_names_out(onehot_features)\n",
    "\n",
    "# Combine One-Hot Encoded feature names with label-encoded and numerical feature names\n",
    "all_feature_names = list(onehot_feature_names) + label_features + [col for col in X_train.columns if col not in onehot_features + label_features]\n",
    "\n",
    "# Step 3: Extract the feature importances from the RandomForestRegressor\n",
    "importances = rf_model_imp.feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Step 4: Plot the feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Feature Importance after Hybrid Encoding\")\n",
    "plt.bar(range(len(importances)), importances[indices], align=\"center\")\n",
    "plt.xticks(range(len(importances)), np.array(all_feature_names)[indices], rotation=90)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important features\n",
    "important_features = ['partType', 'coolingRate', 'Nickel%', 'HeatTreatTime', 'Chromium%' , 'quenchTime']\n",
    "\n",
    "# Subset the dataset to include only the important features\n",
    "X_important = df[important_features]\n",
    "\n",
    "# Split the dataset\n",
    "X_train_imp, X_test_imp, y_train_imp, y_test_imp = train_test_split(X_important, df['Lifespan'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor with the best parameters obtained from previous tuning\n",
    "rf_model_imp = RandomForestRegressor(\n",
    "    max_depth=15,\n",
    "    n_estimators=387,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model with reduced features\n",
    "rf_model_imp.fit(X_train_imp, y_train_imp)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_imp = rf_model_imp.predict(X_test_imp)\n",
    "rmse_imp = root_mean_squared_error(y_test_imp, y_pred_imp)\n",
    "r2_imp = r2_score(y_test_imp, y_pred_imp)\n",
    "mae_imp = mean_absolute_error(y_test_imp, y_pred_imp)\n",
    "msle_imp = mean_squared_log_error(y_test_imp, y_pred_imp)\n",
    "\n",
    "print(f\"Reduced Features RMSE: {rmse_imp:.2f}\")\n",
    "print(f\"Reduced Features R² Score: {r2_imp:.2f}\")\n",
    "print(f\"Reduced Features MAE: {mae_imp:.2f}\")\n",
    "print(f\"Reduced Features MSLE: {msle_imp:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP1801-ML(GPU)",
   "language": "python",
   "name": "comp1801-ml"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
