{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import os\n",
    "    import glob\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # Importing libraries for data visualization\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Creating a model\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential, Model\n",
    "    from tensorflow.keras.layers import Dense, Activation, Input\n",
    "\n",
    "    # Importing libraries for evaluation\n",
    "    from itertools import product\n",
    "    from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "    from sklearn.model_selection import train_test_split, KFold\n",
    "    from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score\n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: ../Datasets/Dataset.csv\n",
      "Loaded dataset: ../Datasets/Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Find the CSV file in the Datasets directory\n",
    "data_path = '../Datasets/*.csv'\n",
    "file_list = glob.glob(data_path)\n",
    "\n",
    "for file in file_list:\n",
    "    print(f\"Found file: {file}\")\n",
    "\n",
    "# Ensure there is exactly one file\n",
    "if len(file_list) == 1:\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_list[0])\n",
    "    print(f\"Loaded dataset: {file_list[0]}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No CSV file found or multiple CSV files found in the Datasets directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be saved to: ../Models/\n"
     ]
    }
   ],
   "source": [
    "# File path to save the trained model\n",
    "destination = '../Models/'\n",
    "os.makedirs(destination, exist_ok=True)\n",
    "print(f\"Model will be saved to: {destination}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_unified = ['partType', 'microstructure', 'seedLocation', 'castType']\n",
    "\n",
    "# Initialize and fit the encoder\n",
    "ohe = OneHotEncoder(sparse_output=False, drop=None)\n",
    "# Reshape the data to handle multiple categorical columns\n",
    "encoded_data = ohe.fit_transform(df[categorical_cols_unified].values)\n",
    "\n",
    "# Convert to DataFrame with feature names\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_data,\n",
    "    columns=ohe.get_feature_names_out(categorical_cols_unified)\n",
    ")\n",
    "\n",
    "# Combine with non-categorical columns if needed\n",
    "df = pd.concat([df.drop(columns=categorical_cols_unified), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X = df.drop('Lifespan',axis=1)\n",
    "\n",
    "# Target\n",
    "y = df['Lifespan']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 23)\n",
      "(300, 23)\n",
      "(700,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  1.0\n",
      "Min:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Create the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transfrom\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# everything has been scaled between 1 and 0\n",
    "print('Max: ',X_train.max())\n",
    "print('Min: ', X_train.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# input layer\n",
    "model.add(Dense(19,activation='relu'))\n",
    "\n",
    "# hidden layers\n",
    "model.add(Dense(19,activation='relu'))\n",
    "model.add(Dense(19,activation='relu'))\n",
    "model.add(Dense(19,activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1851126.0000 - val_loss: 1776179.3750\n",
      "Epoch 2/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1834680.2500 - val_loss: 1775597.6250\n",
      "Epoch 3/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1792345.5000 - val_loss: 1774950.0000\n",
      "Epoch 4/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1805207.5000 - val_loss: 1774159.3750\n",
      "Epoch 5/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1818765.1250 - val_loss: 1773137.2500\n",
      "Epoch 6/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1801332.1250 - val_loss: 1771792.2500\n",
      "Epoch 7/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1822369.8750 - val_loss: 1770016.2500\n",
      "Epoch 8/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1792462.3750 - val_loss: 1767652.5000\n",
      "Epoch 9/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1817713.0000 - val_loss: 1764479.1250\n",
      "Epoch 10/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1769191.5000 - val_loss: 1760193.5000\n",
      "Epoch 11/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1812650.0000 - val_loss: 1754379.1250\n",
      "Epoch 12/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1764863.8750 - val_loss: 1746520.6250\n",
      "Epoch 13/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1753802.5000 - val_loss: 1735859.1250\n",
      "Epoch 14/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1744035.3750 - val_loss: 1721596.6250\n",
      "Epoch 15/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1757122.1250 - val_loss: 1702703.1250\n",
      "Epoch 16/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1717966.5000 - val_loss: 1677980.3750\n",
      "Epoch 17/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1719687.1250 - val_loss: 1645803.6250\n",
      "Epoch 18/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1681888.7500 - val_loss: 1604591.3750\n",
      "Epoch 19/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1630392.5000 - val_loss: 1552272.7500\n",
      "Epoch 20/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1541144.7500 - val_loss: 1486920.5000\n",
      "Epoch 21/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1510412.7500 - val_loss: 1406307.6250\n",
      "Epoch 22/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1405920.7500 - val_loss: 1308976.3750\n",
      "Epoch 23/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1340691.7500 - val_loss: 1193775.6250\n",
      "Epoch 24/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1202467.0000 - val_loss: 1061340.8750\n",
      "Epoch 25/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1052292.5000 - val_loss: 912622.3750\n",
      "Epoch 26/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 913290.3750 - val_loss: 748585.1875\n",
      "Epoch 27/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 719188.8750 - val_loss: 579369.3750\n",
      "Epoch 28/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 565174.1875 - val_loss: 418318.4375\n",
      "Epoch 29/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 396390.2812 - val_loss: 280894.3438\n",
      "Epoch 30/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 262978.0000 - val_loss: 183479.7812\n",
      "Epoch 31/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 173290.7188 - val_loss: 132790.7969\n",
      "Epoch 32/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 132856.5938 - val_loss: 120868.8672\n",
      "Epoch 33/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 121422.4688 - val_loss: 126060.8516\n",
      "Epoch 34/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 122785.7500 - val_loss: 130168.7734\n",
      "Epoch 35/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 123610.3203 - val_loss: 127817.1094\n",
      "Epoch 36/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 120911.7266 - val_loss: 124033.8828\n",
      "Epoch 37/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 119661.3281 - val_loss: 121043.5078\n",
      "Epoch 38/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 118611.4297 - val_loss: 119924.3438\n",
      "Epoch 39/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 112770.1641 - val_loss: 119500.2031\n",
      "Epoch 40/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 113168.2188 - val_loss: 119316.7891\n",
      "Epoch 41/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 107949.6094 - val_loss: 119274.2656\n",
      "Epoch 42/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 119327.9531 - val_loss: 119284.8281\n",
      "Epoch 43/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 114057.6250 - val_loss: 119329.4922\n",
      "Epoch 44/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 113090.6562 - val_loss: 119281.2031\n",
      "Epoch 45/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 113067.9297 - val_loss: 119317.4531\n",
      "Epoch 46/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 117229.1016 - val_loss: 119236.3594\n",
      "Epoch 47/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 118189.9453 - val_loss: 119034.3359\n",
      "Epoch 48/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 111518.0938 - val_loss: 118782.9844\n",
      "Epoch 49/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 111316.0547 - val_loss: 118447.6797\n",
      "Epoch 50/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 111458.6641 - val_loss: 118387.9766\n",
      "Epoch 51/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 108308.0859 - val_loss: 118255.8438\n",
      "Epoch 52/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 113826.4453 - val_loss: 118226.5312\n",
      "Epoch 53/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 108585.6406 - val_loss: 117763.9844\n",
      "Epoch 54/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 113985.1641 - val_loss: 117592.0938\n",
      "Epoch 55/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 107426.3047 - val_loss: 117475.6250\n",
      "Epoch 56/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 114861.0469 - val_loss: 117363.6641\n",
      "Epoch 57/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 109262.7734 - val_loss: 117318.8906\n",
      "Epoch 58/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 113927.8047 - val_loss: 117365.7188\n",
      "Epoch 59/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 107354.3516 - val_loss: 117351.7500\n",
      "Epoch 60/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 107263.6016 - val_loss: 117456.5859\n",
      "Epoch 61/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 113249.4922 - val_loss: 117179.8516\n",
      "Epoch 62/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 110680.9453 - val_loss: 116819.9219\n",
      "Epoch 63/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 105580.1719 - val_loss: 116264.2266\n",
      "Epoch 64/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 106576.9531 - val_loss: 116113.8281\n",
      "Epoch 65/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 104338.4609 - val_loss: 116168.9297\n",
      "Epoch 66/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 111210.0547 - val_loss: 116209.3047\n",
      "Epoch 67/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 107347.7500 - val_loss: 116129.4531\n",
      "Epoch 68/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 108181.6719 - val_loss: 116101.9453\n",
      "Epoch 69/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 105451.8906 - val_loss: 115821.2500\n",
      "Epoch 70/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 107928.1172 - val_loss: 115607.0234\n",
      "Epoch 71/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109929.5781 - val_loss: 115356.1094\n",
      "Epoch 72/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 100700.1875 - val_loss: 115255.5703\n",
      "Epoch 73/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 108375.8672 - val_loss: 114933.2812\n",
      "Epoch 74/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 97522.7969 - val_loss: 114726.0938\n",
      "Epoch 75/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 106262.6719 - val_loss: 114761.4141\n",
      "Epoch 76/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 109556.2656 - val_loss: 114688.2969\n",
      "Epoch 77/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 103757.3281 - val_loss: 114433.6641\n",
      "Epoch 78/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 105085.5469 - val_loss: 114355.1094\n",
      "Epoch 79/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 103544.7578 - val_loss: 114213.8125\n",
      "Epoch 80/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 100027.4609 - val_loss: 114022.4766\n",
      "Epoch 81/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 101326.8672 - val_loss: 114124.6953\n",
      "Epoch 82/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 101555.9141 - val_loss: 113886.7891\n",
      "Epoch 83/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 101104.7656 - val_loss: 113919.4141\n",
      "Epoch 84/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 96544.9297 - val_loss: 113799.2109\n",
      "Epoch 85/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 102425.8828 - val_loss: 113524.1328\n",
      "Epoch 86/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 105160.6094 - val_loss: 113284.1328\n",
      "Epoch 87/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 102139.8438 - val_loss: 112921.3203\n",
      "Epoch 88/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 99520.7266 - val_loss: 112656.0391\n",
      "Epoch 89/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 101957.2344 - val_loss: 112758.9297\n",
      "Epoch 90/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 99119.4922 - val_loss: 113075.5625\n",
      "Epoch 91/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 102441.1094 - val_loss: 113005.2109\n",
      "Epoch 92/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 100191.1719 - val_loss: 112961.6797\n",
      "Epoch 93/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 97774.0391 - val_loss: 113227.8516\n",
      "Epoch 94/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 98952.4688 - val_loss: 112884.6250\n",
      "Epoch 95/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 100034.6250 - val_loss: 112453.2422\n",
      "Epoch 96/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 100866.1172 - val_loss: 112059.7500\n",
      "Epoch 97/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 97265.6328 - val_loss: 111852.5312\n",
      "Epoch 98/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 101013.4141 - val_loss: 111737.5000\n",
      "Epoch 99/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 96698.1328 - val_loss: 111424.2500\n",
      "Epoch 100/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 96591.6719 - val_loss: 111763.6875\n",
      "Epoch 101/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 92912.2734 - val_loss: 111684.2812\n",
      "Epoch 102/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 97942.1016 - val_loss: 111926.6562\n",
      "Epoch 103/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 97527.2734 - val_loss: 110915.6172\n",
      "Epoch 104/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 96341.3281 - val_loss: 110335.6016\n",
      "Epoch 105/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 94480.6719 - val_loss: 110326.5703\n",
      "Epoch 106/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 92331.3750 - val_loss: 110840.9219\n",
      "Epoch 107/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 97127.3281 - val_loss: 111671.7734\n",
      "Epoch 108/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 94325.3438 - val_loss: 110976.4141\n",
      "Epoch 109/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 98511.2578 - val_loss: 110096.7969\n",
      "Epoch 110/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 97459.3672 - val_loss: 110091.8516\n",
      "Epoch 111/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 98180.9453 - val_loss: 110394.7734\n",
      "Epoch 112/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 93664.6250 - val_loss: 110730.6172\n",
      "Epoch 113/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90757.1641 - val_loss: 110777.6953\n",
      "Epoch 114/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 93788.2578 - val_loss: 110357.6953\n",
      "Epoch 115/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 93469.3672 - val_loss: 110261.8906\n",
      "Epoch 116/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 90278.9062 - val_loss: 109472.4922\n",
      "Epoch 117/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 93953.6250 - val_loss: 109086.9141\n",
      "Epoch 118/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 92541.0156 - val_loss: 109596.6328\n",
      "Epoch 119/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 94881.6328 - val_loss: 109256.9062\n",
      "Epoch 120/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 91692.0000 - val_loss: 109130.3906\n",
      "Epoch 121/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 92412.0000 - val_loss: 109523.0234\n",
      "Epoch 122/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 94907.5703 - val_loss: 109801.9609\n",
      "Epoch 123/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 94789.9688 - val_loss: 108973.5859\n",
      "Epoch 124/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 89308.4922 - val_loss: 108898.6562\n",
      "Epoch 125/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 93289.0859 - val_loss: 108407.3672\n",
      "Epoch 126/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 94603.3438 - val_loss: 108499.2969\n",
      "Epoch 127/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 89751.4922 - val_loss: 108388.9297\n",
      "Epoch 128/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 93723.0312 - val_loss: 108268.0234\n",
      "Epoch 129/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 93655.5391 - val_loss: 108929.6953\n",
      "Epoch 130/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 92139.3438 - val_loss: 108778.1719\n",
      "Epoch 131/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 95294.2266 - val_loss: 108134.5000\n",
      "Epoch 132/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 97101.0859 - val_loss: 107915.5234\n",
      "Epoch 133/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88403.6719 - val_loss: 108143.0938\n",
      "Epoch 134/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 93337.8672 - val_loss: 107546.4297\n",
      "Epoch 135/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 91181.7266 - val_loss: 107814.6797\n",
      "Epoch 136/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 92304.8750 - val_loss: 108662.5391\n",
      "Epoch 137/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 92361.4062 - val_loss: 108472.3438\n",
      "Epoch 138/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88701.2188 - val_loss: 107966.0156\n",
      "Epoch 139/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 91594.9297 - val_loss: 108382.1484\n",
      "Epoch 140/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87623.1094 - val_loss: 107431.3281\n",
      "Epoch 141/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 89716.1875 - val_loss: 106971.7734\n",
      "Epoch 142/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87443.8203 - val_loss: 107203.6484\n",
      "Epoch 143/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90124.0781 - val_loss: 108892.7734\n",
      "Epoch 144/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88976.6953 - val_loss: 109006.8125\n",
      "Epoch 145/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88378.7812 - val_loss: 107830.6562\n",
      "Epoch 146/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88169.6797 - val_loss: 106960.2969\n",
      "Epoch 147/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88514.5234 - val_loss: 107683.4609\n",
      "Epoch 148/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88376.6562 - val_loss: 108462.4375\n",
      "Epoch 149/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88397.5625 - val_loss: 107616.3438\n",
      "Epoch 150/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 89458.3281 - val_loss: 107276.1172\n",
      "Epoch 151/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88116.4297 - val_loss: 106486.1562\n",
      "Epoch 152/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 90573.2656 - val_loss: 107332.7109\n",
      "Epoch 153/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 91465.7578 - val_loss: 107424.0703\n",
      "Epoch 154/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 90551.9922 - val_loss: 106937.1172\n",
      "Epoch 155/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87490.3672 - val_loss: 106950.5234\n",
      "Epoch 156/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86067.4141 - val_loss: 107823.3594\n",
      "Epoch 157/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88817.2969 - val_loss: 107334.3438\n",
      "Epoch 158/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 92010.3672 - val_loss: 107556.0234\n",
      "Epoch 159/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87824.3438 - val_loss: 107319.2188\n",
      "Epoch 160/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 90772.9844 - val_loss: 106852.8828\n",
      "Epoch 161/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 91762.2734 - val_loss: 106699.5312\n",
      "Epoch 162/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 90271.6016 - val_loss: 106856.0156\n",
      "Epoch 163/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86753.5469 - val_loss: 107061.0234\n",
      "Epoch 164/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 89055.7031 - val_loss: 107556.5234\n",
      "Epoch 165/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88181.1953 - val_loss: 107825.3359\n",
      "Epoch 166/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88867.9141 - val_loss: 106848.7188\n",
      "Epoch 167/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85567.2891 - val_loss: 106588.5625\n",
      "Epoch 168/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87993.6641 - val_loss: 106918.8828\n",
      "Epoch 169/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 90745.2500 - val_loss: 107044.3984\n",
      "Epoch 170/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 91207.8438 - val_loss: 106789.5703\n",
      "Epoch 171/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 89490.1094 - val_loss: 106709.9844\n",
      "Epoch 172/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 84074.2109 - val_loss: 106910.7500\n",
      "Epoch 173/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87034.6328 - val_loss: 107044.6562\n",
      "Epoch 174/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87129.0859 - val_loss: 107149.7500\n",
      "Epoch 175/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88499.0859 - val_loss: 106745.6562\n",
      "Epoch 176/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 89577.4766 - val_loss: 106516.9297\n",
      "Epoch 177/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86448.2578 - val_loss: 106857.6406\n",
      "Epoch 178/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88581.5859 - val_loss: 106705.3359\n",
      "Epoch 179/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88693.4141 - val_loss: 106905.8594\n",
      "Epoch 180/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87751.1172 - val_loss: 107308.2031\n",
      "Epoch 181/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86309.2422 - val_loss: 106626.4219\n",
      "Epoch 182/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86599.4531 - val_loss: 106964.7188\n",
      "Epoch 183/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85839.4375 - val_loss: 107332.5781\n",
      "Epoch 184/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85554.9141 - val_loss: 106322.4609\n",
      "Epoch 185/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88142.4609 - val_loss: 106278.7500\n",
      "Epoch 186/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86551.3672 - val_loss: 105982.1875\n",
      "Epoch 187/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87625.8672 - val_loss: 106557.7500\n",
      "Epoch 188/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85505.3594 - val_loss: 107484.1875\n",
      "Epoch 189/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86997.8984 - val_loss: 107016.8203\n",
      "Epoch 190/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87482.4219 - val_loss: 106846.6562\n",
      "Epoch 191/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87853.2422 - val_loss: 107077.5312\n",
      "Epoch 192/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87094.2734 - val_loss: 106660.5703\n",
      "Epoch 193/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88825.6641 - val_loss: 106133.1562\n",
      "Epoch 194/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 84206.6719 - val_loss: 106272.0938\n",
      "Epoch 195/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 83836.5625 - val_loss: 106787.1172\n",
      "Epoch 196/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85421.7031 - val_loss: 107815.1250\n",
      "Epoch 197/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 88330.6094 - val_loss: 106610.9375\n",
      "Epoch 198/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81423.0391 - val_loss: 106279.7188\n",
      "Epoch 199/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85438.4219 - val_loss: 107305.0859\n",
      "Epoch 200/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 82476.5000 - val_loss: 106962.1562\n",
      "Epoch 201/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 90603.6406 - val_loss: 107528.1484\n",
      "Epoch 202/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86031.4766 - val_loss: 106727.3984\n",
      "Epoch 203/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86305.4297 - val_loss: 105633.8516\n",
      "Epoch 204/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86083.0859 - val_loss: 105563.7188\n",
      "Epoch 205/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86882.9297 - val_loss: 106143.7031\n",
      "Epoch 206/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86079.1562 - val_loss: 106130.7188\n",
      "Epoch 207/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81792.4531 - val_loss: 106634.1953\n",
      "Epoch 208/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 82461.5469 - val_loss: 105857.6953\n",
      "Epoch 209/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85851.9844 - val_loss: 106378.3359\n",
      "Epoch 210/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87306.9297 - val_loss: 105905.6328\n",
      "Epoch 211/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81190.2188 - val_loss: 105869.3516\n",
      "Epoch 212/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86556.0781 - val_loss: 106260.4922\n",
      "Epoch 213/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86000.7344 - val_loss: 106227.0781\n",
      "Epoch 214/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85675.0000 - val_loss: 106383.5859\n",
      "Epoch 215/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85777.0781 - val_loss: 106176.0859\n",
      "Epoch 216/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 83163.2188 - val_loss: 105471.5938\n",
      "Epoch 217/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85127.0391 - val_loss: 105504.0391\n",
      "Epoch 218/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86620.9531 - val_loss: 106412.8906\n",
      "Epoch 219/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84328.0156 - val_loss: 105709.7344\n",
      "Epoch 220/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 83284.3359 - val_loss: 105784.6797\n",
      "Epoch 221/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85185.8047 - val_loss: 105613.4375\n",
      "Epoch 222/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 83336.2656 - val_loss: 104327.3047\n",
      "Epoch 223/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 83185.4688 - val_loss: 104346.5000\n",
      "Epoch 224/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 84686.7266 - val_loss: 104634.6562\n",
      "Epoch 225/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 82040.1016 - val_loss: 104976.4844\n",
      "Epoch 226/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 84764.0547 - val_loss: 104545.0156\n",
      "Epoch 227/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 83177.0469 - val_loss: 104500.7500\n",
      "Epoch 228/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80819.2109 - val_loss: 105105.6172\n",
      "Epoch 229/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 84203.3984 - val_loss: 104019.3047\n",
      "Epoch 230/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 82968.8359 - val_loss: 102950.7422\n",
      "Epoch 231/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 79675.2656 - val_loss: 103562.2266\n",
      "Epoch 232/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 82387.4531 - val_loss: 104459.3828\n",
      "Epoch 233/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78427.5859 - val_loss: 103054.7891\n",
      "Epoch 234/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81623.2422 - val_loss: 102111.2891\n",
      "Epoch 235/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81422.2109 - val_loss: 102874.1172\n",
      "Epoch 236/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 79313.7812 - val_loss: 103003.6562\n",
      "Epoch 237/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81301.5391 - val_loss: 103162.5234\n",
      "Epoch 238/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 82489.8594 - val_loss: 102329.6250\n",
      "Epoch 239/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81030.8594 - val_loss: 101307.2812\n",
      "Epoch 240/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80380.3125 - val_loss: 100902.6250\n",
      "Epoch 241/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81890.7188 - val_loss: 101354.3906\n",
      "Epoch 242/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 79562.9219 - val_loss: 100665.0000\n",
      "Epoch 243/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77625.0625 - val_loss: 101101.3438\n",
      "Epoch 244/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 80161.5000 - val_loss: 100872.1953\n",
      "Epoch 245/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78748.7266 - val_loss: 100517.2969\n",
      "Epoch 246/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78882.8672 - val_loss: 100241.2188\n",
      "Epoch 247/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80534.6250 - val_loss: 99975.7266\n",
      "Epoch 248/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81946.6328 - val_loss: 99176.4453\n",
      "Epoch 249/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73842.1562 - val_loss: 99750.2578\n",
      "Epoch 250/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80967.1328 - val_loss: 98934.4766\n",
      "Epoch 251/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76059.8438 - val_loss: 98468.6641\n",
      "Epoch 252/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77324.1953 - val_loss: 99780.0859\n",
      "Epoch 253/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77331.3203 - val_loss: 98575.8438\n",
      "Epoch 254/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75489.4531 - val_loss: 97515.6328\n",
      "Epoch 255/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75980.2734 - val_loss: 97148.5703\n",
      "Epoch 256/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77429.1641 - val_loss: 98351.9219\n",
      "Epoch 257/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76694.1016 - val_loss: 97679.3594\n",
      "Epoch 258/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75790.0547 - val_loss: 97255.3125\n",
      "Epoch 259/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 78817.1094 - val_loss: 96477.9844\n",
      "Epoch 260/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75174.9531 - val_loss: 96433.7734\n",
      "Epoch 261/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76022.9766 - val_loss: 96180.3125\n",
      "Epoch 262/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71092.1719 - val_loss: 96278.4922\n",
      "Epoch 263/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73418.7109 - val_loss: 96080.9375\n",
      "Epoch 264/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72124.9453 - val_loss: 95502.0234\n",
      "Epoch 265/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70119.4531 - val_loss: 95308.5312\n",
      "Epoch 266/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71557.8906 - val_loss: 95177.7812\n",
      "Epoch 267/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72630.7656 - val_loss: 94780.7891\n",
      "Epoch 268/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74365.6641 - val_loss: 94055.9766\n",
      "Epoch 269/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70869.9766 - val_loss: 93501.6797\n",
      "Epoch 270/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73513.5469 - val_loss: 95142.1250\n",
      "Epoch 271/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70769.2891 - val_loss: 93820.2812\n",
      "Epoch 272/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70779.5078 - val_loss: 93290.3281\n",
      "Epoch 273/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72066.4453 - val_loss: 92663.8828\n",
      "Epoch 274/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 67119.5469 - val_loss: 92584.9531\n",
      "Epoch 275/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71601.9297 - val_loss: 92061.4688\n",
      "Epoch 276/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 66565.4609 - val_loss: 91874.7656\n",
      "Epoch 277/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70437.8750 - val_loss: 92722.0547\n",
      "Epoch 278/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 69414.5625 - val_loss: 91252.7188\n",
      "Epoch 279/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 70880.9844 - val_loss: 90387.0625\n",
      "Epoch 280/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 67892.5000 - val_loss: 91754.7109\n",
      "Epoch 281/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 68536.3672 - val_loss: 91028.6953\n",
      "Epoch 282/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71688.6016 - val_loss: 89735.6719\n",
      "Epoch 283/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 66906.6328 - val_loss: 89117.7344\n",
      "Epoch 284/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 65075.0078 - val_loss: 89160.1797\n",
      "Epoch 285/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 68623.7578 - val_loss: 88886.4688\n",
      "Epoch 286/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 64335.9023 - val_loss: 89172.7656\n",
      "Epoch 287/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 66824.2031 - val_loss: 88214.5234\n",
      "Epoch 288/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 66523.3828 - val_loss: 88341.3281\n",
      "Epoch 289/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 68799.1406 - val_loss: 88477.3594\n",
      "Epoch 290/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 65907.7344 - val_loss: 87268.8047\n",
      "Epoch 291/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 64838.1680 - val_loss: 86599.7344\n",
      "Epoch 292/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 66181.0234 - val_loss: 85951.4375\n",
      "Epoch 293/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 64101.2852 - val_loss: 86903.6406\n",
      "Epoch 294/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 66199.9375 - val_loss: 86008.4922\n",
      "Epoch 295/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 65535.1250 - val_loss: 85642.1172\n",
      "Epoch 296/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 63943.4062 - val_loss: 85821.9141\n",
      "Epoch 297/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 63340.4922 - val_loss: 85397.1484\n",
      "Epoch 298/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 63910.3594 - val_loss: 84430.8438\n",
      "Epoch 299/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66075.0234 - val_loss: 84078.2109\n",
      "Epoch 300/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 63639.4062 - val_loss: 83825.9922\n",
      "Epoch 301/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 62975.6562 - val_loss: 83578.4141\n",
      "Epoch 302/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 58682.4609 - val_loss: 82972.7031\n",
      "Epoch 303/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 60287.1055 - val_loss: 83001.5703\n",
      "Epoch 304/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 61975.5352 - val_loss: 82748.6875\n",
      "Epoch 305/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 60443.3125 - val_loss: 81560.2500\n",
      "Epoch 306/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 60220.7773 - val_loss: 81800.3047\n",
      "Epoch 307/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 58805.0586 - val_loss: 81229.9453\n",
      "Epoch 308/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 59852.7852 - val_loss: 81205.5547\n",
      "Epoch 309/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 57912.6016 - val_loss: 80592.3281\n",
      "Epoch 310/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 59521.8516 - val_loss: 80102.9219\n",
      "Epoch 311/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 56339.7461 - val_loss: 79502.3203\n",
      "Epoch 312/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 57887.4844 - val_loss: 79047.2031\n",
      "Epoch 313/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 58951.8477 - val_loss: 78648.1094\n",
      "Epoch 314/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 53451.5859 - val_loss: 78054.7578\n",
      "Epoch 315/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 54847.8359 - val_loss: 77418.4609\n",
      "Epoch 316/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 58148.0430 - val_loss: 77960.7891\n",
      "Epoch 317/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 56175.3906 - val_loss: 77180.7656\n",
      "Epoch 318/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 57293.7461 - val_loss: 75291.1875\n",
      "Epoch 319/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 57159.5664 - val_loss: 74846.4297\n",
      "Epoch 320/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 53977.5625 - val_loss: 75263.5234\n",
      "Epoch 321/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52377.1602 - val_loss: 74873.1953\n",
      "Epoch 322/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 55750.6992 - val_loss: 74109.4531\n",
      "Epoch 323/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52157.2539 - val_loss: 73363.2891\n",
      "Epoch 324/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52572.7500 - val_loss: 73403.0859\n",
      "Epoch 325/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52650.5820 - val_loss: 73681.7500\n",
      "Epoch 326/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49757.2539 - val_loss: 71966.2891\n",
      "Epoch 327/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 52607.1523 - val_loss: 70655.7188\n",
      "Epoch 328/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 51512.8633 - val_loss: 70993.8438\n",
      "Epoch 329/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 53243.9570 - val_loss: 70573.0078\n",
      "Epoch 330/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 52566.5898 - val_loss: 69721.0156\n",
      "Epoch 331/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51664.1719 - val_loss: 70178.1328\n",
      "Epoch 332/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 50274.1172 - val_loss: 68808.9922\n",
      "Epoch 333/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 48319.7695 - val_loss: 68242.9531\n",
      "Epoch 334/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50266.6641 - val_loss: 68054.5469\n",
      "Epoch 335/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 48062.0352 - val_loss: 66600.6172\n",
      "Epoch 336/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49445.5469 - val_loss: 66876.1953\n",
      "Epoch 337/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 47980.4414 - val_loss: 66419.8047\n",
      "Epoch 338/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 47482.6445 - val_loss: 65554.8750\n",
      "Epoch 339/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46754.5273 - val_loss: 64655.4141\n",
      "Epoch 340/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 44059.7578 - val_loss: 64058.2266\n",
      "Epoch 341/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 46713.9023 - val_loss: 63591.2266\n",
      "Epoch 342/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 44362.9414 - val_loss: 63192.6992\n",
      "Epoch 343/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 45748.9570 - val_loss: 63090.4648\n",
      "Epoch 344/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 44353.9805 - val_loss: 61851.4414\n",
      "Epoch 345/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46397.7539 - val_loss: 61833.6719\n",
      "Epoch 346/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 44368.1055 - val_loss: 60712.9883\n",
      "Epoch 347/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 43646.3242 - val_loss: 60406.9414\n",
      "Epoch 348/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41764.7500 - val_loss: 59727.8789\n",
      "Epoch 349/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42388.2305 - val_loss: 59848.0000\n",
      "Epoch 350/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41495.1680 - val_loss: 58492.7344\n",
      "Epoch 351/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39929.3047 - val_loss: 58132.9648\n",
      "Epoch 352/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 44210.4961 - val_loss: 57805.5469\n",
      "Epoch 353/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42018.9805 - val_loss: 57285.1055\n",
      "Epoch 354/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41339.4688 - val_loss: 56885.8398\n",
      "Epoch 355/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41931.3477 - val_loss: 55517.9219\n",
      "Epoch 356/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 39061.3438 - val_loss: 56004.3516\n",
      "Epoch 357/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 40144.3164 - val_loss: 55680.8516\n",
      "Epoch 358/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 39096.5078 - val_loss: 53796.6250\n",
      "Epoch 359/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39100.1797 - val_loss: 53813.2930\n",
      "Epoch 360/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39060.6367 - val_loss: 53407.3047\n",
      "Epoch 361/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39424.5078 - val_loss: 53029.5234\n",
      "Epoch 362/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38184.7852 - val_loss: 52455.6602\n",
      "Epoch 363/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 40483.6562 - val_loss: 52672.1367\n",
      "Epoch 364/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36464.8906 - val_loss: 50980.6055\n",
      "Epoch 365/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37034.6602 - val_loss: 50900.0508\n",
      "Epoch 366/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38992.4180 - val_loss: 50717.9688\n",
      "Epoch 367/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37983.0000 - val_loss: 49590.8125\n",
      "Epoch 368/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36763.6016 - val_loss: 49724.1367\n",
      "Epoch 369/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35823.9961 - val_loss: 48801.8008\n",
      "Epoch 370/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 33057.5391 - val_loss: 49127.7266\n",
      "Epoch 371/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 34649.3867 - val_loss: 48226.7773\n",
      "Epoch 372/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 35653.8555 - val_loss: 47410.5781\n",
      "Epoch 373/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 33828.2695 - val_loss: 47011.5234\n",
      "Epoch 374/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 33679.8086 - val_loss: 47470.9688\n",
      "Epoch 375/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 33121.9219 - val_loss: 46432.0039\n",
      "Epoch 376/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 32607.0625 - val_loss: 45548.8320\n",
      "Epoch 377/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 33655.1133 - val_loss: 45991.5352\n",
      "Epoch 378/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30559.8301 - val_loss: 45417.9219\n",
      "Epoch 379/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32094.4023 - val_loss: 44245.1484\n",
      "Epoch 380/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 32693.2363 - val_loss: 44265.9453\n",
      "Epoch 381/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 31732.9180 - val_loss: 44024.5781\n",
      "Epoch 382/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30451.9766 - val_loss: 43432.8047\n",
      "Epoch 383/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 31703.3164 - val_loss: 43213.3750\n",
      "Epoch 384/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31828.5801 - val_loss: 42518.5273\n",
      "Epoch 385/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30363.5156 - val_loss: 42299.9141\n",
      "Epoch 386/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31627.2812 - val_loss: 41936.6914\n",
      "Epoch 387/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 31106.5820 - val_loss: 41190.7305\n",
      "Epoch 388/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31291.1914 - val_loss: 42168.8594\n",
      "Epoch 389/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 28955.8242 - val_loss: 40880.5156\n",
      "Epoch 390/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28983.8301 - val_loss: 40114.9805\n",
      "Epoch 391/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 30421.7832 - val_loss: 40236.2500\n",
      "Epoch 392/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28931.7930 - val_loss: 40310.1055\n",
      "Epoch 393/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29799.1152 - val_loss: 39314.3320\n",
      "Epoch 394/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 28168.2676 - val_loss: 39045.9805\n",
      "Epoch 395/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28058.4688 - val_loss: 39051.3984\n",
      "Epoch 396/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30073.2559 - val_loss: 38643.8047\n",
      "Epoch 397/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30145.5430 - val_loss: 38253.8594\n",
      "Epoch 398/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28667.2207 - val_loss: 37843.7617\n",
      "Epoch 399/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27070.3672 - val_loss: 37661.8359\n",
      "Epoch 400/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27537.2773 - val_loss: 37269.8828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x311ec4b20>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,y=y_train.values,\n",
    "          validation_data=(X_test,y_test.values),\n",
    "          batch_size=128,epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MAE:  152.39851504720048\n",
      "MSE:  37269.88456688071\n",
      "RMSE:  193.05409751383345\n",
      "Variance Regression Score:  0.6907014363383501\n",
      "\n",
      "\n",
      "Descriptive Statistics:\n",
      " count    1000.000000\n",
      "mean     1298.556320\n",
      "std       340.071434\n",
      "min       417.990000\n",
      "25%      1047.257500\n",
      "50%      1266.040000\n",
      "75%      1563.050000\n",
      "max      2134.530000\n",
      "Name: Lifespan, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print('MAE: ',mean_absolute_error(y_test,predictions))\n",
    "print('MSE: ',mean_squared_error(y_test,predictions))\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(y_test,predictions)))\n",
    "print('Variance Regression Score: ',explained_variance_score(y_test,predictions))\n",
    "\n",
    "print('\\n\\nDescriptive Statistics:\\n',df['Lifespan'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features of new part type:\n",
      "coolingRate                   13.00\n",
      "quenchTime                     3.84\n",
      "forgeTime                      6.47\n",
      "HeatTreatTime                 46.87\n",
      "Nickel%                       65.73\n",
      "Iron%                         16.52\n",
      "Cobalt%                       16.82\n",
      "Chromium%                      0.93\n",
      "smallDefects                  10.00\n",
      "largeDefects                   0.00\n",
      "sliverDefects                  0.00\n",
      "partType_Blade                 0.00\n",
      "partType_Block                 0.00\n",
      "partType_Nozzle                1.00\n",
      "partType_Valve                 0.00\n",
      "microstructure_colGrain        0.00\n",
      "microstructure_equiGrain       1.00\n",
      "microstructure_singleGrain     0.00\n",
      "seedLocation_Bottom            1.00\n",
      "seedLocation_Top               0.00\n",
      "castType_Continuous            0.00\n",
      "castType_Die                   1.00\n",
      "castType_Investment            0.00\n",
      "Name: 0, dtype: float64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\n",
      "Prediction Lifespan: 1493.33\n",
      "\n",
      "Original Lifespan: 1469.17\n"
     ]
    }
   ],
   "source": [
    "# Get features of new part type\n",
    "single_partType = df.drop('Lifespan', axis=1).iloc[0]\n",
    "print(f'Features of new part type:\\n{single_partType}')\n",
    "\n",
    "# Convert to DataFrame with feature names\n",
    "single_partType_df = pd.DataFrame([single_partType.values], columns=single_partType.index)\n",
    "\n",
    "# Scale the features while preserving feature names\n",
    "single_partType_scaled = scaler.transform(single_partType_df)\n",
    "\n",
    "# Run the model and get the lifespan prediction\n",
    "print('\\nPrediction Lifespan:', model.predict(single_partType_scaled)[0,0])\n",
    "\n",
    "# Print original lifespan\n",
    "print('\\nOriginal Lifespan:', df.iloc[0]['Lifespan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations to test: 144\n",
      "Starting grid search with cross-validation...\n",
      "\n",
      "Combination 1/144:\n",
      "Mean Validation Loss: 107815.3125 (±6571.0783)\n",
      "Time taken: 20.67 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 2/144:\n",
      "Mean Validation Loss: 97337.1453 (±7415.7840)\n",
      "Time taken: 34.69 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 3/144:\n",
      "Mean Validation Loss: 114695.5859 (±8366.2648)\n",
      "Time taken: 17.22 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 4/144:\n",
      "Mean Validation Loss: 104705.5703 (±8082.9146)\n",
      "Time taken: 32.25 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 5/144:\n",
      "Mean Validation Loss: 92407.3734 (±7388.9524)\n",
      "Time taken: 18.18 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 6/144:\n",
      "Mean Validation Loss: 92570.0922 (±7479.7109)\n",
      "Time taken: 34.99 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 7/144:\n",
      "Mean Validation Loss: 92915.6531 (±7385.4989)\n",
      "Time taken: 17.23 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 8/144:\n",
      "Mean Validation Loss: 91983.8859 (±7955.9720)\n",
      "Time taken: 32.68 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 9/144:\n",
      "Mean Validation Loss: 1718023.0000 (±43880.2942)\n",
      "Time taken: 18.33 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 10/144:\n",
      "Mean Validation Loss: 1641579.1750 (±44708.1101)\n",
      "Time taken: 35.09 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 11/144:\n",
      "Mean Validation Loss: 1753058.4250 (±45858.1393)\n",
      "Time taken: 17.09 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 12/144:\n",
      "Mean Validation Loss: 1710699.0500 (±45157.7179)\n",
      "Time taken: 32.39 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 13/144:\n",
      "Mean Validation Loss: 1125151.5500 (±36474.4761)\n",
      "Time taken: 18.15 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 14/144:\n",
      "Mean Validation Loss: 660119.3375 (±27415.4853)\n",
      "Time taken: 34.73 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 15/144:\n",
      "Mean Validation Loss: 1394379.0750 (±40016.3479)\n",
      "Time taken: 17.07 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 16/144:\n",
      "Mean Validation Loss: 1065059.9750 (±34302.4078)\n",
      "Time taken: 32.72 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 17/144:\n",
      "Mean Validation Loss: 99729.2656 (±6740.7126)\n",
      "Time taken: 18.18 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 18/144:\n",
      "Mean Validation Loss: 93814.8313 (±7108.4138)\n",
      "Time taken: 40.19 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 19/144:\n",
      "Mean Validation Loss: 109343.1641 (±6718.5904)\n",
      "Time taken: 17.33 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 20/144:\n",
      "Mean Validation Loss: 99311.7594 (±6440.9136)\n",
      "Time taken: 32.78 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 21/144:\n",
      "Mean Validation Loss: 92681.7297 (±7445.6159)\n",
      "Time taken: 18.62 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 22/144:\n",
      "Mean Validation Loss: 92267.3109 (±7830.6045)\n",
      "Time taken: 35.66 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 23/144:\n",
      "Mean Validation Loss: 92560.8156 (±7775.7219)\n",
      "Time taken: 17.28 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 24/144:\n",
      "Mean Validation Loss: 92786.6062 (±7861.5721)\n",
      "Time taken: 32.95 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 25/144:\n",
      "Mean Validation Loss: 1640141.3000 (±43043.4141)\n",
      "Time taken: 18.77 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 26/144:\n",
      "Mean Validation Loss: 1499466.1000 (±40351.8820)\n",
      "Time taken: 36.20 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 27/144:\n",
      "Mean Validation Loss: 1705149.3750 (±44462.0185)\n",
      "Time taken: 17.23 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 28/144:\n",
      "Mean Validation Loss: 1624048.6750 (±44868.9908)\n",
      "Time taken: 32.92 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 29/144:\n",
      "Mean Validation Loss: 692976.5750 (±27417.1922)\n",
      "Time taken: 18.69 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 30/144:\n",
      "Mean Validation Loss: 232125.8906 (±14067.9565)\n",
      "Time taken: 36.28 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 31/144:\n",
      "Mean Validation Loss: 1081370.0250 (±34515.6643)\n",
      "Time taken: 17.90 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 32/144:\n",
      "Mean Validation Loss: 615562.3500 (±26078.3732)\n",
      "Time taken: 32.48 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 33/144:\n",
      "Mean Validation Loss: 95390.9422 (±6703.4669)\n",
      "Time taken: 18.35 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 34/144:\n",
      "Mean Validation Loss: 92953.0250 (±7456.5365)\n",
      "Time taken: 35.06 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 35/144:\n",
      "Mean Validation Loss: 102913.9156 (±6453.3710)\n",
      "Time taken: 17.14 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 36/144:\n",
      "Mean Validation Loss: 95114.5781 (±7232.2556)\n",
      "Time taken: 32.83 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 37/144:\n",
      "Mean Validation Loss: 92584.7672 (±7726.3859)\n",
      "Time taken: 18.48 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 38/144:\n",
      "Mean Validation Loss: 80240.2105 (±30452.3213)\n",
      "Time taken: 35.62 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 39/144:\n",
      "Mean Validation Loss: 92301.3687 (±7772.2901)\n",
      "Time taken: 17.19 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 40/144:\n",
      "Mean Validation Loss: 79493.8160 (±29239.3845)\n",
      "Time taken: 32.56 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 41/144:\n",
      "Mean Validation Loss: 1496046.2250 (±42456.7464)\n",
      "Time taken: 18.46 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 42/144:\n",
      "Mean Validation Loss: 1244335.4000 (±36960.1663)\n",
      "Time taken: 35.13 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 43/144:\n",
      "Mean Validation Loss: 1616187.4750 (±43496.2954)\n",
      "Time taken: 17.23 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 44/144:\n",
      "Mean Validation Loss: 1466566.1250 (±42107.3761)\n",
      "Time taken: 32.63 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 45/144:\n",
      "Mean Validation Loss: 258193.5094 (±15217.4370)\n",
      "Time taken: 18.48 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 46/144:\n",
      "Mean Validation Loss: 114467.0125 (±5205.3917)\n",
      "Time taken: 35.38 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 47/144:\n",
      "Mean Validation Loss: 630607.8125 (±25788.8500)\n",
      "Time taken: 17.27 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 48/144:\n",
      "Mean Validation Loss: 211616.0625 (±13202.1667)\n",
      "Time taken: 32.77 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 49/144:\n",
      "Mean Validation Loss: 95701.7969 (±6358.2759)\n",
      "Time taken: 18.78 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 50/144:\n",
      "Mean Validation Loss: 89115.6687 (±10747.1436)\n",
      "Time taken: 35.59 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 51/144:\n",
      "Mean Validation Loss: 104027.1375 (±5626.9681)\n",
      "Time taken: 17.36 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 52/144:\n",
      "Mean Validation Loss: 92997.1828 (±7791.8969)\n",
      "Time taken: 32.93 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 53/144:\n",
      "Mean Validation Loss: 92338.2797 (±7565.5640)\n",
      "Time taken: 25.22 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 54/144:\n",
      "Mean Validation Loss: 77606.9016 (±29698.3014)\n",
      "Time taken: 40.17 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 55/144:\n",
      "Mean Validation Loss: 91863.2563 (±7045.1507)\n",
      "Time taken: 19.61 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 56/144:\n",
      "Mean Validation Loss: 92547.5984 (±7444.2300)\n",
      "Time taken: 34.71 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 57/144:\n",
      "Mean Validation Loss: 1718578.6750 (±44116.8778)\n",
      "Time taken: 18.82 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 58/144:\n",
      "Mean Validation Loss: 1642986.5000 (±43859.8355)\n",
      "Time taken: 35.28 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 59/144:\n",
      "Mean Validation Loss: 1754032.4000 (±45598.5785)\n",
      "Time taken: 17.27 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 60/144:\n",
      "Mean Validation Loss: 1710574.6500 (±45244.4384)\n",
      "Time taken: 32.49 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 61/144:\n",
      "Mean Validation Loss: 1126463.1000 (±36413.8596)\n",
      "Time taken: 18.67 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 62/144:\n",
      "Mean Validation Loss: 660223.9375 (±27690.0720)\n",
      "Time taken: 35.35 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 63/144:\n",
      "Mean Validation Loss: 1396285.8250 (±40503.4619)\n",
      "Time taken: 17.44 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 64/144:\n",
      "Mean Validation Loss: 1065108.0250 (±34435.8552)\n",
      "Time taken: 32.70 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 65/144:\n",
      "Mean Validation Loss: 89866.2234 (±12415.3649)\n",
      "Time taken: 18.83 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 66/144:\n",
      "Mean Validation Loss: 81573.0516 (±22338.8773)\n",
      "Time taken: 35.52 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 67/144:\n",
      "Mean Validation Loss: 97737.2312 (±6683.4315)\n",
      "Time taken: 17.53 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 68/144:\n",
      "Mean Validation Loss: 89234.8266 (±13027.4901)\n",
      "Time taken: 32.74 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 69/144:\n",
      "Mean Validation Loss: 79309.4070 (±30077.4873)\n",
      "Time taken: 18.71 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 70/144:\n",
      "Mean Validation Loss: 92679.6766 (±7115.8838)\n",
      "Time taken: 35.18 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 71/144:\n",
      "Mean Validation Loss: 81935.9512 (±27059.6937)\n",
      "Time taken: 17.39 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 72/144:\n",
      "Mean Validation Loss: 75861.7141 (±26785.7154)\n",
      "Time taken: 32.91 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 73/144:\n",
      "Mean Validation Loss: 1641151.3500 (±44448.4437)\n",
      "Time taken: 18.81 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 74/144:\n",
      "Mean Validation Loss: 1499978.4750 (±40066.3092)\n",
      "Time taken: 35.50 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 75/144:\n",
      "Mean Validation Loss: 1706024.9500 (±45174.8829)\n",
      "Time taken: 17.46 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 76/144:\n",
      "Mean Validation Loss: 1624549.0000 (±44673.7019)\n",
      "Time taken: 33.04 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 77/144:\n",
      "Mean Validation Loss: 693960.1750 (±27646.7262)\n",
      "Time taken: 18.69 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 78/144:\n",
      "Mean Validation Loss: 232573.5875 (±14349.0583)\n",
      "Time taken: 35.57 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 79/144:\n",
      "Mean Validation Loss: 1082293.1625 (±35612.3980)\n",
      "Time taken: 17.68 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 80/144:\n",
      "Mean Validation Loss: 616520.8750 (±26726.1417)\n",
      "Time taken: 32.93 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 81/144:\n",
      "Mean Validation Loss: 89367.0766 (±11853.9685)\n",
      "Time taken: 19.27 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 82/144:\n",
      "Mean Validation Loss: 76763.1668 (±27264.4364)\n",
      "Time taken: 36.43 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 83/144:\n",
      "Mean Validation Loss: 88528.9328 (±10710.3774)\n",
      "Time taken: 17.88 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 84/144:\n",
      "Mean Validation Loss: 86482.3570 (±15722.9774)\n",
      "Time taken: 33.96 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 85/144:\n",
      "Mean Validation Loss: 80418.7953 (±29276.2539)\n",
      "Time taken: 19.25 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 86/144:\n",
      "Mean Validation Loss: 25824.7824 (±6800.6291)\n",
      "Time taken: 36.80 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 87/144:\n",
      "Mean Validation Loss: 67599.8723 (±32207.2836)\n",
      "Time taken: 18.19 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 88/144:\n",
      "Mean Validation Loss: 66671.4496 (±34964.4230)\n",
      "Time taken: 34.02 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 89/144:\n",
      "Mean Validation Loss: 1497823.7500 (±43242.0257)\n",
      "Time taken: 25.95 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 90/144:\n",
      "Mean Validation Loss: 1247155.1000 (±37249.3064)\n",
      "Time taken: 36.71 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 91/144:\n",
      "Mean Validation Loss: 1620433.0000 (±44070.2438)\n",
      "Time taken: 17.97 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 92/144:\n",
      "Mean Validation Loss: 1469768.5250 (±41452.3661)\n",
      "Time taken: 33.57 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 93/144:\n",
      "Mean Validation Loss: 258910.3219 (±15611.3198)\n",
      "Time taken: 19.50 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 94/144:\n",
      "Mean Validation Loss: 114488.0875 (±5210.6772)\n",
      "Time taken: 37.26 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 95/144:\n",
      "Mean Validation Loss: 631539.3250 (±25925.6748)\n",
      "Time taken: 18.50 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 96/144:\n",
      "Mean Validation Loss: 212171.1688 (±12942.0450)\n",
      "Time taken: 34.67 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 97/144:\n",
      "Mean Validation Loss: 93177.8141 (±6752.7010)\n",
      "Time taken: 20.22 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 98/144:\n",
      "Mean Validation Loss: 83368.8055 (±25331.1397)\n",
      "Time taken: 36.93 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 99/144:\n",
      "Mean Validation Loss: 98651.1078 (±7167.7710)\n",
      "Time taken: 18.33 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 100/144:\n",
      "Mean Validation Loss: 83778.9469 (±17038.5869)\n",
      "Time taken: 33.93 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 101/144:\n",
      "Mean Validation Loss: 91822.4937 (±7461.8442)\n",
      "Time taken: 19.92 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 102/144:\n",
      "Mean Validation Loss: 92218.1281 (±6829.5909)\n",
      "Time taken: 37.32 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 103/144:\n",
      "Mean Validation Loss: 78433.3789 (±29095.7095)\n",
      "Time taken: 18.81 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 104/144:\n",
      "Mean Validation Loss: 91881.1031 (±7519.6069)\n",
      "Time taken: 33.44 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 105/144:\n",
      "Mean Validation Loss: 1718907.3250 (±44943.4560)\n",
      "Time taken: 19.10 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 106/144:\n",
      "Mean Validation Loss: 1642887.6250 (±43981.9297)\n",
      "Time taken: 35.85 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 107/144:\n",
      "Mean Validation Loss: 1754783.6750 (±44307.9356)\n",
      "Time taken: 17.98 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 108/144:\n",
      "Mean Validation Loss: 1711506.5250 (±45201.2771)\n",
      "Time taken: 33.35 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 109/144:\n",
      "Mean Validation Loss: 1126835.0250 (±36724.4922)\n",
      "Time taken: 19.12 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 110/144:\n",
      "Mean Validation Loss: 659994.3000 (±27917.1045)\n",
      "Time taken: 35.88 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 111/144:\n",
      "Mean Validation Loss: 1394359.2000 (±40322.3488)\n",
      "Time taken: 17.75 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 112/144:\n",
      "Mean Validation Loss: 1066794.0250 (±34899.1126)\n",
      "Time taken: 33.47 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 113/144:\n",
      "Mean Validation Loss: 87012.6766 (±12369.5192)\n",
      "Time taken: 19.29 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 114/144:\n",
      "Mean Validation Loss: 50730.1645 (±34703.5215)\n",
      "Time taken: 36.06 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 115/144:\n",
      "Mean Validation Loss: 93562.8781 (±6480.9069)\n",
      "Time taken: 18.11 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 116/144:\n",
      "Mean Validation Loss: 74076.5699 (±26504.4763)\n",
      "Time taken: 33.64 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 117/144:\n",
      "Mean Validation Loss: 78411.4086 (±27672.6227)\n",
      "Time taken: 19.59 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 118/144:\n",
      "Mean Validation Loss: 32337.6969 (±13986.1566)\n",
      "Time taken: 36.40 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 119/144:\n",
      "Mean Validation Loss: 76623.1605 (±27718.2072)\n",
      "Time taken: 18.18 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 120/144:\n",
      "Mean Validation Loss: 93015.8250 (±8170.3779)\n",
      "Time taken: 33.78 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 121/144:\n",
      "Mean Validation Loss: 1642595.1250 (±45072.8778)\n",
      "Time taken: 19.56 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 122/144:\n",
      "Mean Validation Loss: 1502207.6750 (±42202.7472)\n",
      "Time taken: 39.66 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 123/144:\n",
      "Mean Validation Loss: 1707997.6500 (±43816.6000)\n",
      "Time taken: 18.05 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 124/144:\n",
      "Mean Validation Loss: 1626063.5500 (±44052.5340)\n",
      "Time taken: 33.65 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 125/144:\n",
      "Mean Validation Loss: 694118.2375 (±28145.4315)\n",
      "Time taken: 29.40 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 126/144:\n",
      "Mean Validation Loss: 232118.4750 (±14104.6362)\n",
      "Time taken: 41.80 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 127/144:\n",
      "Mean Validation Loss: 1081595.7875 (±34866.7110)\n",
      "Time taken: 20.86 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 128/144:\n",
      "Mean Validation Loss: 617748.9750 (±26410.1059)\n",
      "Time taken: 37.89 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 129/144:\n",
      "Mean Validation Loss: 70430.2398 (±23663.8355)\n",
      "Time taken: 20.60 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 130/144:\n",
      "Mean Validation Loss: 53044.7445 (±37291.1397)\n",
      "Time taken: 37.51 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 131/144:\n",
      "Mean Validation Loss: 84418.6367 (±13159.5845)\n",
      "Time taken: 18.31 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 132/144:\n",
      "Mean Validation Loss: 71556.4141 (±30004.9812)\n",
      "Time taken: 34.59 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 133/144:\n",
      "Mean Validation Loss: 23913.9305 (±2040.9534)\n",
      "Time taken: 20.06 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 134/144:\n",
      "Mean Validation Loss: 28363.0988 (±7573.0467)\n",
      "Time taken: 37.46 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 135/144:\n",
      "Mean Validation Loss: 25575.0617 (±2147.2548)\n",
      "Time taken: 18.56 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 136/144:\n",
      "Mean Validation Loss: 26716.6266 (±4725.3894)\n",
      "Time taken: 34.89 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 137/144:\n",
      "Mean Validation Loss: 1497891.8000 (±43587.0179)\n",
      "Time taken: 19.94 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 138/144:\n",
      "Mean Validation Loss: 1247438.6250 (±37738.3192)\n",
      "Time taken: 37.75 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 139/144:\n",
      "Mean Validation Loss: 1621952.3250 (±42999.2779)\n",
      "Time taken: 18.78 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 140/144:\n",
      "Mean Validation Loss: 1469918.3500 (±41244.6899)\n",
      "Time taken: 34.95 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 141/144:\n",
      "Mean Validation Loss: 259024.7219 (±15371.6407)\n",
      "Time taken: 19.98 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 142/144:\n",
      "Mean Validation Loss: 114482.4516 (±5206.2248)\n",
      "Time taken: 37.67 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 143/144:\n",
      "Mean Validation Loss: 633022.0750 (±27220.6550)\n",
      "Time taken: 18.47 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 144/144:\n",
      "Mean Validation Loss: 212112.1750 (±13135.9851)\n",
      "Time taken: 34.57 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Best hyperparameters found:\n",
      "----------------------------------------------------------------\n",
      "    n_layers=4,\n",
      "\n",
      "----------------------------------------------------------------\n",
      "    n_neurons=64,\n",
      "\n",
      "----------------------------------------------------------------\n",
      "    activation=relu,\n",
      "\n",
      "----------------------------------------------------------------\n",
      "    learning_rate=0.01,\n",
      "\n",
      "----------------------------------------------------------------\n",
      "    batch_size=64,\n",
      "\n",
      "----------------------------------------------------------------\n",
      "    epochs=200,\n",
      "\n",
      "Mean Validation Loss: 23913.9305\n",
      "Epoch 1/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1779710.6250 - val_loss: 968026.6875\n",
      "Epoch 2/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 557190.5000 - val_loss: 202278.4062\n",
      "Epoch 3/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 211671.7188 - val_loss: 209852.8281\n",
      "Epoch 4/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 135662.6250 - val_loss: 115884.9609\n",
      "Epoch 5/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 110720.1641 - val_loss: 114824.4922\n",
      "Epoch 6/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 105251.5000 - val_loss: 115577.1094\n",
      "Epoch 7/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 99349.7734 - val_loss: 111332.5859\n",
      "Epoch 8/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 93638.7500 - val_loss: 110765.4141\n",
      "Epoch 9/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 96858.1719 - val_loss: 108043.6172\n",
      "Epoch 10/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 93120.3281 - val_loss: 113999.4531\n",
      "Epoch 11/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 96413.0156 - val_loss: 113356.7500\n",
      "Epoch 12/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 95373.6250 - val_loss: 110024.4062\n",
      "Epoch 13/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 91194.8047 - val_loss: 107376.8125\n",
      "Epoch 14/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 83456.4766 - val_loss: 106475.7266\n",
      "Epoch 15/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 89781.4297 - val_loss: 105998.6094\n",
      "Epoch 16/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 95712.1719 - val_loss: 109434.8984\n",
      "Epoch 17/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 89782.0859 - val_loss: 109480.7578\n",
      "Epoch 18/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87412.9531 - val_loss: 108807.1328\n",
      "Epoch 19/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 84464.6875 - val_loss: 106021.8281\n",
      "Epoch 20/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 89880.2109 - val_loss: 122053.4688\n",
      "Epoch 21/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 90383.6328 - val_loss: 118625.2500\n",
      "Epoch 22/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 89474.3516 - val_loss: 112691.6406\n",
      "Epoch 23/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 89355.7734 - val_loss: 106994.1719\n",
      "Epoch 24/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88990.7422 - val_loss: 108938.0391\n",
      "Epoch 25/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85352.1328 - val_loss: 107324.6172\n",
      "Epoch 26/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85808.8672 - val_loss: 121246.3203\n",
      "Epoch 27/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85854.6875 - val_loss: 115398.6406\n",
      "Epoch 28/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 93408.5781 - val_loss: 107980.1172\n",
      "Epoch 29/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86936.6562 - val_loss: 115300.3594\n",
      "Epoch 30/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87844.9531 - val_loss: 126086.8281\n",
      "Epoch 31/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87172.0938 - val_loss: 108036.9609\n",
      "Epoch 32/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81017.8281 - val_loss: 109144.1094\n",
      "Epoch 33/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 83448.2422 - val_loss: 114588.3828\n",
      "Epoch 34/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85446.6250 - val_loss: 107663.3047\n",
      "Epoch 35/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81359.4922 - val_loss: 110456.1562\n",
      "Epoch 36/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 83261.8516 - val_loss: 113876.9453\n",
      "Epoch 37/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81925.1172 - val_loss: 108792.4141\n",
      "Epoch 38/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80484.3828 - val_loss: 115157.7500\n",
      "Epoch 39/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 79388.2812 - val_loss: 110570.6953\n",
      "Epoch 40/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86642.2578 - val_loss: 108186.4531\n",
      "Epoch 41/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81377.6875 - val_loss: 115302.0234\n",
      "Epoch 42/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81138.0156 - val_loss: 103921.9531\n",
      "Epoch 43/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80111.3672 - val_loss: 108869.8984\n",
      "Epoch 44/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 79244.9453 - val_loss: 97328.8203\n",
      "Epoch 45/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73014.5859 - val_loss: 103429.5078\n",
      "Epoch 46/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 66009.1484 - val_loss: 88690.9766\n",
      "Epoch 47/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 64905.2344 - val_loss: 85657.1094\n",
      "Epoch 48/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 54533.3047 - val_loss: 81396.3516\n",
      "Epoch 49/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 53295.6562 - val_loss: 75924.4844\n",
      "Epoch 50/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50908.9883 - val_loss: 70082.7969\n",
      "Epoch 51/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49976.2422 - val_loss: 61330.9648\n",
      "Epoch 52/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 43289.7617 - val_loss: 61243.6133\n",
      "Epoch 53/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 36420.6914 - val_loss: 52446.8867\n",
      "Epoch 54/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37032.0547 - val_loss: 67317.2266\n",
      "Epoch 55/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38421.6055 - val_loss: 48123.4062\n",
      "Epoch 56/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27825.4082 - val_loss: 38380.9180\n",
      "Epoch 57/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32739.4648 - val_loss: 35663.9414\n",
      "Epoch 58/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23228.8203 - val_loss: 32670.5059\n",
      "Epoch 59/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22466.6543 - val_loss: 32389.3105\n",
      "Epoch 60/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 24534.0293 - val_loss: 30041.0098\n",
      "Epoch 61/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22436.6309 - val_loss: 31297.6074\n",
      "Epoch 62/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24313.9434 - val_loss: 38652.3281\n",
      "Epoch 63/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27365.0312 - val_loss: 28808.3496\n",
      "Epoch 64/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21639.2031 - val_loss: 31710.4199\n",
      "Epoch 65/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18199.1797 - val_loss: 28794.4258\n",
      "Epoch 66/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21907.8613 - val_loss: 35882.5586\n",
      "Epoch 67/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21391.8125 - val_loss: 27203.9023\n",
      "Epoch 68/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22369.2734 - val_loss: 30300.1270\n",
      "Epoch 69/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21657.9629 - val_loss: 28556.4570\n",
      "Epoch 70/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21542.4746 - val_loss: 33862.4766\n",
      "Epoch 71/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23767.3770 - val_loss: 26598.1992\n",
      "Epoch 72/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 19794.8945 - val_loss: 27257.6426\n",
      "Epoch 73/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20319.8379 - val_loss: 27422.5059\n",
      "Epoch 74/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20658.4023 - val_loss: 28494.5332\n",
      "Epoch 75/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 22355.9258 - val_loss: 30402.6875\n",
      "Epoch 76/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22599.1016 - val_loss: 26022.9980\n",
      "Epoch 77/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 16560.6348 - val_loss: 30114.9395\n",
      "Epoch 78/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17367.2402 - val_loss: 27024.8398\n",
      "Epoch 79/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17930.1289 - val_loss: 26220.8125\n",
      "Epoch 80/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18256.3379 - val_loss: 26170.1270\n",
      "Epoch 81/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18134.9375 - val_loss: 26198.3535\n",
      "Epoch 82/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17053.5723 - val_loss: 26075.4297\n",
      "Epoch 83/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15854.4609 - val_loss: 24671.6602\n",
      "Epoch 84/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15822.1963 - val_loss: 24767.0742\n",
      "Epoch 85/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 15138.5830 - val_loss: 25826.4492\n",
      "Epoch 86/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17177.0801 - val_loss: 25412.8809\n",
      "Epoch 87/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 17885.9941 - val_loss: 25968.7969\n",
      "Epoch 88/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16306.2031 - val_loss: 26545.9160\n",
      "Epoch 89/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 15782.4775 - val_loss: 28663.8242\n",
      "Epoch 90/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17504.2676 - val_loss: 38056.4883\n",
      "Epoch 91/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 20032.4648 - val_loss: 26763.4141\n",
      "Epoch 92/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17923.2910 - val_loss: 24603.3691\n",
      "Epoch 93/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15125.9512 - val_loss: 25526.3750\n",
      "Epoch 94/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 17492.4375 - val_loss: 25179.0430\n",
      "Epoch 95/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 15854.0225 - val_loss: 28355.2695\n",
      "Epoch 96/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18850.3086 - val_loss: 24954.4648\n",
      "Epoch 97/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15992.6738 - val_loss: 25725.6973\n",
      "Epoch 98/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16935.9453 - val_loss: 24466.2910\n",
      "Epoch 99/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14351.2422 - val_loss: 24860.7773\n",
      "Epoch 100/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16737.7969 - val_loss: 36179.9727\n",
      "Epoch 101/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18470.4141 - val_loss: 26984.4648\n",
      "Epoch 102/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14624.8193 - val_loss: 25861.5215\n",
      "Epoch 103/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14474.2500 - val_loss: 25754.4473\n",
      "Epoch 104/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14956.2275 - val_loss: 25404.2559\n",
      "Epoch 105/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15834.1396 - val_loss: 24940.7109\n",
      "Epoch 106/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 16413.3027 - val_loss: 25750.3105\n",
      "Epoch 107/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17902.1211 - val_loss: 28092.5840\n",
      "Epoch 108/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16359.4463 - val_loss: 25372.5566\n",
      "Epoch 109/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 13359.2988 - val_loss: 24538.3926\n",
      "Epoch 110/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14821.5723 - val_loss: 25984.5898\n",
      "Epoch 111/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 14567.1846 - val_loss: 30551.1035\n",
      "Epoch 112/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 15244.7314 - val_loss: 25322.1738\n",
      "Epoch 113/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13850.2412 - val_loss: 26887.4551\n",
      "Epoch 114/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 14389.1006 - val_loss: 26117.1094\n",
      "Epoch 115/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 15979.1729 - val_loss: 25298.5859\n",
      "Epoch 116/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13497.5781 - val_loss: 27136.1191\n",
      "Epoch 117/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14139.9521 - val_loss: 26801.4609\n",
      "Epoch 118/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 15611.6191 - val_loss: 26987.0469\n",
      "Epoch 119/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 13366.2402 - val_loss: 29070.4824\n",
      "Epoch 120/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13736.9189 - val_loss: 25448.5098\n",
      "Epoch 121/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 13525.6094 - val_loss: 26422.5254\n",
      "Epoch 122/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 12790.5156 - val_loss: 30776.3965\n",
      "Epoch 123/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14463.6680 - val_loss: 34211.0273\n",
      "Epoch 124/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 14691.0898 - val_loss: 27236.1504\n",
      "Epoch 125/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12515.7471 - val_loss: 25170.0273\n",
      "Epoch 126/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13257.8486 - val_loss: 27924.1777\n",
      "Epoch 127/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12632.4541 - val_loss: 27211.7031\n",
      "Epoch 128/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12818.1094 - val_loss: 26721.4004\n",
      "Epoch 129/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11922.7314 - val_loss: 24338.7266\n",
      "Epoch 130/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12817.3359 - val_loss: 24720.2832\n",
      "Epoch 131/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 14845.4531 - val_loss: 24244.8770\n",
      "Epoch 132/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12958.1406 - val_loss: 25482.4727\n",
      "Epoch 133/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 10957.0127 - val_loss: 26221.4727\n",
      "Epoch 134/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 11318.5459 - val_loss: 24752.4941\n",
      "Epoch 135/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12349.7480 - val_loss: 29708.5098\n",
      "Epoch 136/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 12850.5537 - val_loss: 27052.8711\n",
      "Epoch 137/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 11263.8086 - val_loss: 23248.1992\n",
      "Epoch 138/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11307.7305 - val_loss: 23018.0039\n",
      "Epoch 139/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 12052.2910 - val_loss: 27877.4277\n",
      "Epoch 140/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 14939.1201 - val_loss: 27025.0996\n",
      "Epoch 141/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12420.6475 - val_loss: 27619.8496\n",
      "Epoch 142/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 11154.6152 - val_loss: 22389.8340\n",
      "Epoch 143/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10854.8955 - val_loss: 23095.9980\n",
      "Epoch 144/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10964.2930 - val_loss: 23417.8184\n",
      "Epoch 145/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8808.1670 - val_loss: 22030.2402\n",
      "Epoch 146/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 10086.4307 - val_loss: 24729.0273\n",
      "Epoch 147/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 13350.0430 - val_loss: 21768.3672\n",
      "Epoch 148/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10941.0117 - val_loss: 22208.2109\n",
      "Epoch 149/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9316.3887 - val_loss: 21200.5957\n",
      "Epoch 150/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 10084.5947 - val_loss: 21783.9727\n",
      "Epoch 151/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8479.1611 - val_loss: 21764.0840\n",
      "Epoch 152/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9233.5215 - val_loss: 25702.7891\n",
      "Epoch 153/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 10719.8271 - val_loss: 29214.5703\n",
      "Epoch 154/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9584.9658 - val_loss: 21659.4141\n",
      "Epoch 155/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8999.4092 - val_loss: 24913.2500\n",
      "Epoch 156/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10742.0889 - val_loss: 21868.1953\n",
      "Epoch 157/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9939.6299 - val_loss: 21015.8535\n",
      "Epoch 158/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8749.6064 - val_loss: 22689.8301\n",
      "Epoch 159/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8817.5449 - val_loss: 22096.6191\n",
      "Epoch 160/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8183.7812 - val_loss: 20743.2715\n",
      "Epoch 161/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8352.0947 - val_loss: 21893.9199\n",
      "Epoch 162/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8719.0010 - val_loss: 20172.3125\n",
      "Epoch 163/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9050.3008 - val_loss: 22410.6074\n",
      "Epoch 164/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8755.5459 - val_loss: 23299.1016\n",
      "Epoch 165/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7705.1895 - val_loss: 27575.6719\n",
      "Epoch 166/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9910.9326 - val_loss: 24981.0293\n",
      "Epoch 167/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10140.0410 - val_loss: 20889.8105\n",
      "Epoch 168/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9746.5273 - val_loss: 22404.9316\n",
      "Epoch 169/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8189.2388 - val_loss: 20726.0859\n",
      "Epoch 170/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8124.5537 - val_loss: 20104.1172\n",
      "Epoch 171/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6805.6826 - val_loss: 20850.1309\n",
      "Epoch 172/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8373.8643 - val_loss: 20652.1543\n",
      "Epoch 173/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8244.3213 - val_loss: 22571.7891\n",
      "Epoch 174/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7495.9556 - val_loss: 20419.3848\n",
      "Epoch 175/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6785.1958 - val_loss: 19484.7227\n",
      "Epoch 176/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7305.6006 - val_loss: 23529.5098\n",
      "Epoch 177/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8106.6460 - val_loss: 19252.9707\n",
      "Epoch 178/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7127.0811 - val_loss: 20129.3125\n",
      "Epoch 179/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6914.9810 - val_loss: 20894.1309\n",
      "Epoch 180/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6973.0527 - val_loss: 20790.9941\n",
      "Epoch 181/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7055.7231 - val_loss: 19527.2012\n",
      "Epoch 182/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7249.3862 - val_loss: 20922.5527\n",
      "Epoch 183/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7880.9863 - val_loss: 19540.3281\n",
      "Epoch 184/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8575.8535 - val_loss: 22841.4355\n",
      "Epoch 185/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8498.8936 - val_loss: 21790.6289\n",
      "Epoch 186/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8182.0752 - val_loss: 19680.1797\n",
      "Epoch 187/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6670.5527 - val_loss: 19042.6133\n",
      "Epoch 188/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6128.3364 - val_loss: 20175.9355\n",
      "Epoch 189/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6070.7969 - val_loss: 19865.7070\n",
      "Epoch 190/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6569.2236 - val_loss: 21361.2246\n",
      "Epoch 191/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8276.2783 - val_loss: 23995.2773\n",
      "Epoch 192/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7748.3369 - val_loss: 19993.9941\n",
      "Epoch 193/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8808.9668 - val_loss: 21653.4473\n",
      "Epoch 194/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7534.8013 - val_loss: 19954.8809\n",
      "Epoch 195/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7034.5171 - val_loss: 20558.2305\n",
      "Epoch 196/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5859.9409 - val_loss: 19007.3770\n",
      "Epoch 197/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6109.7227 - val_loss: 20715.5234\n",
      "Epoch 198/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6537.3843 - val_loss: 20800.1055\n",
      "Epoch 199/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6669.4980 - val_loss: 20442.1484\n",
      "Epoch 200/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6836.9810 - val_loss: 19339.7891\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\n",
      "Final Model Performance on Test Set:\n",
      "MSE: 19339.7881\n",
      "RMSE: 139.0676\n",
      "MAE: 111.7367\n",
      "Explained Variance Score: 0.8387\n"
     ]
    }
   ],
   "source": [
    "def create_model(n_layers, n_neurons, activation, learning_rate):\n",
    "    \"\"\"\n",
    "    Creates a neural network model with specified hyperparameters\n",
    "    \n",
    "    Parameters:\n",
    "    n_layers (int): Number of hidden layers\n",
    "    n_neurons (int): Number of neurons per layer\n",
    "    activation (str): Activation function to use\n",
    "    learning_rate (float): Learning rate for Adam optimizer\n",
    "    \n",
    "    Returns:\n",
    "    model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Define the input layer explicitly\n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    # First hidden layer\n",
    "    x = Dense(n_neurons, activation=activation)(inputs)\n",
    "    \n",
    "    # Additional hidden layers\n",
    "    for _ in range(n_layers - 1):\n",
    "        x = Dense(n_neurons, activation=activation)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                 loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_layers': [2, 3, 4],\n",
    "    'n_neurons': [16, 32, 64],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'batch_size': [64, 128],\n",
    "    'epochs': [200, 400]\n",
    "}\n",
    "\n",
    "# Create all combinations of hyperparameters\n",
    "param_combinations = [dict(zip(param_grid.keys(), v)) for v in product(*param_grid.values())]\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=101)\n",
    "\n",
    "print(f\"Total combinations to test: {len(param_combinations)}\")\n",
    "print(\"Starting grid search with cross-validation...\")\n",
    "\n",
    "for i, params in enumerate(param_combinations, 1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Store validation scores for each fold\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(k_fold.split(X_train), 1):\n",
    "        # Split data for this fold\n",
    "        X_train_fold = X_train[train_idx]\n",
    "        y_train_fold = y_train.iloc[train_idx]\n",
    "        X_val_fold = X_train[val_idx]\n",
    "        y_val_fold = y_train.iloc[val_idx]\n",
    "        \n",
    "        # Create and train model\n",
    "        model = create_model(\n",
    "            n_layers=params['n_layers'],\n",
    "            n_neurons=params['n_neurons'],\n",
    "            activation=params['activation'],\n",
    "            learning_rate=params['learning_rate']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            validation_data=(X_val_fold, y_val_fold),\n",
    "            batch_size=params['batch_size'],\n",
    "            epochs=params['epochs'],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Get the best validation score for this fold\n",
    "        best_val_loss = min(history.history['val_loss'])\n",
    "        fold_scores.append(best_val_loss)\n",
    "    \n",
    "    # Calculate mean validation score across folds\n",
    "    mean_val_loss = np.mean(fold_scores)\n",
    "    std_val_loss = np.std(fold_scores)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        **params,\n",
    "        'mean_val_loss': mean_val_loss,\n",
    "        'std_val_loss': std_val_loss,\n",
    "        'time': time.time() - start_time\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nCombination {i}/{len(param_combinations)}:\")\n",
    "    print(f\"Mean Validation Loss: {mean_val_loss:.4f} (±{std_val_loss:.4f})\")\n",
    "    print(f\"Time taken: {results[-1]['time']:.2f} seconds\")\n",
    "    print(f\"-\" * 64)\n",
    "    for param, value in params.items():\n",
    "        print(f\"    {param}={value},\")\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by mean validation loss\n",
    "best_params = results_df.loc[results_df['mean_val_loss'].idxmin()]\n",
    "\n",
    "print(f\"Mean Validation Loss: {best_params['mean_val_loss']:.4f}\")\n",
    "print(f\"-\" * 64)\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(f\"-\" * 64)\n",
    "for param, value in best_params.items():\n",
    "    if param not in ['mean_val_loss', 'std_val_loss', 'time']:\n",
    "        print(f\"    {param}={value},\\n\")\n",
    "print(f\"-\" * 64)\n",
    "\n",
    "# Create and train final model with best parameters\n",
    "final_model = create_model(\n",
    "    n_layers=int(best_params['n_layers']),\n",
    "    n_neurons=int(best_params['n_neurons']),\n",
    "    activation=best_params['activation'],\n",
    "    learning_rate=best_params['learning_rate']\n",
    ")\n",
    "\n",
    "# Train the final model\n",
    "final_history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=int(best_params['batch_size']),\n",
    "    epochs=int(best_params['epochs']),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history for final model\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(final_history.history['loss'], label='Training Loss')\n",
    "plt.plot(final_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Final Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate final model on test set\n",
    "test_predictions = final_model.predict(X_test)\n",
    "final_mse = mean_squared_error(y_test, test_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_mae = mean_absolute_error(y_test, test_predictions)\n",
    "final_ev_score = explained_variance_score(y_test, test_predictions)\n",
    "\n",
    "print(\"\\nFinal Model Performance on Test Set:\")\n",
    "print(f\"MSE: {final_mse:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"Explained Variance Score: {final_ev_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features of new part type:\n",
      "coolingRate                   13.00\n",
      "quenchTime                     3.84\n",
      "forgeTime                      6.47\n",
      "HeatTreatTime                 46.87\n",
      "Nickel%                       65.73\n",
      "Iron%                         16.52\n",
      "Cobalt%                       16.82\n",
      "Chromium%                      0.93\n",
      "smallDefects                  10.00\n",
      "largeDefects                   0.00\n",
      "sliverDefects                  0.00\n",
      "partType_Blade                 0.00\n",
      "partType_Block                 0.00\n",
      "partType_Nozzle                1.00\n",
      "partType_Valve                 0.00\n",
      "microstructure_colGrain        0.00\n",
      "microstructure_equiGrain       1.00\n",
      "microstructure_singleGrain     0.00\n",
      "seedLocation_Bottom            1.00\n",
      "seedLocation_Top               0.00\n",
      "castType_Continuous            0.00\n",
      "castType_Die                   1.00\n",
      "castType_Investment            0.00\n",
      "Name: 0, dtype: float64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\n",
      "Prediction Lifespan: 1413.9783\n",
      "\n",
      "Original Lifespan: 1469.17\n"
     ]
    }
   ],
   "source": [
    "# Get features of new part type\n",
    "single_partType = df.drop('Lifespan', axis=1).iloc[0]\n",
    "print(f'Features of new part type:\\n{single_partType}')\n",
    "\n",
    "# Convert to DataFrame with feature names\n",
    "single_partType_df = pd.DataFrame([single_partType.values], columns=single_partType.index)\n",
    "\n",
    "# Scale the features while preserving feature names\n",
    "single_partType_scaled = scaler.transform(single_partType_df)\n",
    "\n",
    "# Run the model and get the lifespan prediction\n",
    "print('\\nPrediction Lifespan:', final_model.predict(single_partType_scaled)[0,0])\n",
    "\n",
    "# Print original lifespan\n",
    "print('\\nOriginal Lifespan:', df.iloc[0]['Lifespan'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP1801-ML(GPU)",
   "language": "python",
   "name": "comp1801-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
