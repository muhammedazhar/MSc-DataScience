{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import os\n",
    "    import glob\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # Importing libraries for data visualization\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Creating a model\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential, Model # type: ignore\n",
    "    from tensorflow.keras.layers import Dense, Activation, Input # type: ignore\n",
    "\n",
    "    # Importing libraries for evaluation\n",
    "    from itertools import product\n",
    "    from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "    from sklearn.model_selection import train_test_split, KFold\n",
    "    from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score\n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: ../Datasets/Dataset.csv\n",
      "Loaded dataset: ../Datasets/Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Find the CSV file in the Datasets directory\n",
    "data_path = '../Datasets/*.csv'\n",
    "file_list = glob.glob(data_path)\n",
    "\n",
    "for file in file_list:\n",
    "    print(f\"Found file: {file}\")\n",
    "\n",
    "# Ensure there is exactly one file\n",
    "if len(file_list) == 1:\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_list[0])\n",
    "    print(f\"Loaded dataset: {file_list[0]}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No CSV file found or multiple CSV files found in the Datasets directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be saved to: ../Models/\n"
     ]
    }
   ],
   "source": [
    "# File path to save the trained model\n",
    "destination = '../Models/'\n",
    "os.makedirs(destination, exist_ok=True)\n",
    "print(f\"Model will be saved to: {destination}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_unified = ['partType', 'microstructure', 'seedLocation', 'castType']\n",
    "\n",
    "# Initialize and fit the encoder\n",
    "ohe = OneHotEncoder(sparse_output=False, drop=None)\n",
    "# Reshape the data to handle multiple categorical columns\n",
    "encoded_data = ohe.fit_transform(df[categorical_cols_unified].values)\n",
    "\n",
    "# Convert to DataFrame with feature names\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_data,\n",
    "    columns=ohe.get_feature_names_out(categorical_cols_unified)\n",
    ")\n",
    "\n",
    "# Combine with non-categorical columns if needed\n",
    "df = pd.concat([df.drop(columns=categorical_cols_unified), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X = df.drop('Lifespan',axis=1)\n",
    "\n",
    "# Target\n",
    "y = df['Lifespan']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 23)\n",
      "(300, 23)\n",
      "(700,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  1.0\n",
      "Min:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Create the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transfrom\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# everything has been scaled between 1 and 0\n",
    "print('Max: ',X_train.max())\n",
    "print('Min: ', X_train.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# input layer\n",
    "model.add(Dense(19,activation='relu'))\n",
    "\n",
    "# hidden layers\n",
    "model.add(Dense(19,activation='relu'))\n",
    "model.add(Dense(19,activation='relu'))\n",
    "model.add(Dense(19,activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1827980.5000 - val_loss: 1775885.5000\n",
      "Epoch 2/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1804023.0000 - val_loss: 1775324.7500\n",
      "Epoch 3/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1808878.0000 - val_loss: 1774670.1250\n",
      "Epoch 4/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1826731.5000 - val_loss: 1773843.6250\n",
      "Epoch 5/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1820403.1250 - val_loss: 1772759.0000\n",
      "Epoch 6/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1794404.8750 - val_loss: 1771287.2500\n",
      "Epoch 7/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1800076.3750 - val_loss: 1769250.8750\n",
      "Epoch 8/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1803019.2500 - val_loss: 1766408.5000\n",
      "Epoch 9/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1772646.0000 - val_loss: 1762477.3750\n",
      "Epoch 10/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1788770.0000 - val_loss: 1757060.7500\n",
      "Epoch 11/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1828414.7500 - val_loss: 1749570.3750\n",
      "Epoch 12/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1776857.5000 - val_loss: 1739195.2500\n",
      "Epoch 13/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1771902.3750 - val_loss: 1724751.2500\n",
      "Epoch 14/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1739162.8750 - val_loss: 1704801.2500\n",
      "Epoch 15/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1735511.2500 - val_loss: 1677737.0000\n",
      "Epoch 16/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1704514.3750 - val_loss: 1641614.8750\n",
      "Epoch 17/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1652689.8750 - val_loss: 1593937.5000\n",
      "Epoch 18/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1621721.5000 - val_loss: 1532044.0000\n",
      "Epoch 19/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1534173.6250 - val_loss: 1453521.5000\n",
      "Epoch 20/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1467729.2500 - val_loss: 1355552.1250\n",
      "Epoch 21/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1349278.8750 - val_loss: 1236413.7500\n",
      "Epoch 22/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1234519.1250 - val_loss: 1095087.6250\n",
      "Epoch 23/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1104177.8750 - val_loss: 933330.1250\n",
      "Epoch 24/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 931506.1875 - val_loss: 756261.7500\n",
      "Epoch 25/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 766179.9375 - val_loss: 573581.6875\n",
      "Epoch 26/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 564973.2500 - val_loss: 401285.1875\n",
      "Epoch 27/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 376888.8125 - val_loss: 259520.6094\n",
      "Epoch 28/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 254601.6406 - val_loss: 166928.0156\n",
      "Epoch 29/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 156332.7969 - val_loss: 129468.5078\n",
      "Epoch 30/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 118596.1641 - val_loss: 129481.1484\n",
      "Epoch 31/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 119265.5547 - val_loss: 137911.9531\n",
      "Epoch 32/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 122976.2578 - val_loss: 137619.2188\n",
      "Epoch 33/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 123989.0312 - val_loss: 132262.2500\n",
      "Epoch 34/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 118245.3672 - val_loss: 127621.9219\n",
      "Epoch 35/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 115710.4219 - val_loss: 125338.3984\n",
      "Epoch 36/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 116513.1406 - val_loss: 124531.1875\n",
      "Epoch 37/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 117600.5469 - val_loss: 124087.6641\n",
      "Epoch 38/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 110212.8828 - val_loss: 123880.2812\n",
      "Epoch 39/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 116407.2891 - val_loss: 123912.8281\n",
      "Epoch 40/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 112740.3516 - val_loss: 124121.1719\n",
      "Epoch 41/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 112970.6328 - val_loss: 124023.6797\n",
      "Epoch 42/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 107933.7891 - val_loss: 123734.1562\n",
      "Epoch 43/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 110249.4141 - val_loss: 123146.9844\n",
      "Epoch 44/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 116111.9375 - val_loss: 122754.6250\n",
      "Epoch 45/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 116593.6172 - val_loss: 122260.1328\n",
      "Epoch 46/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 105388.5859 - val_loss: 121843.7734\n",
      "Epoch 47/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 113418.7891 - val_loss: 121627.8906\n",
      "Epoch 48/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 112721.1797 - val_loss: 121535.9453\n",
      "Epoch 49/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 110510.9219 - val_loss: 121390.7578\n",
      "Epoch 50/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 102954.2422 - val_loss: 121330.2969\n",
      "Epoch 51/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 109575.8672 - val_loss: 121283.8906\n",
      "Epoch 52/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 105613.0000 - val_loss: 120979.3828\n",
      "Epoch 53/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 108837.2422 - val_loss: 120646.7031\n",
      "Epoch 54/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 108174.8047 - val_loss: 120498.5859\n",
      "Epoch 55/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 108391.8438 - val_loss: 120260.2969\n",
      "Epoch 56/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 103925.1484 - val_loss: 120186.2656\n",
      "Epoch 57/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 106926.2969 - val_loss: 119719.2500\n",
      "Epoch 58/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 103150.7500 - val_loss: 119491.7031\n",
      "Epoch 59/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 105565.6641 - val_loss: 119265.7188\n",
      "Epoch 60/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 105735.0234 - val_loss: 119191.1484\n",
      "Epoch 61/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 102825.1328 - val_loss: 118578.2500\n",
      "Epoch 62/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 107249.2500 - val_loss: 118375.1172\n",
      "Epoch 63/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 103527.6953 - val_loss: 118240.4531\n",
      "Epoch 64/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 109527.5391 - val_loss: 118023.9219\n",
      "Epoch 65/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 108197.1562 - val_loss: 117969.6172\n",
      "Epoch 66/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 99516.4141 - val_loss: 117699.4531\n",
      "Epoch 67/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 106827.8750 - val_loss: 117385.0234\n",
      "Epoch 68/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 104698.4141 - val_loss: 117411.2969\n",
      "Epoch 69/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 101325.2344 - val_loss: 117214.8828\n",
      "Epoch 70/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100057.5625 - val_loss: 117036.5312\n",
      "Epoch 71/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100672.3594 - val_loss: 116651.8125\n",
      "Epoch 72/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 106428.3438 - val_loss: 116540.1094\n",
      "Epoch 73/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 97305.9375 - val_loss: 116587.7031\n",
      "Epoch 74/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 109765.1406 - val_loss: 116476.7578\n",
      "Epoch 75/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 105538.1094 - val_loss: 116432.5234\n",
      "Epoch 76/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 100903.1172 - val_loss: 116261.2031\n",
      "Epoch 77/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102431.3984 - val_loss: 115962.1875\n",
      "Epoch 78/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 104302.3516 - val_loss: 115865.2266\n",
      "Epoch 79/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 101286.5938 - val_loss: 115508.6953\n",
      "Epoch 80/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 104004.9688 - val_loss: 115628.6641\n",
      "Epoch 81/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102337.0781 - val_loss: 115462.0781\n",
      "Epoch 82/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 102149.6172 - val_loss: 115723.1094\n",
      "Epoch 83/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 96174.3828 - val_loss: 115457.1484\n",
      "Epoch 84/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 104785.8125 - val_loss: 114974.1719\n",
      "Epoch 85/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100132.2422 - val_loss: 114941.3750\n",
      "Epoch 86/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 102408.9844 - val_loss: 114478.2656\n",
      "Epoch 87/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 101807.1875 - val_loss: 114632.6562\n",
      "Epoch 88/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 99777.1875 - val_loss: 114669.4375\n",
      "Epoch 89/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 98113.6562 - val_loss: 114361.2422\n",
      "Epoch 90/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100190.6406 - val_loss: 113980.6172\n",
      "Epoch 91/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94296.4453 - val_loss: 113659.2969\n",
      "Epoch 92/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 99607.5859 - val_loss: 114091.7969\n",
      "Epoch 93/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 101086.1094 - val_loss: 114430.2500\n",
      "Epoch 94/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100183.9219 - val_loss: 113856.7344\n",
      "Epoch 95/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 100947.9688 - val_loss: 113557.9453\n",
      "Epoch 96/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 101210.3438 - val_loss: 113358.9297\n",
      "Epoch 97/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 97107.7188 - val_loss: 112987.3203\n",
      "Epoch 98/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94814.4062 - val_loss: 113160.2109\n",
      "Epoch 99/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 93416.3125 - val_loss: 113428.1094\n",
      "Epoch 100/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91330.5703 - val_loss: 113029.2422\n",
      "Epoch 101/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94837.3438 - val_loss: 112886.9609\n",
      "Epoch 102/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 97234.8984 - val_loss: 113037.3359\n",
      "Epoch 103/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 101201.5156 - val_loss: 113247.9609\n",
      "Epoch 104/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 93403.7266 - val_loss: 112815.2812\n",
      "Epoch 105/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 101515.5781 - val_loss: 112040.6406\n",
      "Epoch 106/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 98219.2969 - val_loss: 112189.3438\n",
      "Epoch 107/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 97430.1484 - val_loss: 112447.7969\n",
      "Epoch 108/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 95048.8438 - val_loss: 112531.1484\n",
      "Epoch 109/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 97593.6875 - val_loss: 112855.0000\n",
      "Epoch 110/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 95230.3281 - val_loss: 112358.7031\n",
      "Epoch 111/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 95805.6172 - val_loss: 111990.7344\n",
      "Epoch 112/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 94326.5469 - val_loss: 111862.6953\n",
      "Epoch 113/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 98455.9375 - val_loss: 111707.0312\n",
      "Epoch 114/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 93789.1719 - val_loss: 111829.5781\n",
      "Epoch 115/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 99092.9219 - val_loss: 112081.3594\n",
      "Epoch 116/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 93054.4453 - val_loss: 112099.7891\n",
      "Epoch 117/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 98836.7031 - val_loss: 111510.5781\n",
      "Epoch 118/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 97326.7422 - val_loss: 111258.9766\n",
      "Epoch 119/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100338.6016 - val_loss: 110979.5469\n",
      "Epoch 120/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94595.5547 - val_loss: 111277.7422\n",
      "Epoch 121/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 96293.3672 - val_loss: 112175.6797\n",
      "Epoch 122/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 96101.0312 - val_loss: 111374.0000\n",
      "Epoch 123/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94065.9062 - val_loss: 110824.8203\n",
      "Epoch 124/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94233.3125 - val_loss: 110492.1562\n",
      "Epoch 125/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 93669.6328 - val_loss: 110323.8125\n",
      "Epoch 126/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 95523.4609 - val_loss: 110542.9219\n",
      "Epoch 127/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94924.8438 - val_loss: 110766.9062\n",
      "Epoch 128/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94270.2734 - val_loss: 110136.6406\n",
      "Epoch 129/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94461.8359 - val_loss: 110656.7188\n",
      "Epoch 130/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 96724.0547 - val_loss: 110746.5078\n",
      "Epoch 131/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 97886.0156 - val_loss: 110299.3594\n",
      "Epoch 132/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 96718.5000 - val_loss: 109959.1484\n",
      "Epoch 133/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 97213.3984 - val_loss: 109902.2031\n",
      "Epoch 134/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94298.7500 - val_loss: 110099.8047\n",
      "Epoch 135/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 96461.6406 - val_loss: 110060.3672\n",
      "Epoch 136/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 95522.6250 - val_loss: 110258.2266\n",
      "Epoch 137/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 93460.4531 - val_loss: 110143.4297\n",
      "Epoch 138/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88800.1875 - val_loss: 109404.6406\n",
      "Epoch 139/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 96965.7422 - val_loss: 109437.3672\n",
      "Epoch 140/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 98104.0312 - val_loss: 109883.1719\n",
      "Epoch 141/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 91015.1719 - val_loss: 109807.8047\n",
      "Epoch 142/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 92397.1406 - val_loss: 109880.3047\n",
      "Epoch 143/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 92999.6719 - val_loss: 109360.6797\n",
      "Epoch 144/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 99576.0859 - val_loss: 109081.4766\n",
      "Epoch 145/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 92387.8203 - val_loss: 109050.2500\n",
      "Epoch 146/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 93057.5859 - val_loss: 108907.1172\n",
      "Epoch 147/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 94723.2500 - val_loss: 109313.5703\n",
      "Epoch 148/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 93041.1797 - val_loss: 109086.7969\n",
      "Epoch 149/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 92179.3125 - val_loss: 109158.7344\n",
      "Epoch 150/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90152.5625 - val_loss: 109073.4141\n",
      "Epoch 151/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 92581.1875 - val_loss: 108752.0703\n",
      "Epoch 152/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91125.2734 - val_loss: 108831.1719\n",
      "Epoch 153/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89908.3594 - val_loss: 108925.3359\n",
      "Epoch 154/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91374.7969 - val_loss: 108526.3281\n",
      "Epoch 155/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 92976.8594 - val_loss: 108446.8516\n",
      "Epoch 156/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 92635.4844 - val_loss: 108852.8672\n",
      "Epoch 157/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 91459.9766 - val_loss: 108408.7500\n",
      "Epoch 158/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89578.7188 - val_loss: 109074.1953\n",
      "Epoch 159/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91789.5312 - val_loss: 108663.8281\n",
      "Epoch 160/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89570.4062 - val_loss: 108173.4844\n",
      "Epoch 161/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89874.7188 - val_loss: 107882.9922\n",
      "Epoch 162/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87253.2344 - val_loss: 108360.7031\n",
      "Epoch 163/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87259.0156 - val_loss: 108672.5703\n",
      "Epoch 164/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88013.8203 - val_loss: 109056.4531\n",
      "Epoch 165/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91334.2188 - val_loss: 108077.4375\n",
      "Epoch 166/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87813.6875 - val_loss: 107401.2656\n",
      "Epoch 167/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 89849.3281 - val_loss: 107521.1484\n",
      "Epoch 168/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 92329.6094 - val_loss: 108459.8125\n",
      "Epoch 169/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87392.4922 - val_loss: 108769.2109\n",
      "Epoch 170/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 91060.2969 - val_loss: 108916.9219\n",
      "Epoch 171/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 92563.1094 - val_loss: 108236.3828\n",
      "Epoch 172/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 97241.5469 - val_loss: 107057.8906\n",
      "Epoch 173/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89876.2188 - val_loss: 107490.0000\n",
      "Epoch 174/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89208.0078 - val_loss: 108790.0859\n",
      "Epoch 175/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90176.5312 - val_loss: 108927.0859\n",
      "Epoch 176/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89762.4922 - val_loss: 107715.9297\n",
      "Epoch 177/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89526.5234 - val_loss: 107103.3984\n",
      "Epoch 178/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90768.0391 - val_loss: 107324.3438\n",
      "Epoch 179/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88899.2188 - val_loss: 108126.9922\n",
      "Epoch 180/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89638.6484 - val_loss: 108443.5703\n",
      "Epoch 181/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88095.3516 - val_loss: 107608.3516\n",
      "Epoch 182/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91425.0078 - val_loss: 107459.7734\n",
      "Epoch 183/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90002.5078 - val_loss: 107685.9219\n",
      "Epoch 184/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91742.1719 - val_loss: 107934.4766\n",
      "Epoch 185/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89577.0312 - val_loss: 107804.2578\n",
      "Epoch 186/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 92909.3281 - val_loss: 107274.7188\n",
      "Epoch 187/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89278.2031 - val_loss: 107120.7500\n",
      "Epoch 188/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89685.9922 - val_loss: 107013.4297\n",
      "Epoch 189/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89549.2969 - val_loss: 107353.6562\n",
      "Epoch 190/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88279.0312 - val_loss: 109019.8750\n",
      "Epoch 191/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91653.3828 - val_loss: 107893.4141\n",
      "Epoch 192/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89803.2734 - val_loss: 106501.9062\n",
      "Epoch 193/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91166.5859 - val_loss: 106431.2656\n",
      "Epoch 194/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 93650.0078 - val_loss: 107677.1328\n",
      "Epoch 195/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 89321.1484 - val_loss: 106793.5781\n",
      "Epoch 196/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 85998.4531 - val_loss: 107370.8672\n",
      "Epoch 197/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90631.8750 - val_loss: 108013.5703\n",
      "Epoch 198/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87171.9844 - val_loss: 108218.7500\n",
      "Epoch 199/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 89703.6641 - val_loss: 107001.2969\n",
      "Epoch 200/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85867.7578 - val_loss: 106431.2578\n",
      "Epoch 201/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90813.7891 - val_loss: 107345.4688\n",
      "Epoch 202/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 90554.4531 - val_loss: 107492.3516\n",
      "Epoch 203/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89149.5234 - val_loss: 107388.1484\n",
      "Epoch 204/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86050.2734 - val_loss: 106682.4062\n",
      "Epoch 205/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90324.6562 - val_loss: 107191.2422\n",
      "Epoch 206/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90455.1016 - val_loss: 107675.6172\n",
      "Epoch 207/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90429.4062 - val_loss: 107124.7969\n",
      "Epoch 208/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84843.4219 - val_loss: 106565.3672\n",
      "Epoch 209/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86497.9922 - val_loss: 107191.0859\n",
      "Epoch 210/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89013.4531 - val_loss: 107669.2109\n",
      "Epoch 211/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 91849.0234 - val_loss: 106734.2812\n",
      "Epoch 212/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89746.3047 - val_loss: 106640.3438\n",
      "Epoch 213/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90636.8828 - val_loss: 107247.1172\n",
      "Epoch 214/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91610.6016 - val_loss: 107413.3906\n",
      "Epoch 215/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88519.0469 - val_loss: 107293.9766\n",
      "Epoch 216/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87579.8438 - val_loss: 106666.6562\n",
      "Epoch 217/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88097.9062 - val_loss: 106243.1016\n",
      "Epoch 218/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87359.7500 - val_loss: 107061.2422\n",
      "Epoch 219/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90825.0391 - val_loss: 107277.9766\n",
      "Epoch 220/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88402.6953 - val_loss: 107127.1953\n",
      "Epoch 221/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86330.6328 - val_loss: 107304.1562\n",
      "Epoch 222/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90587.1562 - val_loss: 107107.8516\n",
      "Epoch 223/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 92528.6562 - val_loss: 107148.4766\n",
      "Epoch 224/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 86683.3359 - val_loss: 106500.8203\n",
      "Epoch 225/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 88649.9375 - val_loss: 106590.8906\n",
      "Epoch 226/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86286.6875 - val_loss: 107231.5156\n",
      "Epoch 227/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86866.5312 - val_loss: 107796.7031\n",
      "Epoch 228/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88673.6641 - val_loss: 107455.5859\n",
      "Epoch 229/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89047.4297 - val_loss: 106721.0156\n",
      "Epoch 230/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88482.5703 - val_loss: 106239.0078\n",
      "Epoch 231/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91913.7891 - val_loss: 106853.3047\n",
      "Epoch 232/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86421.2969 - val_loss: 107346.6797\n",
      "Epoch 233/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 95407.7656 - val_loss: 107864.0703\n",
      "Epoch 234/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90867.6719 - val_loss: 106813.1094\n",
      "Epoch 235/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89100.3594 - val_loss: 106273.6250\n",
      "Epoch 236/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89706.3125 - val_loss: 106698.9766\n",
      "Epoch 237/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88644.7734 - val_loss: 107497.1641\n",
      "Epoch 238/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 82881.3047 - val_loss: 107388.9141\n",
      "Epoch 239/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84961.7656 - val_loss: 106778.4297\n",
      "Epoch 240/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88443.0938 - val_loss: 106794.1406\n",
      "Epoch 241/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88798.2031 - val_loss: 106434.7891\n",
      "Epoch 242/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 92148.0938 - val_loss: 106548.6641\n",
      "Epoch 243/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88098.0469 - val_loss: 106564.6797\n",
      "Epoch 244/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89930.0547 - val_loss: 106534.8438\n",
      "Epoch 245/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 93297.7344 - val_loss: 107065.8438\n",
      "Epoch 246/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88515.9688 - val_loss: 106729.5703\n",
      "Epoch 247/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88333.5859 - val_loss: 106993.0938\n",
      "Epoch 248/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 89485.9141 - val_loss: 107419.5781\n",
      "Epoch 249/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89378.8125 - val_loss: 106595.1016\n",
      "Epoch 250/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87345.7031 - val_loss: 106370.2266\n",
      "Epoch 251/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88168.2344 - val_loss: 106816.6953\n",
      "Epoch 252/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 83938.1094 - val_loss: 107522.0703\n",
      "Epoch 253/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 92465.1016 - val_loss: 107336.6250\n",
      "Epoch 254/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87907.1328 - val_loss: 107443.9453\n",
      "Epoch 255/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87158.1875 - val_loss: 106783.3828\n",
      "Epoch 256/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91653.1953 - val_loss: 105831.3438\n",
      "Epoch 257/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87331.2734 - val_loss: 106684.1641\n",
      "Epoch 258/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89065.3125 - val_loss: 107261.5859\n",
      "Epoch 259/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90870.3516 - val_loss: 107162.5469\n",
      "Epoch 260/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89236.0703 - val_loss: 106326.7969\n",
      "Epoch 261/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 86416.9922 - val_loss: 106461.4219\n",
      "Epoch 262/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84368.7891 - val_loss: 107322.3750\n",
      "Epoch 263/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86877.3516 - val_loss: 107005.1016\n",
      "Epoch 264/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89825.7422 - val_loss: 108581.5156\n",
      "Epoch 265/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 83998.1328 - val_loss: 108070.6484\n",
      "Epoch 266/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88421.6406 - val_loss: 106827.7266\n",
      "Epoch 267/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88026.6797 - val_loss: 106157.3516\n",
      "Epoch 268/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86098.4375 - val_loss: 106232.9375\n",
      "Epoch 269/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84633.9141 - val_loss: 106591.7500\n",
      "Epoch 270/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87841.1562 - val_loss: 107473.4297\n",
      "Epoch 271/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 88565.7969 - val_loss: 107125.3203\n",
      "Epoch 272/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 92241.2266 - val_loss: 106460.6016\n",
      "Epoch 273/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90022.1328 - val_loss: 106350.4531\n",
      "Epoch 274/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86699.2109 - val_loss: 107141.2422\n",
      "Epoch 275/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86013.5625 - val_loss: 107411.2656\n",
      "Epoch 276/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 85854.4141 - val_loss: 107895.6797\n",
      "Epoch 277/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89670.4297 - val_loss: 107312.2188\n",
      "Epoch 278/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86162.5703 - val_loss: 106751.1328\n",
      "Epoch 279/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89524.8203 - val_loss: 106330.3828\n",
      "Epoch 280/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88778.0156 - val_loss: 107594.6641\n",
      "Epoch 281/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87490.2422 - val_loss: 106708.5391\n",
      "Epoch 282/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86796.8047 - val_loss: 106739.1719\n",
      "Epoch 283/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89472.3438 - val_loss: 108341.7812\n",
      "Epoch 284/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88749.9844 - val_loss: 106689.1484\n",
      "Epoch 285/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87252.2656 - val_loss: 105959.3906\n",
      "Epoch 286/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90624.1172 - val_loss: 106883.0938\n",
      "Epoch 287/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87766.0859 - val_loss: 108347.5859\n",
      "Epoch 288/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87813.2969 - val_loss: 107943.2969\n",
      "Epoch 289/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86673.3359 - val_loss: 106171.9609\n",
      "Epoch 290/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86303.7109 - val_loss: 106061.5000\n",
      "Epoch 291/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87603.0547 - val_loss: 106629.6641\n",
      "Epoch 292/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89834.5391 - val_loss: 108265.9531\n",
      "Epoch 293/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85286.4766 - val_loss: 108593.2109\n",
      "Epoch 294/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86345.3438 - val_loss: 107482.7266\n",
      "Epoch 295/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87067.0859 - val_loss: 106145.3594\n",
      "Epoch 296/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 89023.1250 - val_loss: 106441.1016\n",
      "Epoch 297/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 91022.7109 - val_loss: 107092.4297\n",
      "Epoch 298/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 89559.3828 - val_loss: 108017.3750\n",
      "Epoch 299/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 92979.2812 - val_loss: 107209.5234\n",
      "Epoch 300/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87776.5547 - val_loss: 107135.5703\n",
      "Epoch 301/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85146.1562 - val_loss: 106728.7422\n",
      "Epoch 302/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86776.7734 - val_loss: 106935.9531\n",
      "Epoch 303/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86205.5078 - val_loss: 108208.1250\n",
      "Epoch 304/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90467.9531 - val_loss: 107405.2656\n",
      "Epoch 305/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 85309.0781 - val_loss: 107273.2500\n",
      "Epoch 306/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87373.7656 - val_loss: 106380.1484\n",
      "Epoch 307/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82259.0547 - val_loss: 107134.2109\n",
      "Epoch 308/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87589.1094 - val_loss: 107903.5391\n",
      "Epoch 309/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 86120.5156 - val_loss: 107204.8359\n",
      "Epoch 310/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90298.6484 - val_loss: 106606.2812\n",
      "Epoch 311/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89393.6875 - val_loss: 107142.8281\n",
      "Epoch 312/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 83090.9453 - val_loss: 107134.1719\n",
      "Epoch 313/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86356.6250 - val_loss: 107061.1328\n",
      "Epoch 314/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86115.3828 - val_loss: 107100.4375\n",
      "Epoch 315/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86308.5391 - val_loss: 106823.1406\n",
      "Epoch 316/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 88932.1641 - val_loss: 107857.9219\n",
      "Epoch 317/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 92231.8672 - val_loss: 107218.2969\n",
      "Epoch 318/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 86517.2188 - val_loss: 106923.7500\n",
      "Epoch 319/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 88047.2891 - val_loss: 106847.8594\n",
      "Epoch 320/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87390.3750 - val_loss: 107951.5312\n",
      "Epoch 321/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88022.0469 - val_loss: 107289.1797\n",
      "Epoch 322/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87319.8281 - val_loss: 106587.7500\n",
      "Epoch 323/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 88945.9453 - val_loss: 106808.1641\n",
      "Epoch 324/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 83667.3359 - val_loss: 107142.8359\n",
      "Epoch 325/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87672.1719 - val_loss: 107389.5625\n",
      "Epoch 326/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 86392.1641 - val_loss: 106323.4453\n",
      "Epoch 327/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86406.8984 - val_loss: 105882.1953\n",
      "Epoch 328/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90601.6094 - val_loss: 107012.3906\n",
      "Epoch 329/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90429.3672 - val_loss: 106760.7891\n",
      "Epoch 330/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 87573.0859 - val_loss: 108493.6172\n",
      "Epoch 331/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84933.3438 - val_loss: 107900.7969\n",
      "Epoch 332/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86682.4766 - val_loss: 107167.2969\n",
      "Epoch 333/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 83932.3516 - val_loss: 106252.2188\n",
      "Epoch 334/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86654.2109 - val_loss: 107194.5938\n",
      "Epoch 335/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87828.6094 - val_loss: 106753.3516\n",
      "Epoch 336/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91354.8359 - val_loss: 107644.1094\n",
      "Epoch 337/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84037.2266 - val_loss: 107844.5312\n",
      "Epoch 338/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82989.0625 - val_loss: 106716.6953\n",
      "Epoch 339/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90536.9297 - val_loss: 107324.3281\n",
      "Epoch 340/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 87574.5625 - val_loss: 107003.6953\n",
      "Epoch 341/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 87812.2422 - val_loss: 106187.3203\n",
      "Epoch 342/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 83014.1484 - val_loss: 105854.1172\n",
      "Epoch 343/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 86420.0312 - val_loss: 107007.5469\n",
      "Epoch 344/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 84106.4141 - val_loss: 107429.0000\n",
      "Epoch 345/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 88066.8672 - val_loss: 106310.4688\n",
      "Epoch 346/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 84172.3750 - val_loss: 105900.0156\n",
      "Epoch 347/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 84190.2656 - val_loss: 105898.9375\n",
      "Epoch 348/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 85332.7734 - val_loss: 105621.3906\n",
      "Epoch 349/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 84563.9219 - val_loss: 104606.2656\n",
      "Epoch 350/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 84689.3906 - val_loss: 106095.5078\n",
      "Epoch 351/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 84923.9766 - val_loss: 106036.2188\n",
      "Epoch 352/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86069.3359 - val_loss: 104982.4766\n",
      "Epoch 353/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 85259.9766 - val_loss: 104616.6094\n",
      "Epoch 354/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 83991.7188 - val_loss: 104962.5547\n",
      "Epoch 355/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 82932.0000 - val_loss: 104622.2734\n",
      "Epoch 356/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 82604.0078 - val_loss: 103676.1484\n",
      "Epoch 357/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84003.4219 - val_loss: 103581.5625\n",
      "Epoch 358/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 84710.0547 - val_loss: 102723.2031\n",
      "Epoch 359/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 83155.9844 - val_loss: 101563.6562\n",
      "Epoch 360/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 79893.3750 - val_loss: 102382.8672\n",
      "Epoch 361/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 83314.0625 - val_loss: 102399.2422\n",
      "Epoch 362/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 80646.4141 - val_loss: 100974.0234\n",
      "Epoch 363/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80982.3594 - val_loss: 99879.8984\n",
      "Epoch 364/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82365.3516 - val_loss: 100249.0703\n",
      "Epoch 365/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 76186.4844 - val_loss: 100691.4922\n",
      "Epoch 366/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82120.2500 - val_loss: 99751.2656\n",
      "Epoch 367/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 79958.2969 - val_loss: 98846.4297\n",
      "Epoch 368/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 75868.8047 - val_loss: 98692.5547\n",
      "Epoch 369/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 79485.4141 - val_loss: 98145.1250\n",
      "Epoch 370/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 77892.4688 - val_loss: 98495.2969\n",
      "Epoch 371/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 79088.5234 - val_loss: 97337.4531\n",
      "Epoch 372/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 78067.7266 - val_loss: 96613.4141\n",
      "Epoch 373/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 77058.9219 - val_loss: 96150.2656\n",
      "Epoch 374/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74488.9609 - val_loss: 96711.6172\n",
      "Epoch 375/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 77534.3984 - val_loss: 96271.1562\n",
      "Epoch 376/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 71855.0469 - val_loss: 95272.9844\n",
      "Epoch 377/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71745.8203 - val_loss: 95170.2500\n",
      "Epoch 378/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73145.2031 - val_loss: 95571.3203\n",
      "Epoch 379/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 76264.7734 - val_loss: 95402.1016\n",
      "Epoch 380/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 72756.1641 - val_loss: 94153.6016\n",
      "Epoch 381/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 75480.2344 - val_loss: 93493.2344\n",
      "Epoch 382/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 74457.8203 - val_loss: 94101.1875\n",
      "Epoch 383/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 73174.1719 - val_loss: 94645.7578\n",
      "Epoch 384/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69686.4141 - val_loss: 93049.0625\n",
      "Epoch 385/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71458.5469 - val_loss: 92297.6953\n",
      "Epoch 386/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73064.4141 - val_loss: 92368.7734\n",
      "Epoch 387/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73054.2578 - val_loss: 91295.1172\n",
      "Epoch 388/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73684.1797 - val_loss: 91704.0078\n",
      "Epoch 389/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71666.4375 - val_loss: 90934.1250\n",
      "Epoch 390/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70500.7188 - val_loss: 91186.1875\n",
      "Epoch 391/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71322.3203 - val_loss: 91041.2266\n",
      "Epoch 392/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70298.3047 - val_loss: 90089.2188\n",
      "Epoch 393/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 66768.2344 - val_loss: 90002.8750\n",
      "Epoch 394/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 70491.5859 - val_loss: 89142.5391\n",
      "Epoch 395/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 68927.8125 - val_loss: 90368.0312\n",
      "Epoch 396/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69918.2344 - val_loss: 88481.0234\n",
      "Epoch 397/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 67177.7578 - val_loss: 88242.5469\n",
      "Epoch 398/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70638.0234 - val_loss: 88374.1484\n",
      "Epoch 399/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67267.5391 - val_loss: 89288.1016\n",
      "Epoch 400/400\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69289.0547 - val_loss: 87536.7891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15cc83040>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,y=y_train.values,\n",
    "          validation_data=(X_test,y_test.values),\n",
    "          batch_size=128,epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MAE:  250.4800823079427\n",
      "MSE:  87536.7909553834\n",
      "RMSE:  295.8661706842866\n",
      "Variance Regression Score:  0.28748769939607555\n",
      "\n",
      "\n",
      "Descriptive Statistics:\n",
      " count    1000.000000\n",
      "mean     1298.556320\n",
      "std       340.071434\n",
      "min       417.990000\n",
      "25%      1047.257500\n",
      "50%      1266.040000\n",
      "75%      1563.050000\n",
      "max      2134.530000\n",
      "Name: Lifespan, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print('MAE: ',mean_absolute_error(y_test,predictions))\n",
    "print('MSE: ',mean_squared_error(y_test,predictions))\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(y_test,predictions)))\n",
    "print('Variance Regression Score: ',explained_variance_score(y_test,predictions))\n",
    "\n",
    "print('\\n\\nDescriptive Statistics:\\n',df['Lifespan'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features of new part type:\n",
      "coolingRate                   13.00\n",
      "quenchTime                     3.84\n",
      "forgeTime                      6.47\n",
      "HeatTreatTime                 46.87\n",
      "Nickel%                       65.73\n",
      "Iron%                         16.52\n",
      "Cobalt%                       16.82\n",
      "Chromium%                      0.93\n",
      "smallDefects                  10.00\n",
      "largeDefects                   0.00\n",
      "sliverDefects                  0.00\n",
      "partType_Blade                 0.00\n",
      "partType_Block                 0.00\n",
      "partType_Nozzle                1.00\n",
      "partType_Valve                 0.00\n",
      "microstructure_colGrain        0.00\n",
      "microstructure_equiGrain       1.00\n",
      "microstructure_singleGrain     0.00\n",
      "seedLocation_Bottom            1.00\n",
      "seedLocation_Top               0.00\n",
      "castType_Continuous            0.00\n",
      "castType_Die                   1.00\n",
      "castType_Investment            0.00\n",
      "Name: 0, dtype: float64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\n",
      "Prediction Lifespan: 1419.282\n",
      "\n",
      "Original Lifespan: 1469.17\n"
     ]
    }
   ],
   "source": [
    "# Get features of new part type\n",
    "single_partType = df.drop('Lifespan', axis=1).iloc[0]\n",
    "print(f'Features of new part type:\\n{single_partType}')\n",
    "\n",
    "# Convert to DataFrame with feature names\n",
    "single_partType_df = pd.DataFrame([single_partType.values], columns=single_partType.index)\n",
    "\n",
    "# Scale the features while preserving feature names\n",
    "single_partType_scaled = scaler.transform(single_partType_df)\n",
    "\n",
    "# Run the model and get the lifespan prediction\n",
    "print('\\nPrediction Lifespan:', model.predict(single_partType_scaled)[0,0])\n",
    "\n",
    "# Print original lifespan\n",
    "print('\\nOriginal Lifespan:', df.iloc[0]['Lifespan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations to test: 144\n",
      "Starting grid search with cross-validation...\n",
      "\n",
      "Combination 1/144:\n",
      "Mean Validation Loss: 107287.8781 (±5095.2454)\n",
      "Time taken: 29.23 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 2/144:\n",
      "Mean Validation Loss: 98847.4219 (±6937.8539)\n",
      "Time taken: 55.04 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 3/144:\n",
      "Mean Validation Loss: 114260.1750 (±7853.9936)\n",
      "Time taken: 25.49 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 4/144:\n",
      "Mean Validation Loss: 105093.8000 (±5601.5674)\n",
      "Time taken: 47.77 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 5/144:\n",
      "Mean Validation Loss: 80701.9598 (±27840.6645)\n",
      "Time taken: 28.67 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 6/144:\n",
      "Mean Validation Loss: 78750.4402 (±29701.1083)\n",
      "Time taken: 59.97 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 7/144:\n",
      "Mean Validation Loss: 92953.9688 (±7562.0479)\n",
      "Time taken: 27.58 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 8/144:\n",
      "Mean Validation Loss: 92226.5266 (±7799.3143)\n",
      "Time taken: 49.28 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 9/144:\n",
      "Mean Validation Loss: 1717852.1000 (±45313.0715)\n",
      "Time taken: 26.59 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 10/144:\n",
      "Mean Validation Loss: 1640401.1500 (±45327.8205)\n",
      "Time taken: 54.34 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 11/144:\n",
      "Mean Validation Loss: 1752756.4250 (±44656.9952)\n",
      "Time taken: 25.49 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 12/144:\n",
      "Mean Validation Loss: 1709247.1750 (±43643.7103)\n",
      "Time taken: 46.99 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 13/144:\n",
      "Mean Validation Loss: 1125162.2000 (±34963.7272)\n",
      "Time taken: 29.14 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 14/144:\n",
      "Mean Validation Loss: 659611.7875 (±27284.7038)\n",
      "Time taken: 60.02 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 15/144:\n",
      "Mean Validation Loss: 1395309.1000 (±40019.1267)\n",
      "Time taken: 28.16 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 16/144:\n",
      "Mean Validation Loss: 1064906.9375 (±34932.9673)\n",
      "Time taken: 50.04 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 17/144:\n",
      "Mean Validation Loss: 101334.4000 (±7844.5698)\n",
      "Time taken: 36.05 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 18/144:\n",
      "Mean Validation Loss: 91605.6812 (±6700.9179)\n",
      "Time taken: 53.71 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 19/144:\n",
      "Mean Validation Loss: 107890.6875 (±8202.1221)\n",
      "Time taken: 26.96 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 20/144:\n",
      "Mean Validation Loss: 98120.0125 (±6062.7045)\n",
      "Time taken: 47.45 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 21/144:\n",
      "Mean Validation Loss: 92223.2703 (±7485.2660)\n",
      "Time taken: 25.97 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 22/144:\n",
      "Mean Validation Loss: 92496.7375 (±7015.2636)\n",
      "Time taken: 52.29 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 23/144:\n",
      "Mean Validation Loss: 92503.9828 (±7790.4903)\n",
      "Time taken: 23.99 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 24/144:\n",
      "Mean Validation Loss: 92331.3594 (±8024.3316)\n",
      "Time taken: 64.74 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 25/144:\n",
      "Mean Validation Loss: 1639868.3500 (±43961.7383)\n",
      "Time taken: 35.74 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 26/144:\n",
      "Mean Validation Loss: 1499718.1000 (±41637.2684)\n",
      "Time taken: 49.79 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 27/144:\n",
      "Mean Validation Loss: 1707135.7750 (±43646.6211)\n",
      "Time taken: 25.76 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 28/144:\n",
      "Mean Validation Loss: 1623512.4750 (±44031.9459)\n",
      "Time taken: 48.67 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 29/144:\n",
      "Mean Validation Loss: 693207.2875 (±28054.9071)\n",
      "Time taken: 27.75 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 30/144:\n",
      "Mean Validation Loss: 232065.8500 (±14184.4613)\n",
      "Time taken: 55.09 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 31/144:\n",
      "Mean Validation Loss: 1081504.8250 (±34795.7170)\n",
      "Time taken: 27.18 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 32/144:\n",
      "Mean Validation Loss: 615970.5625 (±26517.7523)\n",
      "Time taken: 49.94 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 33/144:\n",
      "Mean Validation Loss: 94913.6938 (±6676.3162)\n",
      "Time taken: 25.66 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 34/144:\n",
      "Mean Validation Loss: 92849.0750 (±7625.1422)\n",
      "Time taken: 50.84 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 35/144:\n",
      "Mean Validation Loss: 103260.3250 (±5569.1759)\n",
      "Time taken: 27.96 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 36/144:\n",
      "Mean Validation Loss: 94901.0031 (±7207.3049)\n",
      "Time taken: 57.43 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 37/144:\n",
      "Mean Validation Loss: 92754.2859 (±7420.4187)\n",
      "Time taken: 28.77 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 38/144:\n",
      "Mean Validation Loss: 66184.5559 (±36451.2259)\n",
      "Time taken: 49.75 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 39/144:\n",
      "Mean Validation Loss: 92171.4656 (±7579.0230)\n",
      "Time taken: 24.76 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 40/144:\n",
      "Mean Validation Loss: 79169.8359 (±28447.3755)\n",
      "Time taken: 50.28 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 41/144:\n",
      "Mean Validation Loss: 1495708.4250 (±41835.6835)\n",
      "Time taken: 26.39 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 42/144:\n",
      "Mean Validation Loss: 1244843.8250 (±36936.7982)\n",
      "Time taken: 50.23 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 43/144:\n",
      "Mean Validation Loss: 1618497.7750 (±43384.1953)\n",
      "Time taken: 25.96 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 44/144:\n",
      "Mean Validation Loss: 1467610.7500 (±41405.8867)\n",
      "Time taken: 55.05 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 45/144:\n",
      "Mean Validation Loss: 257992.0250 (±15674.4023)\n",
      "Time taken: 27.86 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 46/144:\n",
      "Mean Validation Loss: 114470.2422 (±5215.6334)\n",
      "Time taken: 51.28 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 47/144:\n",
      "Mean Validation Loss: 630574.8625 (±26626.7448)\n",
      "Time taken: 24.37 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 48/144:\n",
      "Mean Validation Loss: 211549.0219 (±13256.3052)\n",
      "Time taken: 48.80 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=2,\n",
      "    n_neurons=64,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 49/144:\n",
      "Mean Validation Loss: 93912.8922 (±7027.2294)\n",
      "Time taken: 26.66 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 50/144:\n",
      "Mean Validation Loss: 92883.5234 (±6932.8683)\n",
      "Time taken: 48.25 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 51/144:\n",
      "Mean Validation Loss: 106392.1469 (±8276.7588)\n",
      "Time taken: 24.71 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 52/144:\n",
      "Mean Validation Loss: 95388.1094 (±7994.9666)\n",
      "Time taken: 52.88 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 53/144:\n",
      "Mean Validation Loss: 92512.6938 (±7482.2652)\n",
      "Time taken: 27.72 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 54/144:\n",
      "Mean Validation Loss: 79456.8148 (±30431.0047)\n",
      "Time taken: 55.82 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 55/144:\n",
      "Mean Validation Loss: 92389.5984 (±7115.6151)\n",
      "Time taken: 32.49 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 56/144:\n",
      "Mean Validation Loss: 92251.8813 (±7340.0621)\n",
      "Time taken: 58.16 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 57/144:\n",
      "Mean Validation Loss: 1720134.5500 (±45050.0618)\n",
      "Time taken: 26.08 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 58/144:\n",
      "Mean Validation Loss: 1640756.3250 (±43506.0518)\n",
      "Time taken: 52.50 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 59/144:\n",
      "Mean Validation Loss: 1754284.2250 (±45005.8778)\n",
      "Time taken: 26.58 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 60/144:\n",
      "Mean Validation Loss: 1709211.0000 (±46425.0281)\n",
      "Time taken: 46.32 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 61/144:\n",
      "Mean Validation Loss: 1127273.3000 (±35987.1234)\n",
      "Time taken: 31.49 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 62/144:\n",
      "Mean Validation Loss: 660082.6750 (±26621.1142)\n",
      "Time taken: 55.31 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 63/144:\n",
      "Mean Validation Loss: 1394868.3750 (±40862.5610)\n",
      "Time taken: 27.29 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 64/144:\n",
      "Mean Validation Loss: 1065326.0250 (±35346.2856)\n",
      "Time taken: 52.60 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=16,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 65/144:\n",
      "Mean Validation Loss: 93250.6734 (±7040.7613)\n",
      "Time taken: 30.04 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 66/144:\n",
      "Mean Validation Loss: 92315.1359 (±7093.9535)\n",
      "Time taken: 58.59 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 67/144:\n",
      "Mean Validation Loss: 96675.0500 (±7436.0607)\n",
      "Time taken: 28.49 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 68/144:\n",
      "Mean Validation Loss: 85808.0359 (±12812.0247)\n",
      "Time taken: 52.34 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 69/144:\n",
      "Mean Validation Loss: 93593.3953 (±7265.7961)\n",
      "Time taken: 27.03 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 70/144:\n",
      "Mean Validation Loss: 78914.8906 (±30700.2340)\n",
      "Time taken: 57.98 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 71/144:\n",
      "Mean Validation Loss: 91966.8109 (±8043.5513)\n",
      "Time taken: 25.54 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 72/144:\n",
      "Mean Validation Loss: 92631.3594 (±7947.3780)\n",
      "Time taken: 63.71 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=relu,\n",
      "    learning_rate=0.01,\n",
      "    batch_size=128,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 73/144:\n",
      "Mean Validation Loss: 1640813.3750 (±44376.9567)\n",
      "Time taken: 30.92 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=200,\n",
      "\n",
      "Combination 74/144:\n",
      "Mean Validation Loss: 1499704.2500 (±41521.6745)\n",
      "Time taken: 55.80 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=64,\n",
      "    epochs=400,\n",
      "\n",
      "Combination 75/144:\n",
      "Mean Validation Loss: 1706478.5250 (±44636.4025)\n",
      "Time taken: 27.63 seconds\n",
      "----------------------------------------------------------------\n",
      "    n_layers=3,\n",
      "    n_neurons=32,\n",
      "    activation=tanh,\n",
      "    learning_rate=0.001,\n",
      "    batch_size=128,\n",
      "    epochs=200,\n"
     ]
    }
   ],
   "source": [
    "def create_model(n_layers, n_neurons, activation, learning_rate):\n",
    "    \"\"\"\n",
    "    Creates a neural network model with specified hyperparameters\n",
    "    \n",
    "    Parameters:\n",
    "    n_layers (int): Number of hidden layers\n",
    "    n_neurons (int): Number of neurons per layer\n",
    "    activation (str): Activation function to use\n",
    "    learning_rate (float): Learning rate for Adam optimizer\n",
    "    \n",
    "    Returns:\n",
    "    model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Define the input layer explicitly\n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    # First hidden layer\n",
    "    x = Dense(n_neurons, activation=activation)(inputs)\n",
    "    \n",
    "    # Additional hidden layers\n",
    "    for _ in range(n_layers - 1):\n",
    "        x = Dense(n_neurons, activation=activation)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                 loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_layers': [2, 3, 4],\n",
    "    'n_neurons': [16, 32, 64],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'batch_size': [64, 128],\n",
    "    'epochs': [200, 400]\n",
    "}\n",
    "\n",
    "# Create all combinations of hyperparameters\n",
    "param_combinations = [dict(zip(param_grid.keys(), v)) for v in product(*param_grid.values())]\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=101)\n",
    "\n",
    "print(f\"Total combinations to test: {len(param_combinations)}\")\n",
    "print(\"Starting grid search with cross-validation...\")\n",
    "\n",
    "for i, params in enumerate(param_combinations, 1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Store validation scores for each fold\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(k_fold.split(X_train), 1):\n",
    "        # Split data for this fold\n",
    "        X_train_fold = X_train[train_idx]\n",
    "        y_train_fold = y_train.iloc[train_idx]\n",
    "        X_val_fold = X_train[val_idx]\n",
    "        y_val_fold = y_train.iloc[val_idx]\n",
    "        \n",
    "        # Create and train model\n",
    "        model = create_model(\n",
    "            n_layers=params['n_layers'],\n",
    "            n_neurons=params['n_neurons'],\n",
    "            activation=params['activation'],\n",
    "            learning_rate=params['learning_rate']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            validation_data=(X_val_fold, y_val_fold),\n",
    "            batch_size=params['batch_size'],\n",
    "            epochs=params['epochs'],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Get the best validation score for this fold\n",
    "        best_val_loss = min(history.history['val_loss'])\n",
    "        fold_scores.append(best_val_loss)\n",
    "    \n",
    "    # Calculate mean validation score across folds\n",
    "    mean_val_loss = np.mean(fold_scores)\n",
    "    std_val_loss = np.std(fold_scores)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        **params,\n",
    "        'mean_val_loss': mean_val_loss,\n",
    "        'std_val_loss': std_val_loss,\n",
    "        'time': time.time() - start_time\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nCombination {i}/{len(param_combinations)}:\")\n",
    "    print(f\"Mean Validation Loss: {mean_val_loss:.4f} (±{std_val_loss:.4f})\")\n",
    "    print(f\"Time taken: {results[-1]['time']:.2f} seconds\")\n",
    "    print(f\"-\" * 64)\n",
    "    for param, value in params.items():\n",
    "        print(f\"    {param}={value},\")\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by mean validation loss\n",
    "best_params = results_df.loc[results_df['mean_val_loss'].idxmin()]\n",
    "\n",
    "print(f\"Mean Validation Loss: {best_params['mean_val_loss']:.4f}\")\n",
    "print(f\"-\" * 64)\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(f\"-\" * 64)\n",
    "for param, value in best_params.items():\n",
    "    if param not in ['mean_val_loss', 'std_val_loss', 'time']:\n",
    "        print(f\"    {param}={value},\\n\")\n",
    "print(f\"-\" * 64)\n",
    "\n",
    "# Create and train final model with best parameters\n",
    "final_model = create_model(\n",
    "    n_layers=int(best_params['n_layers']),\n",
    "    n_neurons=int(best_params['n_neurons']),\n",
    "    activation=best_params['activation'],\n",
    "    learning_rate=best_params['learning_rate']\n",
    ")\n",
    "\n",
    "# Train the final model\n",
    "final_history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=int(best_params['batch_size']),\n",
    "    epochs=int(best_params['epochs']),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history for final model\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(final_history.history['loss'], label='Training Loss')\n",
    "plt.plot(final_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Final Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate final model on test set\n",
    "test_predictions = final_model.predict(X_test)\n",
    "final_mse = mean_squared_error(y_test, test_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_mae = mean_absolute_error(y_test, test_predictions)\n",
    "final_ev_score = explained_variance_score(y_test, test_predictions)\n",
    "\n",
    "print(\"\\nFinal Model Performance on Test Set:\")\n",
    "print(f\"MSE: {final_mse:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"Explained Variance Score: {final_ev_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features of new part type\n",
    "single_partType = df.drop('Lifespan', axis=1).iloc[0]\n",
    "print(f'Features of new part type:\\n{single_partType}')\n",
    "\n",
    "# Convert to DataFrame with feature names\n",
    "single_partType_df = pd.DataFrame([single_partType.values], columns=single_partType.index)\n",
    "\n",
    "# Scale the features while preserving feature names\n",
    "single_partType_scaled = scaler.transform(single_partType_df)\n",
    "\n",
    "# Run the model and get the lifespan prediction\n",
    "print('\\nPrediction Lifespan:', final_model.predict(single_partType_scaled)[0,0])\n",
    "\n",
    "# Print original lifespan\n",
    "print('\\nOriginal Lifespan:', df.iloc[0]['Lifespan'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP1801-ML(GPU)",
   "language": "python",
   "name": "comp1801-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
