{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import os\n",
    "    import glob\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # Importing libraries for data visualization\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Creating a model\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential, Model # type: ignore\n",
    "    from tensorflow.keras.layers import Dense, Activation, Input # type: ignore\n",
    "\n",
    "    # Importing libraries for evaluation\n",
    "    from itertools import product\n",
    "    from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "    from sklearn.model_selection import train_test_split, KFold\n",
    "    from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score\n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the CSV file in the Datasets directory\n",
    "data_path = '../Datasets/*.csv'\n",
    "file_list = glob.glob(data_path)\n",
    "\n",
    "for file in file_list:\n",
    "    print(f\"Found file: {file}\")\n",
    "\n",
    "# Ensure there is exactly one file\n",
    "if len(file_list) == 1:\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_list[0])\n",
    "    print(f\"Loaded dataset: {file_list[0]}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No CSV file found or multiple CSV files found in the Datasets directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to save the trained model\n",
    "destination = '../Models/'\n",
    "os.makedirs(destination, exist_ok=True)\n",
    "print(f\"Model will be saved to: {destination}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_unified = ['partType', 'microstructure', 'seedLocation', 'castType']\n",
    "\n",
    "# Initialize and fit the encoder\n",
    "ohe = OneHotEncoder(sparse_output=False, drop=None)\n",
    "# Reshape the data to handle multiple categorical columns\n",
    "encoded_data = ohe.fit_transform(df[categorical_cols_unified].values)\n",
    "\n",
    "# Convert to DataFrame with feature names\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_data,\n",
    "    columns=ohe.get_feature_names_out(categorical_cols_unified)\n",
    ")\n",
    "\n",
    "# Combine with non-categorical columns if needed\n",
    "df = pd.concat([df.drop(columns=categorical_cols_unified), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X = df.drop('Lifespan',axis=1)\n",
    "\n",
    "# Target\n",
    "y = df['Lifespan']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transfrom\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# everything has been scaled between 1 and 0\n",
    "print('Max: ',X_train.max())\n",
    "print('Min: ', X_train.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# input layer\n",
    "model.add(Dense(19,activation='relu'))\n",
    "\n",
    "# hidden layers\n",
    "model.add(Dense(19,activation='relu'))\n",
    "model.add(Dense(19,activation='relu'))\n",
    "model.add(Dense(19,activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train,y=y_train.values,\n",
    "          validation_data=(X_test,y_test.values),\n",
    "          batch_size=128,epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print('MAE: ',mean_absolute_error(y_test,predictions))\n",
    "print('MSE: ',mean_squared_error(y_test,predictions))\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(y_test,predictions)))\n",
    "print('Variance Regression Score: ',explained_variance_score(y_test,predictions))\n",
    "\n",
    "print('\\n\\nDescriptive Statistics:\\n',df['Lifespan'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features of new part type\n",
    "single_partType = df.drop('Lifespan', axis=1).iloc[0]\n",
    "print(f'Features of new part type:\\n{single_partType}')\n",
    "\n",
    "# Convert to DataFrame with feature names\n",
    "single_partType_df = pd.DataFrame([single_partType.values], columns=single_partType.index)\n",
    "\n",
    "# Scale the features while preserving feature names\n",
    "single_partType_scaled = scaler.transform(single_partType_df)\n",
    "\n",
    "# Run the model and get the lifespan prediction\n",
    "print('\\nPrediction Lifespan:', model.predict(single_partType_scaled)[0,0])\n",
    "\n",
    "# Print original lifespan\n",
    "print('\\nOriginal Lifespan:', df.iloc[0]['Lifespan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_layers, n_neurons, activation, learning_rate):\n",
    "    \"\"\"\n",
    "    Creates a neural network model with specified hyperparameters\n",
    "    \n",
    "    Parameters:\n",
    "    n_layers (int): Number of hidden layers\n",
    "    n_neurons (int): Number of neurons per layer\n",
    "    activation (str): Activation function to use\n",
    "    learning_rate (float): Learning rate for Adam optimizer\n",
    "    \n",
    "    Returns:\n",
    "    model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Define the input layer explicitly\n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    # First hidden layer\n",
    "    x = Dense(n_neurons, activation=activation)(inputs)\n",
    "    \n",
    "    # Additional hidden layers\n",
    "    for _ in range(n_layers - 1):\n",
    "        x = Dense(n_neurons, activation=activation)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                 loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_layers': [2, 3, 4],\n",
    "    'n_neurons': [16, 32, 64],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'batch_size': [64, 128],\n",
    "    'epochs': [200, 400]\n",
    "}\n",
    "\n",
    "# Create all combinations of hyperparameters\n",
    "param_combinations = [dict(zip(param_grid.keys(), v)) for v in product(*param_grid.values())]\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=101)\n",
    "\n",
    "print(f\"Total combinations to test: {len(param_combinations)}\")\n",
    "print(\"Starting grid search with cross-validation...\")\n",
    "\n",
    "for i, params in enumerate(param_combinations, 1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Store validation scores for each fold\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(k_fold.split(X_train), 1):\n",
    "        # Split data for this fold\n",
    "        X_train_fold = X_train[train_idx]\n",
    "        y_train_fold = y_train.iloc[train_idx]\n",
    "        X_val_fold = X_train[val_idx]\n",
    "        y_val_fold = y_train.iloc[val_idx]\n",
    "        \n",
    "        # Create and train model\n",
    "        model = create_model(\n",
    "            n_layers=params['n_layers'],\n",
    "            n_neurons=params['n_neurons'],\n",
    "            activation=params['activation'],\n",
    "            learning_rate=params['learning_rate']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            validation_data=(X_val_fold, y_val_fold),\n",
    "            batch_size=params['batch_size'],\n",
    "            epochs=params['epochs'],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Get the best validation score for this fold\n",
    "        best_val_loss = min(history.history['val_loss'])\n",
    "        fold_scores.append(best_val_loss)\n",
    "    \n",
    "    # Calculate mean validation score across folds\n",
    "    mean_val_loss = np.mean(fold_scores)\n",
    "    std_val_loss = np.std(fold_scores)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        **params,\n",
    "        'mean_val_loss': mean_val_loss,\n",
    "        'std_val_loss': std_val_loss,\n",
    "        'time': time.time() - start_time\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nCombination {i}/{len(param_combinations)}:\")\n",
    "    print(f\"Mean Validation Loss: {mean_val_loss:.4f} (Â±{std_val_loss:.4f})\")\n",
    "    print(f\"Time taken: {results[-1]['time']:.2f} seconds\")\n",
    "    print(f\"-\" * 64)\n",
    "    for param, value in params.items():\n",
    "        print(f\"    {param}={value},\")\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by mean validation loss\n",
    "best_params = results_df.loc[results_df['mean_val_loss'].idxmin()]\n",
    "\n",
    "print(f\"Mean Validation Loss: {best_params['mean_val_loss']:.4f}\")\n",
    "print(f\"-\" * 64)\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(f\"-\" * 64)\n",
    "for param, value in best_params.items():\n",
    "    if param not in ['mean_val_loss', 'std_val_loss', 'time']:\n",
    "        print(f\"    {param}={value},\\n\")\n",
    "print(f\"-\" * 64)\n",
    "\n",
    "# Create and train final model with best parameters\n",
    "final_model = create_model(\n",
    "    n_layers=int(best_params['n_layers']),\n",
    "    n_neurons=int(best_params['n_neurons']),\n",
    "    activation=best_params['activation'],\n",
    "    learning_rate=best_params['learning_rate']\n",
    ")\n",
    "\n",
    "# Train the final model\n",
    "final_history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=int(best_params['batch_size']),\n",
    "    epochs=int(best_params['epochs']),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history for final model\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(final_history.history['loss'], label='Training Loss')\n",
    "plt.plot(final_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Final Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate final model on test set\n",
    "test_predictions = final_model.predict(X_test)\n",
    "final_mse = mean_squared_error(y_test, test_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_mae = mean_absolute_error(y_test, test_predictions)\n",
    "final_ev_score = explained_variance_score(y_test, test_predictions)\n",
    "\n",
    "print(\"\\nFinal Model Performance on Test Set:\")\n",
    "print(f\"MSE: {final_mse:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"Explained Variance Score: {final_ev_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features of new part type\n",
    "single_partType = df.drop('Lifespan', axis=1).iloc[0]\n",
    "print(f'Features of new part type:\\n{single_partType}')\n",
    "\n",
    "# Convert to DataFrame with feature names\n",
    "single_partType_df = pd.DataFrame([single_partType.values], columns=single_partType.index)\n",
    "\n",
    "# Scale the features while preserving feature names\n",
    "single_partType_scaled = scaler.transform(single_partType_df)\n",
    "\n",
    "# Run the model and get the lifespan prediction\n",
    "print('\\nPrediction Lifespan:', final_model.predict(single_partType_scaled)[0,0])\n",
    "\n",
    "# Print original lifespan\n",
    "print('\\nOriginal Lifespan:', df.iloc[0]['Lifespan'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP1801-ML(GPU)",
   "language": "python",
   "name": "comp1801-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
