{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XH-43DBzVZgm"
   },
   "source": [
    "# COMP1801 Tutorial Week 5 - Classification\n",
    "*Dr Peter Soar - 2024/25 (based upon tutorials previously made by Dr Jing Wang in 22/23)*\n",
    "\n",
    "This week we are changing over to the other branch of supervised learning by looking at Classification methods. The general pipeline of finding the best classification model is essentially identical to that of regression. The key differences to be aware of for classification is that there are a different selection of models to test and the evaluation of Classification tasks use different metrics.\n",
    "\n",
    "For this tutorial, read through and try to understand the text and code examples I have provided (ask your tutor if you have any questions) and there will be a selection of exercises. Attempt these exercises on your own, but do ask your tutor for help if you get stuck.\n",
    "\n",
    "$\\newcommand{\\Vec}[1]{\\boldsymbol{#1}}$\n",
    "$\\newcommand{\\Mat}[1]{\\boldsymbol{#1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSo3PZoZeF3M"
   },
   "source": [
    "##0. Do not forget to import all the Python Libraries being used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWa0whuXWUz0"
   },
   "outputs": [],
   "source": [
    "# Import NumPy, which can deal with multi-dimensional arrays such as matrix intuitively.\n",
    "import numpy as np # A useful package for dealing with mathematical processes, we will be using it this week for vectors and matrices\n",
    "import pandas as pd # A common package for viewing tabular data\n",
    "import matplotlib.pyplot as plt # We will be using Matplotlib for our graphs\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import sklearn.datasets # We want to be able to access the sklearn datasets again\n",
    "from sklearn.metrics import  accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score # required for evaluating classification models\n",
    "from sklearn.preprocessing import StandardScaler # We will be using the inbuilt preprocessing functions sklearn provides\n",
    "from sklearn.model_selection import train_test_split # A library that can automatically perform data splitting for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlQRs09lmgZt"
   },
   "source": [
    "#1. Classification models\n",
    "I'm going to start by introducing you to a selection of different classification models that I feel you should be aware of. This is far from an exhaustive list of all classifiers out there, and do feel free to look through the [sklearn documentation](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) and explore the other models available.\n",
    "\n",
    "Which of these models you try to implement with the coursework is up to you - there may be some you feel are more appropriate to the problem so you focus on them, or you might decide to experiment with all of them. There is no definitive right or wrong model to use, but I do expect justifications to support whichever models you decide to experiment with (including any models not on this list).\n",
    "\n",
    "I'm not going to be providing a full pipeline example for this section - just a brief example demonstrating the behaviour on the entire breast cancer dataset from sklearn and evaluated with naive accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd_GavbInWl9"
   },
   "source": [
    "##1.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aviJnohrBNc"
   },
   "source": [
    "Logistic regression is in many ways the form of classification that follows on the most intuitively from the linear regression models we have been exploring so far. Despite the name, Logistic Regression is a classification model, that works by regressing on the *probability* of a data point being in a given class, which we can then use to decide what class the point belongs in.\n",
    "\n",
    "The hypothesis function of the logistic regression model is given as follows:\n",
    "$$\n",
    "h_{\\Vec{\\theta}} (\\Vec{x}^{(i)}) =\n",
    "\\begin{cases}\n",
    "0 & \\textrm{if ${\\Vec{x}^{(i)}} \\Vec{\\theta} < 0$,} \\\\\n",
    "1 & \\textrm{if ${\\Vec{x}^{(i)}} \\Vec{\\theta} > 0$.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The cost function of the logistic regression is given as follows:\n",
    "$$\n",
    "J_{C} (\\Vec{\\theta}) = L (\\Vec{\\theta}) + R_{C} (\\Vec{\\theta}),\n",
    "$$\n",
    "where the loss function $L$ is given by\n",
    "$$\n",
    "L (\\Vec{\\theta})\n",
    "=\n",
    "- \\frac{1}{m} \\sum_{i=0}^{m-1} \\ln \\varsigma (y^{(i)} \\cdot {\\Vec{x}^{(i)}}\\Vec{\\theta})\n",
    "$$\n",
    "<!-- if the class weight is not considered,\n",
    "$$\n",
    "L (\\Vec{\\theta})\n",
    ":=\n",
    "- \\sum_{i=0}^{m-1} \\frac{1}{2 m_{y^{(i)}}} \\ln \\varsigma (y^{(i)} \\cdot {\\Vec{x}^{(i)}}^{\\top} \\Vec{\\theta})\n",
    "$$\n",
    "and\n",
    "$$\n",
    "R_{C} (\\Vec{\\theta})\n",
    ":=\n",
    "\\frac{1}{2 m C} \\sum_{j=1}^{n-1} (\\theta_{j})^2.\n",
    "$$ -->\n",
    "\n",
    "Here,\n",
    "- $m$ is the number of data points.\n",
    "<!-- - $m_{y}$ is the number of data points whose target value is $y$. -->\n",
    "- $L$ is the loss function that gives the cross entropy,\n",
    "- $R_{C}$ is the regularization function that gives a $\\ell^2$ regularization term,\n",
    "- $C > 0$ is the **inverse** regularization weight. We use the inverse one just for historical reasons.\n",
    "\n",
    "  - If $C$ is **small**, the regularization is strong (corresponding to a large regularization weight), and overfitting is strongly avoided, but may cause underfitting.\n",
    "  - If $C$ is **large**, the regularization is weak (corresponding to a small regularization weight). In this case the model is close to the original model, and it may cause overfitting.\n",
    "\n",
    "The key settings/hyperparameters to consider for Logistic Regression are:\n",
    "- `C` - The inverse Regularisation Weight\n",
    "- `penalty` - the regulation type used (note: all do not work with all solvers, see documentation)\n",
    "  - `l1` - applies a L1 penalty (as in our Lasso Regression)\n",
    "  - `l2` - applies a L2 penalty (as in our Ridge Regression)\n",
    "  - `elasticnet` - a method that applies both an L1 and L2 penalty to the optimisation.\n",
    "- `solver` - the solver used to obtain the solution (Options: `lbfgs`, `liblinear`, `newton-cg`, `newton-cholesky`, `sag`, `saga`)\n",
    "- `random_state` - Allows you to recreate the same solution by generating the same quasi-random case (as the solution is found by optimisation, not direct solution).\n",
    "\n",
    "While these are the main ones I would expect you to explore at this stage, there are some more you can look into by reading the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) on sklearn.\n",
    "\n",
    "**Task**: try changing some of these settings in the case below to see if you can understand the impact they are having on the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39_nzxKaqtYG"
   },
   "outputs": [],
   "source": [
    "# Import the Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "################\n",
    "# Load Data\n",
    "################\n",
    "# Load the breast cancer dataset\n",
    "X_pd, y_pd = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "# Convert the data to `np.array`\n",
    "# Note we are using teh whole dataset - not a robust pipeline!\n",
    "X = np.array(X_pd[['mean radius', 'mean texture']]) # just select two features\n",
    "y = np.array(y_pd)\n",
    "\n",
    "################\n",
    "# Initialisation\n",
    "################\n",
    "model = LogisticRegression() # Create our prediction model object\n",
    "# See below an example of some of the key settings\n",
    "#model = LogisticRegression(penalty='l1', solver = 'liblinear', C=0.01, random_state=40)\n",
    "scaler = StandardScaler() # Here is our standardisation object\n",
    "\n",
    "##########\n",
    "# Training\n",
    "##########\n",
    "\n",
    "# It is almost always expected that you scale your data as Logistic regression is regularised\n",
    "scaler.fit(X) # Fit the standardisation object on the polynomial feature matrix\n",
    "X = scaler.transform(X) # Standardise the feature matrix to get our 'final' training data feature matrix\n",
    "\n",
    "model.fit(X, y) # fit our logistic regression model - 'train' the model\n",
    "\n",
    "############\n",
    "# Evaluation\n",
    "############\n",
    "# Evaluate performance of the training data\n",
    "y_pred = model.predict(X) # Use our fitted model to make a prediction\n",
    "\n",
    "# Calculate accuracy - note this can often be a naive classification accuracy criterion!\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot the decision boundary\n",
    "h = .02  # step size in the mesh\n",
    "# boundaries for the grid\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "# Create mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "# Use model to plot the decision boundary for all points\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "# Plot observed points and add text\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k', marker='o', s=50, linewidth=1.5)\n",
    "plt.title(f'Logistic Regression Classifier on Breast Cancer Dataset')\n",
    "plt.xlabel('mean radius')\n",
    "plt.ylabel('mean texture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjZclp_0ndD-"
   },
   "source": [
    "##1.2 Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-R_5dApv-q9"
   },
   "source": [
    "Support Vector Machines are another very popular machine learning method (which can be used for regression and classification). The mathematics behind them are quite complicated and different to what we have been looking at in our linear models - using objects called 'kernels' to draw the line that best separates the classes, rather than regressing on the probability like our Logistic Regression (as such, we don't generally get a probability estimate from the SVM).\n",
    "\n",
    "There are a few key settings you should consider for SVMs:\n",
    "- `C` - The inverse Regularisation Weight (there are no options of the type of regularisation for SVM).\n",
    "- `kernel` - the type of kernel the SVM model uses to fit the lines\n",
    "  - `linear` - Draws a straight line\n",
    "  - `poly` - Same as manually fitting polynomial features and using the linear kernel (though doing it this manual way would be computationally slower).\n",
    "    - `degree` - Choose maximum polynomial degree\n",
    "  - `rbf` - Uses Kernel trick to measure similarity between data points in infinite dimensions and classify based on this.\n",
    "  - `sigmoid` - Creates a complex sigmoid shaped irregular boundary. Only generally useful for very specific datasets as is often does not generalise well.\n",
    "- `random_state` - Allows you to recreate the same solution by generating the same quasi-random case.\n",
    "\n",
    "**Task**: try changing some of these settings/hyperparameters in the case below to see if you can understand the impact they are having on the results.\n",
    "\n",
    "There are some other settings/hyperparameters you could experiment with that you can find in the sklearn [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mde-jmHtu37e"
   },
   "outputs": [],
   "source": [
    "# Import the Support Vector Machine Classification Model\n",
    "from sklearn.svm import SVC\n",
    "################\n",
    "# Load Data\n",
    "################\n",
    "# Load the breast cancer dataset\n",
    "X_pd, y_pd = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "# Convert the data to `np.array`\n",
    "# Note we are using teh whole dataset - not a robust pipeline!\n",
    "X = np.array(X_pd[['mean radius', 'mean texture']]) # just select two features\n",
    "y = np.array(y_pd)\n",
    "\n",
    "################\n",
    "# Initialisation\n",
    "################\n",
    "model = SVC() # Create our prediction model object\n",
    "# See below an example of some of the key settings\n",
    "#model = SVC(C=1,kernel='rbf',random_state=40)\n",
    "scaler = StandardScaler() # Here is our standardisation object\n",
    "\n",
    "##########\n",
    "# Training\n",
    "##########\n",
    "\n",
    "# It is almost always expected that you scale your data as SVM is regularised\n",
    "scaler.fit(X) # Fit the standardisation object on the polynomial feature matrix\n",
    "X = scaler.transform(X) # Standardise the feature matrix to get our 'final' training data feature matrix\n",
    "\n",
    "model.fit(X, y) # fit our SVM - 'train' the model\n",
    "\n",
    "############\n",
    "# Evaluation\n",
    "############\n",
    "# Evaluate performance of the training data\n",
    "y_pred = model.predict(X) # Use our fitted model to make a prediction\n",
    "\n",
    "# Calculate accuracy - note this can often be a naive classification accuracy criterion!\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot the decision boundary\n",
    "h = .02  # step size in the mesh\n",
    "# boundaries for the grid\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "# Create mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "# Use model to plot the decision boundary for all points\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "# Plot observed points and add text\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k', marker='o', s=50, linewidth=1.5)\n",
    "plt.title(f'SVM Classifier on Breast Cancer Dataset')\n",
    "plt.xlabel('mean radius')\n",
    "plt.ylabel('mean texture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwxb3OXhnkVk"
   },
   "source": [
    "##1.3 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zCP229A2oa_"
   },
   "source": [
    "Naive Bayes is an interesting case - it is purely probabilistic and doesn't really have any hyperparameters to tune. It's non-parametric so it doesn't case about feature scaling. One of it's key assumptions is that all features are independent (statistically), which is often not the case. Regardless, Naive Bayes is often surprisingly good at getting classification right (though its prediction of the probability should not be taken too seriously).\n",
    "\n",
    "It is quite unlikely that naive Bayes will be your 'best' model in almost any case, but it is very quick and easy to fit and can act as a good baseline model to compare against. And sometimes it may be good enough for what you are trying to do!\n",
    "\n",
    "You can read the sklearn [documentation](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes) if you are interested in learning more about how Naive Bayes work (and there are other types you can experiment with).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zX_e0sjD1tMP"
   },
   "outputs": [],
   "source": [
    "# Import the Naive Bayes Classification Model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "################\n",
    "# Load Data\n",
    "################\n",
    "# Load the breast cancer dataset\n",
    "X_pd, y_pd = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "# Convert the data to `np.array`\n",
    "# Note we are using the whole dataset - not a robust pipeline!\n",
    "X = np.array(X_pd[['mean radius', 'mean texture']]) # just select two features\n",
    "y = np.array(y_pd)\n",
    "\n",
    "################\n",
    "# Initialisation\n",
    "################\n",
    "model = GaussianNB() # Create our prediction model object\n",
    "scaler = StandardScaler() # Here is our standardisation object\n",
    "\n",
    "##########\n",
    "# Training\n",
    "##########\n",
    "\n",
    "# For Naive Bayes, this is a case where feature scaling doesn't matter!\n",
    "#scaler.fit(X) # Fit the standardisation object on the polynomial feature matrix\n",
    "#X = scaler.transform(X) # Standardise the feature matrix to get our 'final' training data feature matrix\n",
    "\n",
    "model.fit(X, y) # fit our Naive Bayes - 'train' the model\n",
    "\n",
    "############\n",
    "# Evaluation\n",
    "############\n",
    "# Evaluate performance of the training data\n",
    "y_pred = model.predict(X) # Use our fitted model to make a prediction\n",
    "\n",
    "# Calculate accuracy - note this can often be a naive classification accuracy criterion!\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot the decision boundary\n",
    "h = .02  # step size in the mesh\n",
    "# boundaries for the grid\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "# Create mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "# Use model to plot the decision boundary for all points\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "# Plot observed points and add text\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k', marker='o', s=50, linewidth=1.5)\n",
    "plt.title(f'Naive Bayes Classifier on Breast Cancer Dataset')\n",
    "plt.xlabel('mean radius')\n",
    "plt.ylabel('mean texture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sIWfH5qoPEN"
   },
   "source": [
    "##1.4 k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZANTGTWt4A9u"
   },
   "source": [
    "You should already be somewhat familiar with the concept of Knn for regression - and the principles are exactly the same for classification (in fact I would say they are even more intuitive). Using the training date, the Knn classifier can check what the nearest neighbours are for each point needing to be predicted, whichever class contains the most neighbours will be the predicted class.\n",
    "\n",
    "While there are some other options you can read about in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier), the key hyperparameter of interest is the `n_neighbors`.\n",
    "\n",
    "**Task**: try experimenting with the `n_neighbors` to see how it impacts the prediction (but remember that we are not evaluating this data properly using a test dataset!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3ivzTyu3_AX"
   },
   "outputs": [],
   "source": [
    "# Import the Knn Classification Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "################\n",
    "# Load Data\n",
    "################\n",
    "# Load the breast cancer dataset\n",
    "X_pd, y_pd = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "# Convert the data to `np.array`\n",
    "# Note we are using the whole dataset - not a robust pipeline!\n",
    "X = np.array(X_pd[['mean radius', 'mean texture']]) # just select two features\n",
    "y = np.array(y_pd)\n",
    "\n",
    "################\n",
    "# Initialisation\n",
    "################\n",
    "model = KNeighborsClassifier(n_neighbors=3) # Create our prediction model object\n",
    "scaler = StandardScaler() # Here is our standardisation object\n",
    "\n",
    "##########\n",
    "# Training\n",
    "##########\n",
    "\n",
    "# You probably still want to scale your features for best behaviour.\n",
    "scaler.fit(X) # Fit the standardisation object on the polynomial feature matrix\n",
    "X = scaler.transform(X) # Standardise the feature matrix to get our 'final' training data feature matrix\n",
    "\n",
    "model.fit(X, y) # fit our Knn - 'train' the model\n",
    "\n",
    "############\n",
    "# Evaluation\n",
    "############\n",
    "# Evaluate performance of the training data\n",
    "y_pred = model.predict(X) # Use our fitted model to make a prediction\n",
    "\n",
    "# Calculate accuracy - note this can often be a naive classification accuracy criterion!\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot the decision boundary\n",
    "h = .2  # step size in the mesh\n",
    "# boundaries for the grid\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "# Create mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "# Use model to plot the decision boundary for all points\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "# Plot observed points and add text\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k', marker='o', s=50, linewidth=1.5)\n",
    "plt.title(f'Knn Classifier on Breast Cancer Dataset')\n",
    "plt.xlabel('mean radius')\n",
    "plt.ylabel('mean texture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7LwwPQZoVve"
   },
   "source": [
    "##1.5 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfLnFGIr8dzN"
   },
   "source": [
    "Decision Trees are a powerful non-parametric method that you should have encountered already for regression problems. This looks at your variables and asks a series of yes/no questions which it uses to split into a 'tree' of conditions and target predictions. Being non-parametric, the model does not care about feature scaling.\n",
    "\n",
    "Decision trees do have a lot of variables that can (and should) be experimented with to get a model that is picking up realistic behaviour in the data:\n",
    "\n",
    "*   `max_depth` which dictates how many levels the tree can have (where a 'deeper' tree can include more complexity as more yes/no questions are asked at each level).\n",
    "*   `min_samples_split` How many samples need to exist at a node to split the data with a further yes/no question\n",
    "*   `min_samples_leaf` How many samples need to exist on each branch after a split for a split to be considered.\n",
    "\n",
    "You can find more information about these variables (and more) in the sklearn [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier).\n",
    "\n",
    "**Task**: Experiment with these parameters and see what it does to your prediction. Again, remember that our pipeline here is not performing a robust evaluation and that decision trees can be quite prone to overfitting!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHWD4l_06C7q"
   },
   "outputs": [],
   "source": [
    "# Import the Decision Tree Classification Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "################\n",
    "# Load Data\n",
    "################\n",
    "# Load the breast cancer dataset\n",
    "X_pd, y_pd = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "# Convert the data to `np.array`\n",
    "# Note we are using the whole dataset - not a robust pipeline!\n",
    "X = np.array(X_pd[['mean radius', 'mean texture']]) # just select two features\n",
    "y = np.array(y_pd)\n",
    "\n",
    "################\n",
    "# Initialisation\n",
    "################\n",
    "model = DecisionTreeClassifier() # Create our prediction model object\n",
    "#model = DecisionTreeClassifier(max_depth = 3, min_samples_split = 5, min_samples_leaf = 5)\n",
    "scaler = StandardScaler() # Here is our standardisation object\n",
    "\n",
    "##########\n",
    "# Training\n",
    "##########\n",
    "\n",
    "# For Decision Trees, this is a case where feature scaling doesn't matter!\n",
    "#scaler.fit(X) # Fit the standardisation object on the polynomial feature matrix\n",
    "#X = scaler.transform(X) # Standardise the feature matrix to get our 'final' training data feature matrix\n",
    "\n",
    "model.fit(X, y) # fit our Decision Tree - 'train' the model\n",
    "\n",
    "############\n",
    "# Evaluation\n",
    "############\n",
    "# Evaluate performance of the training data\n",
    "y_pred = model.predict(X) # Use our fitted model to make a prediction\n",
    "\n",
    "# Calculate accuracy - note this can often be a naive classification accuracy criterion!\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot the decision boundary\n",
    "h = .2  # step size in the mesh\n",
    "# boundaries for the grid\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "# Create mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "# Use model to plot the decision boundary for all points\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "# Plot observed points and add text\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k', marker='o', s=50, linewidth=1.5)\n",
    "plt.title(f'Decision Tree Classifier on Breast Cancer Dataset')\n",
    "plt.xlabel('mean radius')\n",
    "plt.ylabel('mean texture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTl8emelopX0"
   },
   "source": [
    "##1.6 Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKf_eF1d9zY3"
   },
   "source": [
    "Random forests are what are known as an 'ensemble model', meaning that they are the result of fitting multiple models (in this case Decision Tree models) on subsamples of the data, with the results from all of these runs being averaged to get the best model overall and try to tackle overfitting (though random forests are still very prone to overfitting). Being non-parametric, the model does not care about feature scaling.\n",
    "\n",
    "You can essentially think of this as an advanced form of decision tree - they use all of the same parameters outlined above, with the addition of `n_estimators`, which controls how many Decision Tree Models are created and combined for the final random forest model. As the name implies, these models are made using random samples, so for replicating your tests you should also generally set a `random_state`.\n",
    "\n",
    "One downside of random forests are that they are 'black box' models that are not really interpretable, whereas arguably the biggest benefit of decision trees are that they are very easy to interpret and understand why they have made a choice to classify one way or the other.\n",
    "\n",
    "Read more about Random forests in the [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) documentation.\n",
    "\n",
    "**Task**: Briefly experiment with the hyperparameters to see if you can understand how this is impacting the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rl3TNFIV7ps4"
   },
   "outputs": [],
   "source": [
    "# Import the Random Forest Classification Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "################\n",
    "# Load Data\n",
    "################\n",
    "# Load the breast cancer dataset\n",
    "X_pd, y_pd = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "# Convert the data to `np.array`\n",
    "# Note we are using the whole dataset - not a robust pipeline!\n",
    "X = np.array(X_pd[['mean radius', 'mean texture']]) # just select two features\n",
    "y = np.array(y_pd)\n",
    "\n",
    "################\n",
    "# Initialisation\n",
    "################\n",
    "model = RandomForestClassifier() # Create our prediction model object\n",
    "#model = RandomForestClassifier(n_estimators=50, max_depth = 3, min_samples_split = 5, min_samples_leaf = 5, random_state = 0)\n",
    "scaler = StandardScaler() # Here is our standardisation object\n",
    "\n",
    "##########\n",
    "# Training\n",
    "##########\n",
    "\n",
    "# For Random Forest, this is a case where feature scaling doesn't matter!\n",
    "#scaler.fit(X) # Fit the standardisation object on the polynomial feature matrix\n",
    "#X = scaler.transform(X) # Standardise the feature matrix to get our 'final' training data feature matrix\n",
    "\n",
    "model.fit(X, y) # fit our Random Forest - 'train' the model\n",
    "\n",
    "############\n",
    "# Evaluation\n",
    "############\n",
    "# Evaluate performance of the training data\n",
    "y_pred = model.predict(X) # Use our fitted model to make a prediction\n",
    "\n",
    "# Calculate accuracy - note this can often be a naive classification accuracy criterion!\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot the decision boundary\n",
    "h = .2  # step size in the mesh\n",
    "# boundaries for the grid\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "# Create mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "# Use model to plot the decision boundary for all points\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "# Plot observed points and add text\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k', marker='o', s=50, linewidth=1.5)\n",
    "plt.title(f'Random Forest Classifier on Breast Cancer Dataset')\n",
    "plt.xlabel('mean radius')\n",
    "plt.ylabel('mean texture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnfUSsaBFUNX"
   },
   "source": [
    "# 2. Classification implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_zapgBTFZhm"
   },
   "source": [
    "In this section, I want to provide you with some more realistic examples of how we would aim to fit a classification model in practice, with a robust evaluation and model selection pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKol95srw8Z-"
   },
   "source": [
    "## 2.1 Full ML pipeline (Classification)\n",
    "\n",
    "Here is a repeat of the ML pipeline that was presented in the previous weeks lecture - as said, the general pipeline for fitting a supervised learning ML model is essentially the same whether you are using Regression or classification, so in the pipeline I am just highlighting in red the few differences you have to consider for classification models - as you can see these are quite minimal!\n",
    "\n",
    "General pipeline:\n",
    "- Data loading\n",
    "  - Ensure you perform and Exploratory Data Analysis (EDA) and clean your data before going any further when dealing with real data.\n",
    "  - Convert your data into arrays that your ML model can interface with.\n",
    "  - Split your data into the Train:Validation:Test sets.\n",
    "- Initialization\n",
    "  - Initialize the preprocessor(s) and the <font color='red'>**machine learning model objects** (You will have a different selection of models to consider during classification - see above)</font>.\n",
    "- Model Selection\n",
    "  - Identify the different solutions you want to try: this could involve different model types (e.g. Linear Regression, Lasso, Ridge, Decision tree, Neural Network, etc), preprocessing methods, feature selection/engineering and hyperparameter selection.\n",
    "  - Training (do this for **all** models and hyperparameters)\n",
    "    - **Fit the preprocessor(s)** to the raw training feature matrix.\n",
    "    - **Transform (Preprocess)** the raw training feature matrix into the pre-processed training feature matrix.\n",
    "    - **Fit (Train) the prediction model** using the data pair of the pre-processed training feature matrix and target column vector.\n",
    "    - Recording the performance of the prediction on training data is often also helpful for identifying overfitting.\n",
    "\n",
    "  - Validation (for **all** models and hyperparameters)\n",
    "    - **Transform (Preprocess)** the raw validation feature matrix into the pre-processed validation feature matrix using **the preprocessor(s)** (**Do NOT fit any preprocessor to the validation feature matrix**).\n",
    "    - **Predict** using the pre-processed validation feature matrix to get a predicted target column.\n",
    "    - <font color='red'>**Evaluate** the prediction performance on the validation data by Accuracy Metrics / Confusion Matrix</font>\n",
    "\n",
    "  - Continue training and validation until you have a model (or selection of models) you feel are potentially good enough to offer a robust solution to the provided prediction problem.\n",
    "\n",
    "- Test (for the model and hyperparameters selected by the validation process)\n",
    "  - **Transform (Preprocess)** the raw test feature matrix into the pre-processed test feature matrix by **the preprocessor(s)** (**Do NOT fit any preprocessor to the test feature matrix**).\n",
    "  - **Predict** on the pre-processed test feature matrix to get a predicted target column.\n",
    "  - <font color='red'>**Evaluate** the prediction performance on the test data by Accuracy Metrics / Confusion Matrix</font>\n",
    "\n",
    "If after the test step your model is not providing good enough solutions on the test data, then you will have to go back to rethink how you can create a more robust solution less prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfzMcEE3iJ6B"
   },
   "source": [
    "##2.2 Binary Classification Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHYNVEO8Fy02"
   },
   "source": [
    "Classification can be thought of as coming in two types - binary and multi-class. While very similar in how we approach them (and most models can happily deal with both without changing and variables). However, there are still enough different considerations that it is worth considering them separately.\n",
    "\n",
    "I'm going to start with the slightly simpler case of binary classification, for which we'll be using another `sklearn` dataset about breast cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CZnrHtliTGI"
   },
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "X_pd, y_pd = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "Xy_df = pd.concat([X_pd, y_pd], axis=1)\n",
    "\n",
    "display(Xy_df)\n",
    "\n",
    "# Convert the data to `np.array`\n",
    "# We're just going to use two features for now.\n",
    "X_raw = np.array(X_pd[['mean radius', 'mean texture']])\n",
    "# Note that in the dataset 0 = Malignant and 1 = Benign, but we're going to switch that around going forward\n",
    "# so that the 'positive' class Malignant = 1\n",
    "y = np.array(1.0-y_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvYNpoY4GksQ"
   },
   "source": [
    "We want to robustly split our data into three sets for classification as well. Note that really we always want to shuffle our classification data for tabular problems like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSXb7J5AiNW5"
   },
   "outputs": [],
   "source": [
    "# Split the data into non-test/test data\n",
    "# While we have 20640 pairs of a feature and target, we use 20% only for the testing, not so we hold back the other 80% for training and validation\n",
    "# Generally we always want to shuffle data for classification.\n",
    "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X_raw, y, test_size=0.20, shuffle=True, random_state=0)\n",
    "\n",
    "# Split the non-test data into non-test/test data\n",
    "# We use 25% of the remaining non-test data only for the validation set, leaving the rest for training\n",
    "# In the end, the splitting ratio will be 6:2:2 for the training, validation, and test data.\n",
    "# Generally we always want to shuffle data for classification.\n",
    "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=0.25, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRny4NA-G1Z1"
   },
   "source": [
    "We can see that initialisation, training and validation follow the same steps. Note that the model we are initialising here is a `LogisticRegression` model, though for now we are just using the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tphygJ7inlB"
   },
   "outputs": [],
   "source": [
    "################\n",
    "# Initialisation\n",
    "################\n",
    "model = LogisticRegression() # Create our prediction model object\n",
    "scaler = StandardScaler() # Here is our standardisation object\n",
    "\n",
    "##########\n",
    "# Training\n",
    "##########\n",
    "\n",
    "scaler.fit(X_train_raw) # Fit the standardisation object on the polynomial feature matrix\n",
    "X_train = scaler.transform(X_train_raw) # Standardise the feature matrix to get our 'final' training data feature matrix\n",
    "\n",
    "model.fit(X_train, y_train) # fit our logistic regression model - 'train' the model\n",
    "\n",
    "############\n",
    "# Validation\n",
    "############\n",
    "\n",
    "# Preprocess validation dataset using preprocessor objects for on the training data\n",
    "# Do NOT refit the preprocessors\n",
    "X_valid = scaler.transform(X_valid_raw) # Standardise the validation data\n",
    "\n",
    "# We select the best hyperparameters based on the performance of our model on the validation dataset\n",
    "y_pred_valid = model.predict(X_valid) # Use our fitted logistic regression model to make a prediction based on teh validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuWPh8zPHUHI"
   },
   "source": [
    "When we only have two features, we can visualise the prediction by plotting the decision boundary (which says what category the model predicts fo every possible point in an area), and by plotting the points from each class over this we can see where the model is failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q070LeR-TKAM"
   },
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "# Create a meshgrid to plot the decision boundary\n",
    "x_min, x_max = X_valid[:, 0].min() - 1, X_valid[:, 0].max() + 1\n",
    "y_min, y_max = X_valid[:, 1].min() - 1, X_valid[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "\n",
    "# Predict the class labels for the points in the meshgrid\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Create a scatter plot of the data points\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.contourf(xx, yy, Z, alpha=0.35, cmap = 'bwr')\n",
    "X_neg = X_valid[y_valid==0, :]\n",
    "X_pos = X_valid[y_valid==1, :]\n",
    "plt.scatter(X_neg[:, 0], X_neg[:, 1], color='blue', edgecolors='black', label='Benign')\n",
    "plt.scatter(X_pos[:, 0], X_pos[:, 1], color='red', edgecolors='black', label='Malignant')\n",
    "\n",
    "\n",
    "plt.xlabel('mean radius')\n",
    "plt.ylabel('mean texture')\n",
    "\n",
    "# Show the plot\n",
    "plt.title(f'Logistic Regression Classifier on Breast Cancer Dataset (Validation)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d2V4VYyDtCh"
   },
   "source": [
    "### Classification accuracy metrics\n",
    "One we have our the model fit on the validation data, we can explore various classification accuracy metrics to see how good our model is. While there is no harm in checking all of these, there is generally one metric in particular you care the most about maximising, which is generally based both on your problem (for example we generally care more about recall if a false negative is really bad - e.g. missing if someone has cancer) or the distribution of your data (if your data is well balanced then the accuracy may be a good metric).\n",
    "\n",
    "Here are some of the common classification metrics:\n",
    "- **Accuracy** defined by\n",
    "$$\n",
    "\\mathrm{Acc} := \\frac{\\#\\{i|\\hat{y}^{(i)} \\ne y^{(i)}\\}}{m},\n",
    "$$\n",
    "calculated by `sklearn.metrics.accuracy_score`.\n",
    "\n",
    "- **Precision** for category $y$ defined by\n",
    "$$\n",
    "\\mathrm{Pre}_{y} := \\frac{\\mathrm{TP}_{y}}{\\mathrm{TP}_{y} + \\mathrm{FP}_{y}},\n",
    "$$\n",
    "calculated by `sklearn.metrics.precision_score`.\n",
    "\n",
    "- **Recall** for category $y$ defined by\n",
    "$$\n",
    "\\mathrm{Rec}_{y} := \\frac{\\mathrm{TP}_{y}}{\\mathrm{TP}_{y} + \\mathrm{FN}_{y}},\n",
    "$$\n",
    "calculated by `sklearn.metrics.recall_score`.\n",
    "\n",
    "- **F1 score** for category $y$ defined by\n",
    "$$\n",
    "\\mathrm{F1}_{y} := \\frac{1}{\\frac{1}{\\mathrm{Pre}_{y}} + \\frac{1}{\\mathrm{Rec}_{y}}},\n",
    "$$\n",
    "calculated by `sklearn.metrics.f1_score`  with `average='binary'` (default).\n",
    "\n",
    "Advanced:\n",
    "\n",
    "- **Balanced accuracy** defined by\n",
    "$$\n",
    "\\mathrm{BAcc} := \\frac{1}{\\#\\mathcal{Y}} \\sum_{y \\in \\mathcal{Y}} \\mathrm{Rec}_{y},\n",
    "$$\n",
    "calculated by `sklearn.metrics.balanced_accuracy_score`.\n",
    "\n",
    "- **Macro-averaged F1 score** defined by\n",
    "$$\n",
    "\\mathrm{MacroF1} := \\frac{1}{\\#\\mathcal{Y}} \\sum_{y \\in \\mathcal{Y}} \\mathrm{F1}_{y},\n",
    "$$\n",
    "calculated by `sklearn.metrics.f1_score` with `average='macro'`.\n",
    "\n",
    "- **Weighted-averaged F1 score** defined by\n",
    "$$\n",
    "\\mathrm{WeightF1} := \\sum_{y \\in \\mathcal{Y}} \\frac{m_y}{m} \\mathrm{F1}_{y},\n",
    "$$\n",
    "calculated by `sklearn.metrics.f1_score` with `average='macro'`.\n",
    "\n",
    "Here, $\\mathcal{Y}$ is the set of possible target values, $\\#$ indicates the number of elements in a set, $m$ is the number of data points, $m_{y} := \\#\\{i|y^{(i)} = y.\\}$ is the number of data points whose actual target is $y$. The true positives $\\mathrm{TP}_{y}$, false positives $\\mathrm{FP}_{y}$, false negatives $\\mathrm{FN}_{y}$, and \"true\" negatives $\\mathrm{TN}_{y}$ are defined by\n",
    "$$\n",
    "\\mathrm{TP}_{y} := \\#\\{i|y^{(i)} = y \\textrm{, and } \\hat{y}^{(i)} = y.\\},\n",
    "$$\n",
    "$$\n",
    "\\mathrm{FP}_{y} := \\#\\{i|y^{(i)} \\ne y \\textrm{, and } \\hat{y}^{(i)} = y.\\},\n",
    "$$\n",
    "$$\n",
    "\\mathrm{FN}_{y} := \\#\\{i|y^{(i)} = y \\textrm{, and } \\hat{y}^{(i)} \\ne y.\\},\n",
    "$$\n",
    "$$\n",
    "\\mathrm{TN}_{y} := \\#\\{i|y^{(i)} \\ne y \\textrm{, and } \\hat{y}^{(i)} \\ne y.\\}.\n",
    "$$\n",
    "\n",
    "Also, sklearn has various functions for plotting the confusion matrix and classification reports, which are good for allowing you to evaluate your model at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwI4ykiDmmiD"
   },
   "outputs": [],
   "source": [
    "# The accuracy score: 1 for perfect prediction\n",
    "print('Accuracy: {:.4f}'.format(accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_valid, y_pred_valid)))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_valid, y_pred_valid)))\n",
    "print('F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid)))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='macro')))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='weighted')))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_valid, y_pred_valid, normalize='all')\n",
    "print(f'Confusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat, display_labels=['benign', 'malignant']).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "# Note, the Precision/Recall/F1 in the report match the positive class (1.0) in the report\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5H-o8AWZI4EW"
   },
   "source": [
    "Finally, once we've happy that we've got a good model according to the validation data, we can go ahead with the final evaluation on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elpDDjoBnH7R"
   },
   "outputs": [],
   "source": [
    "############\n",
    "# Test\n",
    "############\n",
    "\n",
    "# Preprocess Test dataset using preprocessor objects for on the training data\n",
    "# Do NOT refit the preprocessors\n",
    "X_test = scaler.transform(X_test_raw) # Standardise the validation data\n",
    "\n",
    "# We select the best hyperparameters based on the performance of our model on the validation dataset\n",
    "y_pred_test = model.predict(X_test) # Use our fitted linear regression model to make a prediction based on teh validation dataset\n",
    "\n",
    "# Plot outputs\n",
    "# Create a meshgrid to plot the decision boundary\n",
    "x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "\n",
    "# Predict the class labels for the points in the meshgrid\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.35, cmap = 'bwr')\n",
    "X_neg = X_test[y_test==0, :]\n",
    "X_pos = X_test[y_test==1, :]\n",
    "plt.scatter(X_neg[:, 0], X_neg[:, 1], color='blue', edgecolors='black', label='Benign')\n",
    "plt.scatter(X_pos[:, 0], X_pos[:, 1], color='red', edgecolors='black', label='Malignant')\n",
    "plt.xlabel('mean radius')\n",
    "plt.ylabel('mean texture')\n",
    "plt.title(f'Logistic Regression Classifier on Breast Cancer Dataset (Test)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# The accuracy score: 1 for perfect prediction\n",
    "print('Accuracy: {:.4f}'.format(accuracy_score(y_test, y_pred_test)))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_test, y_pred_test, normalize='all')\n",
    "print(f'Confusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat, display_labels=['benign', 'malignant']).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLoV5PVC6aLI"
   },
   "source": [
    "### Note on visualising higher order multivariable classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3THTzCl6m3P"
   },
   "source": [
    "In the solutions so far I've been including plots of the class points over the decision boundary, which I find to be a nice way of visualising how the classification model is drawing the lines to make its decisions. See an example of this in the code cell below - the background colour represents the decision boundaries drawn by the classifier, while the labelled points show what class a data point belongs to. We can visually see that our model does a pretty good job, but we have some crossover of points near the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7hzQd2l6xKw"
   },
   "outputs": [],
   "source": [
    "# Create a meshgrid to plot the decision boundary\n",
    "x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "# Predict the class labels for the points in the meshgrid\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.35, cmap = 'bwr')\n",
    "X_neg = X_test[y_test==0, :]\n",
    "X_pos = X_test[y_test==1, :]\n",
    "plt.scatter(X_neg[:, 0], X_neg[:, 1], color='blue', edgecolors='black', label='Benign')\n",
    "plt.scatter(X_pos[:, 0], X_pos[:, 1], color='red', edgecolors='black', label='Malignant')\n",
    "plt.xlabel('mean radius')\n",
    "plt.ylabel('mean texture')\n",
    "plt.title(f'Logistic Regression Classifier on Breast Cancer Dataset (Test)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8qwfe8F72Nh"
   },
   "source": [
    "But this only works because we are only using two features, we would be unable to draw this decision boundary in more than two dimensions (well, this isn't strictly true, with a 3D plot we could draw the decision surface of 3 features, but we still would then be unable to visualise the decision shape if we had any more features). While there is a similar problem in regression, at least we can get some information from plotting the predicted vs the observed values in a 2D plot (even if we are missing some information). For classification though, without the decision boundary it is very hard to intuit how well your model is doing visually.\n",
    "\n",
    "As such, for most classification models with more features I would just be expecting to see the accuracy metrics/confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPtpqT9xoMxC"
   },
   "source": [
    "##2.3 Multi-class classification Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3Jbe9F9KU2Q"
   },
   "source": [
    "In this section I am going to give you an example of how to fit a multi-class classification model, using the 'Iris' dataset on `sklearn`.\n",
    "\n",
    "As most things are the same I'm just going to be highlighting how a multi-class problem requires some different considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2Dxtf0hoSER"
   },
   "outputs": [],
   "source": [
    "# Load the house price dataset\n",
    "X_pd, y_pd = sklearn.datasets.load_iris(return_X_y=True, as_frame=True)\n",
    "Xy_df = pd.concat([X_pd, y_pd], axis=1)\n",
    "\n",
    "display(Xy_df)\n",
    "\n",
    "# Convert the data to `np.array`\n",
    "X_raw = np.array(X_pd[['sepal length (cm)', 'sepal width (cm)']])\n",
    "y = np.array(y_pd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JihOSlAK1Aw"
   },
   "source": [
    "Likewise, splitting, preprocessing and fitting the model is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uo1psA9epBkp"
   },
   "outputs": [],
   "source": [
    "# Split the data into non-test/test data\n",
    "# While we have 20640 pairs of a feature and target, we use 20% only for the testing, not so we hold back the other 80% for training and validation\n",
    "# Generally we always want to shuffle data for classification.\n",
    "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X_raw, y, test_size=0.20, shuffle=True, random_state=0)\n",
    "\n",
    "# Split the non-test data into non-test/test data\n",
    "# We use 25% of the remaining non-test data only for the validation set, leaving the rest for training\n",
    "# In the end, the splitting ratio will be 6:2:2 for the training, validation, and test data.\n",
    "# Generally we always want to shuffle data for classification.\n",
    "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=0.25, shuffle=True, random_state=0)\n",
    "\n",
    "################\n",
    "# Initialisation\n",
    "################\n",
    "model = LogisticRegression() # Create our prediction model object\n",
    "scaler = StandardScaler() # Here is our standardisation object\n",
    "\n",
    "##########\n",
    "# Training\n",
    "##########\n",
    "\n",
    "scaler.fit(X_train_raw) # Fit the standardisation object on the polynomial feature matrix\n",
    "X_train = scaler.transform(X_train_raw) # Standardise the feature matrix to get our 'final' training data feature matrix\n",
    "\n",
    "model.fit(X_train, y_train) # fit our linear regression model - 'train' the model\n",
    "\n",
    "############\n",
    "# Validation\n",
    "############\n",
    "\n",
    "# Preprocess validation dataset using preprocessor objects for on the training data\n",
    "# Do NOT refit the preprocessors\n",
    "X_valid = scaler.transform(X_valid_raw) # Standardise the validation data\n",
    "\n",
    "# We select the best hyperparameters based on the performance of our model on the validation dataset\n",
    "y_pred_valid = model.predict(X_valid) # Use our fitted linear regression model to make a prediction based on teh validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkA4me2GLLlG"
   },
   "source": [
    "We can see below how our confusion matrix has been changed - as you would expect, there are now three groups to correspond to the three classes. This doesn't really change the interpretation, as we still have correct and incorrect predictions.\n",
    "\n",
    "The only thing to get your head around is that we don't really have a 'positive' class any more, so any notion of false positives/negatives applies only *on a the level of the class we are currently looking at* rather than a statement for the entire model prediction.\n",
    "\n",
    "Likewise, our classification report has a row for each class now, so again we have to consider the classes individually from these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUF0ZtyzpViY"
   },
   "outputs": [],
   "source": [
    "# The accuracy score: 1 for perfect prediction\n",
    "print('Accuracy: {:.4f}'.format(sklearn.metrics.accuracy_score(y_valid, y_pred_valid)))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_valid, y_pred_valid, normalize='all')\n",
    "print(f'Confusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat, display_labels=['Setosa', 'Versicolour', 'Virginica']).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGK-aFwgMs2I"
   },
   "source": [
    "Finally we come to testing, and again things remain the same - but now we can see our decision boundary has three sections.\n",
    "\n",
    "The main thing I want you to note is that we now HAVE to average some of our classification metrics (types of `accuracy` are unaffected). In binary classification we can have a 'positive' class that the model considers for the precision/recall/f1, but as mentioned above we don't have this option for multi class problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dh0O6KIcplgh"
   },
   "outputs": [],
   "source": [
    "############\n",
    "# Test\n",
    "############\n",
    "\n",
    "# Preprocess Test dataset using preprocessor objects for on the training data\n",
    "# Do NOT refit the preprocessors\n",
    "X_test = scaler.transform(X_test_raw) # Standardise the validation data\n",
    "\n",
    "# We select the best hyperparameters based on the performance of our model on the validation dataset\n",
    "y_pred_test = model.predict(X_test) # Use our fitted linear regression model to make a prediction based on teh validation dataset\n",
    "\n",
    "# Create a meshgrid to plot the decision boundary\n",
    "x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "\n",
    "# Predict the class labels for the points in the meshgrid\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.contourf(xx, yy, Z, alpha=0.35, cmap = 'jet')\n",
    "# Create a scatter plot of the data points\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "X_0 = X_valid[y_valid==0, :]\n",
    "X_1 = X_valid[y_valid==1, :]\n",
    "X_2 = X_valid[y_valid==2, :]\n",
    "plt.scatter(X_0[:, 0], X_0[:, 1], color='blue', edgecolors='black', label='Setosa')\n",
    "plt.scatter(X_1[:, 0], X_1[:, 1], color='green', edgecolors='black', label='Versicolour')\n",
    "plt.scatter(X_2[:, 0], X_2[:, 1], color='red', edgecolors='black', label='Virginica')\n",
    "\n",
    "# Show the plot\n",
    "plt.title(f'Logistic Regression Classifier on Iris Dataset')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# The accuracy score: 1 for perfect prediction\n",
    "print('Accuracy: {:.4f}'.format(sklearn.metrics.accuracy_score(y_test, y_pred_test)))\n",
    "######## Note we have to average some of our metrics now!\n",
    "print('Precision: {:.4f}'.format(precision_score(y_test, y_pred_test, average='macro')))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_test, y_pred_test, average='macro')))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_test, y_pred_test)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test, average='macro')))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test, average='weighted')))\n",
    "#######################################################\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_test, y_pred_test, normalize='all')\n",
    "print(f'Confusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ay0V-BPP_MFZ"
   },
   "source": [
    "## 2.4 Hyperparameter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rgXCfoY_ivR"
   },
   "source": [
    "As I keep stressing, there really isn't a difference in the model selection process between classification and regression - other than the models you can use and the evaluation metrics.\n",
    "\n",
    "As such, all the techniques and examples I have already shown you relating to hyperparameter selection and model selection still hold true for these questions. As such, you should still split your data into thtee sets (or use cross validation) and you can use any combination of manual experimentation, hand coded loops and grid searches to help you optimise your hyperparameters and choose the best model.\n",
    "\n",
    "But just to give you a directly comparable example, below I am providing an example of how we could optimise our inverse regression weight for a logistic regression model. If you still don't feel confident with the principles of model selection, do make sure you read through this code (and comments) carefully to make sure you understand what is being done, do ask your tutor if you have any doubts or questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLXOvgv3ApUi"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# Load and split data\n",
    "#####################\n",
    "# Load the breast cancer dataset\n",
    "X_pd, y_pd = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "\n",
    "# Convert the data to `np.array`\n",
    "# note I'm selecting 4 features here - which is more than in our previous examples.\n",
    "X_raw = np.array(X_pd[['mean radius', 'mean texture', 'mean perimeter', 'mean area']])\n",
    "# Note that in the dataset 0 = Malignant and 1 = Benign, but we're going to switch that around going forward\n",
    "# so that the 'positive' class Malignant = 1\n",
    "y = np.array(1.0-y_pd)\n",
    "\n",
    "# Split the data\n",
    "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X_raw, y, test_size=1/10, shuffle=True, random_state=0)\n",
    "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=1/9, shuffle=True, random_state=0)\n",
    "\n",
    "\n",
    "###################################\n",
    "# Initialise Model and preprocessors\n",
    "###################################\n",
    "\n",
    "C_indices = np.arange(20) # Get a list of 1-20\n",
    "Cs = 10.0 ** ( C_indices-10) # Define a list of Cs by taking indices from 10 to -10\n",
    "# Get the F1 score arrays\n",
    "# Note that we could be monitoring any of the other metrics to choose our model.\n",
    "#I have chosen f1 here as it give a balanced penalisation of accuracy and recall, but for your case you may care about maximising another metric\n",
    "f1_train_array = np.full([len(Cs)], np.nan)\n",
    "f1_valid_array = np.full([len(Cs)], np.nan)\n",
    "\n",
    "#Initialise our scaler object - remember that for Logistic regression we should always scale features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#######################\n",
    "# Training & validation\n",
    "#######################\n",
    "\n",
    "# preprocessing\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_valid = scaler.transform(X_valid_raw)\n",
    "\n",
    "# Loop to test C values\n",
    "for C_index, C in zip(C_indices, Cs):\n",
    "  # train\n",
    "  model = LogisticRegression(C=C) # fits our Logistic Regression model with a new regularisation weight each iteration\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred_train = model.predict(X_train)\n",
    "  f1_train = f1_score(y_train, y_pred_train)\n",
    "\n",
    "  # Validation\n",
    "  y_pred_valid = model.predict(X_valid)\n",
    "  f1_valid = f1_score(y_valid, y_pred_valid)\n",
    "\n",
    "  # Store f1s for this iteration\n",
    "  f1_train_array[C_index] = f1_train\n",
    "  f1_valid_array[C_index] = f1_valid\n",
    "  print(f'C: {C}, f1 score: {f1_valid}.')\n",
    "\n",
    "# Plot our graph of F1 for training and validation\n",
    "plt.plot(Cs, f1_train_array, label='F1 score on training dataset')\n",
    "plt.plot(Cs, f1_valid_array, label='F1 Score on validation dataset')\n",
    "plt.xlabel(r'Regularization weights $C$')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title(r'Regularization weights $C$ and F1 Score')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Select our best performing C (highest f1)\n",
    "best_c_index = np.nanargmax(f1_valid_array)\n",
    "best_C = Cs[best_c_index]\n",
    "print(f'\\nThe best C:', best_C)\n",
    "\n",
    "######\n",
    "# Test\n",
    "######\n",
    "model = LogisticRegression(C=best_C) # fits our Logistic Reg model with the best weight\n",
    "model.fit(X_train, y_train)\n",
    "# Preprocess test data\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "# Predict test data\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "##########\n",
    "# Evaluate\n",
    "##########\n",
    "\n",
    "# Note, because this is multivariable classification I've not included a scatter plot of the decision surface.\n",
    "\n",
    "# Accuracy metrics - 1 is an perfect prediction\n",
    "print('\\nAccuracy: {:.4f}'.format(accuracy_score(y_test, y_pred_test)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_test, y_pred_test)))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_test, y_pred_test)))\n",
    "print('F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test)))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_test, y_pred_test)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test)))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test)))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_test, y_pred_test, normalize='all')\n",
    "print(f'\\nConfusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat, display_labels=['benign', 'malignant']).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "print(f'\\nClassification Report:')\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCPkI3QjNuSq"
   },
   "source": [
    "##2.5 Comparison with a naive baseline\n",
    "\n",
    "One final thing I would say for evaluating your classification model, is that a good final check that your model is behaving well is to see how it compares to what we would call a 'naive baseline'. Generally I would recommend doing this as a final check after you have evaluated the test data as a bit of a 'sanity check'.\n",
    "\n",
    "This can be done using a simpler model (such as Naive Bayes or K-nearest neighbours), but if often done by just classifying by a very basic rule without even looking at the data. If your model does not outperform this (or is very similar) you have to consider if your model is finding any real patterns in the data.\n",
    "\n",
    "The two most common ways I see this being done is just classifying all things as the same class (generally the majority class if there is an unbalance or the positive class), or by randomly selecting a class.\n",
    "\n",
    "Lets try some examples of this on our breast cancer model fitted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vRswzfOPL1Y"
   },
   "outputs": [],
   "source": [
    "# Here we are just going to assume that every point is positive (malignant)\n",
    "# While this does give us perfect recall, I think we agree that telling 60% of people\n",
    "# incorrectly that they have cancer is not ideal!\n",
    "\n",
    "y_pred_test = np.ones_like(y_test)\n",
    "\n",
    "##########\n",
    "# Evaluate\n",
    "##########\n",
    "\n",
    "# Note, because this is multivariable classification I've not included a scatter plot of the decision surface.\n",
    "\n",
    "# Accuracy metrics - 1 is an perfect prediction\n",
    "print('\\nAccuracy: {:.4f}'.format(accuracy_score(y_test, y_pred_test)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_test, y_pred_test)))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_test, y_pred_test)))\n",
    "print('F1 Score: {:.4f}'.format(f1_score(y_test, y_test)))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_test, y_pred_test)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test)))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test)))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_test, y_pred_test, normalize='all')\n",
    "print(f'\\nConfusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat, display_labels=['benign', 'malignant']).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "print(f'\\nClassification Report:')\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpaT7rspQnwx"
   },
   "outputs": [],
   "source": [
    "# Here I'm generating 0 or 1 randomly for each observation\n",
    "# Note I've set the numpy random seed so this is replicable\n",
    "# The performance of this is pretty rubbish - which is good!\n",
    "np.random.seed(1234)\n",
    "y_pred_test = np.random.randint(2, size=len(y_test))\n",
    "\n",
    "##########\n",
    "# Evaluate\n",
    "##########\n",
    "\n",
    "# Note, because this is multivariable classification I've not included a scatter plot of the decision surface.\n",
    "\n",
    "# Accuracy metrics - 1 is an perfect prediction\n",
    "print('\\nAccuracy: {:.4f}'.format(accuracy_score(y_test, y_pred_test)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_test, y_pred_test)))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_test, y_pred_test)))\n",
    "print('F1 Score: {:.4f}'.format(f1_score(y_test, y_test)))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_test, y_pred_test)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test)))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test)))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_test, y_pred_test, normalize='all')\n",
    "print(f'\\nConfusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat, display_labels=['benign', 'malignant']).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "print(f'\\nClassification Report:')\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSQYqYI4R186"
   },
   "source": [
    "Based on both of these, I thing that we can be confident that our model is performing significantly better than a naive baseline - which is what we want to see!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKYnLUmY_MRY"
   },
   "source": [
    "# 3. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phEWAg6_HaIi"
   },
   "source": [
    "We've reached the stage now where for the most part in the exercises, rather than having a single task I want you to experiment with your models to see how good an accuracy you can get for the problem, using all of the tools I have provided you with so far. So when attempting these exercises to fit the models (and when doing your coursework!) make sure you consider the following (non exhaustive) list of aspects to consider:\n",
    "- Data splitting (consider cross validation)\n",
    "- Preprocessing steps\n",
    "- Features used (will you remove any manually, or leave that all down to regularisation?)\n",
    "- Testing different model types\n",
    "- Hyperparameter tuning (Manual, loops, grid search)\n",
    "- Evaluating your final model **only** on the test data!\n",
    "- Compare classification with a Naive baseline\n",
    "\n",
    "If you complete everything, you might want to try the [UCI Machine learning repository](https://archive.ics.uci.edu/) to get some new datasets to practice with (for both classification and regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QZ5rwQQJwMT"
   },
   "source": [
    "## 3.1 Exercise 1 - Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiOubHgyJ0A6"
   },
   "source": [
    "As the primary dataset we've been working with so far, I would like for you to take everything we've been doing above to see how good a model you can fit on the breast cancer dataset.\n",
    "\n",
    "Guidance:\n",
    "- I would like you to try fitting at least one other type of classification model other than Logistic regression.\n",
    "- Try examining multiple hyperparameters.\n",
    "- Try including all of the feature data in the model.\n",
    "- Consider the dataset and what metric you care the most about optimising\n",
    "- This is quite an 'easy' classification problem, so see how good a solution you can get with a robust 60:20:20 data splitting!\n",
    "- Note: Logistic regression can still use polynomial features\n",
    "\n",
    "The best model you get will depend on what metric you decide to maximise. I chose the recall (do you know why?) and using a 60:20:20 data split I was able to get a recall of $94\\%$ on the test data. Can you beat me?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fg_Tim2OMooq"
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# Your code here\n",
    "##############################################################\n",
    "\n",
    "#####################\n",
    "# Load and split data\n",
    "#####################\n",
    "# Load the breast cancer dataset\n",
    "X_pd, y_pd = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "\n",
    "# Convert the data to `np.array`\n",
    "# note here I'm selecting all of the features\n",
    "columns = X_pd.columns.tolist()\n",
    "X_raw = np.array(X_pd[columns])\n",
    "# Note that in the dataset 0 = Malignant and 1 = Benign, but we're going to switch that around going forward\n",
    "# so that the 'positive' class Malignant = 1\n",
    "y = np.array(1.0-y_pd)\n",
    "\n",
    "# Split the data\n",
    "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X_raw, y, test_size=1/5, shuffle=True, random_state=0)\n",
    "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=1/4, shuffle=True, random_state=0)\n",
    "\n",
    "\n",
    "###################################\n",
    "# Initialise Model and preprocessors\n",
    "###################################\n",
    "\n",
    "C_indices = np.arange(20) # Get a list of 1-20\n",
    "Cs = 10.0 ** ( C_indices-10) # Define a list of Cs by taking indices from 10 to -10\n",
    "# Get the recall score arrays\n",
    "# Note that we could be monitoring any of the other metrics to choose our model.\n",
    "# I have chosen recall as for a cancer case you really want to avoid false negatives\n",
    "met_train_array = np.full([len(Cs)], np.nan)\n",
    "met_valid_array = np.full([len(Cs)], np.nan)\n",
    "\n",
    "#Initialise our scaler object - remember that for Logistic regression we should always scale features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#######################\n",
    "# Training & validation\n",
    "#######################\n",
    "\n",
    "# preprocessing\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_valid = scaler.transform(X_valid_raw)\n",
    "\n",
    "# Loop to test C values\n",
    "for C_index, C in zip(C_indices, Cs):\n",
    "  # train\n",
    "  model = LogisticRegression(C=C) # fits our Logistic Regression model with a new regularisation weight each iteration\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred_train = model.predict(X_train)\n",
    "  met_train = recall_score(y_train, y_pred_train)\n",
    "\n",
    "  # Validation\n",
    "  y_pred_valid = model.predict(X_valid)\n",
    "  met_valid = recall_score(y_valid, y_pred_valid)\n",
    "\n",
    "  # Store metric values for this iteration\n",
    "  met_train_array[C_index] = met_train\n",
    "  met_valid_array[C_index] = met_valid\n",
    "  print(f'C: {C}, Recall score: {f1_valid}.')\n",
    "\n",
    "# Plot our graph of f1 for training and validation\n",
    "plt.plot(Cs, met_train_array, label='Precision score on training dataset')\n",
    "plt.plot(Cs, met_valid_array, label='Precision Score on validation dataset')\n",
    "plt.xlabel(r'Regularization weights $C$')\n",
    "plt.ylabel('Recall score')\n",
    "plt.title(r'Regularization weights $C$ and Recall Score')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Select our best performing C (highest f1)\n",
    "best_c_index = np.nanargmax(met_valid_array)\n",
    "best_C = Cs[best_c_index]\n",
    "print(f'\\nThe best C:', best_C)\n",
    "\n",
    "##########\n",
    "# Evaluate best model on the validation set\n",
    "##########\n",
    "\n",
    "model = LogisticRegression(C=best_C) # fits our Logistic Regression with a best regularisation\n",
    "model.fit(X_train, y_train)\n",
    "# Predict validation data\n",
    "y_pred_valid = model.predict(X_valid)\n",
    "# Note, because this is multivariable classification I've not included a scatter plot of the decision surface.\n",
    "\n",
    "# The accuracy score: 1 for perfect prediction\n",
    "print('Accuracy: {:.4f}'.format(accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_valid, y_pred_valid)))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_valid, y_pred_valid)))\n",
    "print('F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid)))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='macro')))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='weighted')))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_valid, y_pred_valid, normalize='all')\n",
    "print(f'Confusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat, display_labels=['benign', 'malignant']).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "# Note, the Precision/Recall/F1 in the report match the positive class (1.0) in the report\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jnEb06ARC7c"
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# Your code here\n",
    "##############################################################\n",
    "\n",
    "#####################\n",
    "# Load and split data\n",
    "#####################\n",
    "# Load the breast cancer dataset\n",
    "X_pd, y_pd = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "\n",
    "# Convert the data to `np.array`\n",
    "# note here I'm selecting all of the features\n",
    "columns = X_pd.columns.tolist()\n",
    "X_raw = np.array(X_pd[columns])\n",
    "# Note that in the dataset 0 = Malignant and 1 = Benign, but we're going to switch that around going forward\n",
    "# so that the 'positive' class Malignant = 1\n",
    "y = np.array(1.0-y_pd)\n",
    "\n",
    "# Split the data\n",
    "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X_raw, y, test_size=1/5, shuffle=True, random_state=0)\n",
    "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=1/4, shuffle=True, random_state=0)\n",
    "\n",
    "\n",
    "###################################\n",
    "# Initialise Model and preprocessors\n",
    "###################################\n",
    "\n",
    "k_indices = np.arange(10) # Get a list of 1-10\n",
    "ks = [2,3,4,5,8,10,15,20,25,30] # Define a list of ks by taking indices from 10 to -10\n",
    "# Get the score arrays\n",
    "# Note that we could be monitoring any of the other metrics to choose our model.\n",
    "# I have chosen recall as for a cancer case you really want to avoid false negatives\n",
    "met_train_array = np.full([len(ks)], np.nan)\n",
    "met_valid_array = np.full([len(ks)], np.nan)\n",
    "\n",
    "#Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#######################\n",
    "# Training & validation\n",
    "#######################\n",
    "\n",
    "# preprocessing\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_valid = scaler.transform(X_valid_raw)\n",
    "\n",
    "# Loop to test C values\n",
    "for k_index, k in zip(k_indices, ks):\n",
    "  # train\n",
    "  model = KNeighborsClassifier(n_neighbors=k) # fits our Knn with new k each iteration\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred_train = model.predict(X_train)\n",
    "  met_train = recall_score(y_train, y_pred_train)\n",
    "\n",
    "  # Validation\n",
    "  y_pred_valid = model.predict(X_valid)\n",
    "  met_valid = recall_score(y_valid, y_pred_valid)\n",
    "\n",
    "  # Store metric values for this iteration\n",
    "  met_train_array[k_index] = met_train\n",
    "  met_valid_array[k_index] = met_valid\n",
    "  print(f'K: {k}, Recall score: {met_valid}.')\n",
    "\n",
    "# Plot our graph of MSE for training and validation\n",
    "plt.plot(ks, met_train_array, label='Precision score on training dataset')\n",
    "plt.plot(ks, met_valid_array, label='Precision Score on validation dataset')\n",
    "plt.xlabel(r'Neighbours $k$')\n",
    "plt.ylabel('Recall score')\n",
    "plt.title(r'Neighbours $k$ and Recall Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Select our best performing k (highest f1)\n",
    "best_k_index = np.nanargmax(met_valid_array)\n",
    "best_k = ks[best_k_index]\n",
    "print(f'\\nThe best k:', best_k)\n",
    "\n",
    "\n",
    "##########\n",
    "# Evaluate best model on the validation set\n",
    "##########\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=best_k)  # fits our Knn model with best k\n",
    "model.fit(X_train, y_train)\n",
    "# Predict validation data\n",
    "y_pred_valid = model.predict(X_valid)\n",
    "# Note, because this is multivariable classification I've not included a scatter plot of the decision surface.\n",
    "\n",
    "# The accuracy score: 1 for perfect prediction\n",
    "print('Accuracy: {:.4f}'.format(accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_valid, y_pred_valid)))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_valid, y_pred_valid)))\n",
    "print('F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid)))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='macro')))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='weighted')))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_valid, y_pred_valid, normalize='all')\n",
    "print(f'Confusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat, display_labels=['benign', 'malignant']).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "# Note, the Precision/Recall/F1 in the report match the positive class (1.0) in the report\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-EQWsDQVlyu"
   },
   "outputs": [],
   "source": [
    "# Looking at our results above, we can see that:\n",
    "# Logistic Regression has\n",
    "# Recall 0.9556\n",
    "# f1 0.9053\n",
    "\n",
    "# Knn has\n",
    "# Recall 0.9333\n",
    "# f1 0.9655\n",
    "\n",
    "# (of course you can consider the other metrics as well)\n",
    "# We were optimising for recall as the most important factor for cancer detection, so for this scenario I would consider the logistic regression model best.\n",
    "# Also, despite what the testing says, I wouldn't really trust a Knn model using k=2\n",
    "# This seems like it is still very much in danger of overfitting the validation data and being unable to generalise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBJOiLG_Rjl2"
   },
   "outputs": [],
   "source": [
    "# now we want to evalaute our final model!\n",
    "######\n",
    "# Test\n",
    "######\n",
    "model = LogisticRegression(C=best_C) # fits our LogisticRegression model best reg weight\n",
    "model.fit(X_train, y_train)\n",
    "# Preprocess test data\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "# Predict test data\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "##########\n",
    "# Evaluate\n",
    "##########\n",
    "\n",
    "# Note, because this is multivariable classification I've not included a scatter plot of the decision surface.\n",
    "\n",
    "# Accuracy metrics - 1 is an perfect prediction\n",
    "print('\\nAccuracy: {:.4f}'.format(accuracy_score(y_test, y_pred_test)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_test, y_pred_test)))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_test, y_pred_test)))\n",
    "print('F1 Score: {:.4f}'.format(f1_score(y_test, y_test)))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_test, y_pred_test)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test, average='macro')))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test, average='weighted')))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_test, y_pred_test, normalize='all')\n",
    "print(f'\\nConfusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat, display_labels=['benign', 'malignant']).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "print(f'\\nClassification Report:')\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShitdL4cKxEC"
   },
   "source": [
    "##3.2 Exercise 2 - Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PydokSmvK1D6"
   },
   "source": [
    "Please take everything we've been doing and see how good a model you can fit on the multi-class Iris dataset.\n",
    "\n",
    "Guidance:\n",
    "- I would like you to try fitting at least one other type of classification model other than Logistic regression.\n",
    "- Try examining multiple hyperparameters.\n",
    "- Try including all of the data in the model.\n",
    "- Note: Logistic regression can still use polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2wLrbZcWpIu"
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# Your code here\n",
    "##############################################################\n",
    "\n",
    "#####################\n",
    "# Load and split data\n",
    "#####################\n",
    "# Load the breast cancer dataset\n",
    "X_pd, y_pd = sklearn.datasets.load_iris(return_X_y=True, as_frame=True)\n",
    "\n",
    "# Convert the data to `np.array`\n",
    "# note here I'm selecting all of the features\n",
    "columns = X_pd.columns.tolist()\n",
    "X_raw = np.array(X_pd[columns])\n",
    "y = np.array(y_pd)\n",
    "\n",
    "# Split the data\n",
    "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X_raw, y, test_size=1/5, shuffle=True, random_state=0)\n",
    "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=1/4, shuffle=True, random_state=0)\n",
    "\n",
    "\n",
    "###################################\n",
    "# Initialise Model and preprocessors\n",
    "###################################\n",
    "\n",
    "C_indices = np.arange(20) # Get a list of 1-20\n",
    "Cs = 10.0 ** ( C_indices-10) # Define a list of Cs by taking indices from 10 to -10\n",
    "# Get the recall score arrays\n",
    "# Note that we cold be monitoring any of the other metrics to choose our model.\n",
    "# I have chosen recall as for a cancer case you really want to avoid false negatives\n",
    "met_train_array = np.full([len(Cs)], np.nan)\n",
    "met_valid_array = np.full([len(Cs)], np.nan)\n",
    "\n",
    "#Initialise our scaler object - remember that for Logistic regression we should always scale features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#######################\n",
    "# Training & validation\n",
    "#######################\n",
    "\n",
    "# preprocessing\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_valid = scaler.transform(X_valid_raw)\n",
    "\n",
    "# Loop to test C values\n",
    "for C_index, C in zip(C_indices, Cs):\n",
    "  # train\n",
    "  model = LogisticRegression(C=C) # fits our Logistic Regression model with a new regularisation weight each iteration\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred_train = model.predict(X_train)\n",
    "  met_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "\n",
    "  # Validation\n",
    "  y_pred_valid = model.predict(X_valid)\n",
    "  met_valid = f1_score(y_valid, y_pred_valid, average='macro')\n",
    "\n",
    "  # Store metric values for this iteration\n",
    "  met_train_array[C_index] = met_train\n",
    "  met_valid_array[C_index] = met_valid\n",
    "  print(f'C: {C}, f1 score: {met_valid}.')\n",
    "\n",
    "# Plot our graph of f1 for training and validation\n",
    "plt.plot(Cs, met_train_array, label='f1 score on training dataset')\n",
    "plt.plot(Cs, met_valid_array, label='f1 Score on validation dataset')\n",
    "plt.xlabel(r'Regularization weights $C$')\n",
    "plt.ylabel('f1 score')\n",
    "plt.title(r'Regularization weights $C$ and f1 Score')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Select our best performing C (highest f1)\n",
    "best_c_index = np.nanargmax(met_valid_array)\n",
    "best_C = Cs[best_c_index]\n",
    "print(f'\\nThe best C:', best_C)\n",
    "\n",
    "##########\n",
    "# Evaluate best model on the validation set\n",
    "##########\n",
    "\n",
    "model = LogisticRegression(C=best_C) # fits our Logistic Regression with a best regularisation\n",
    "model.fit(X_train, y_train)\n",
    "# Predict validation data\n",
    "y_pred_valid = model.predict(X_valid)\n",
    "# Note, because this is multivariable classification I've not included a scatter plot of the decision surface.\n",
    "\n",
    "# The accuracy score: 1 for perfect prediction\n",
    "print('Accuracy: {:.4f}'.format(accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_valid, y_pred_valid, average='macro')))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_valid, y_pred_valid, average='macro')))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='macro')))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='weighted')))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_valid, y_pred_valid, normalize='all')\n",
    "print(f'Confusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "# Note, the Precision/Recall/F1 in the report match the positive class (1.0) in the report\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRQOvEl1Y9i3"
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# Your code here\n",
    "##############################################################\n",
    " # lets try a decision tree this time\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#####################\n",
    "# Load and split data\n",
    "#####################\n",
    "# Load the breast cancer dataset\n",
    "X_pd, y_pd = sklearn.datasets.load_iris(return_X_y=True, as_frame=True)\n",
    "\n",
    "# Convert the data to `np.array`\n",
    "# note here I'm selecting all of the features\n",
    "columns = X_pd.columns.tolist()\n",
    "X_raw = np.array(X_pd[columns])\n",
    "# Note that in the dataset 0 = Malignant and 1 = Benign, but we're going to switch that around going forward\n",
    "# so that the 'positive' class Malignant = 1\n",
    "y = np.array(y_pd)\n",
    "\n",
    "# Split the data\n",
    "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X_raw, y, test_size=1/5, shuffle=True, random_state=0)\n",
    "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=1/4, shuffle=True, random_state=0)\n",
    "\n",
    "\n",
    "###################################\n",
    "# Initialise Model and preprocessors\n",
    "###################################\n",
    "\n",
    "# Here I'm going to only experiment with the max tree depth, but remember there are a lot of other decision tree parameters to consider (perhaps use a grid search or some nested loops)\n",
    "# I've left the search parameter as C to make my life easy, but you could update the names to be more representative\n",
    "C_indices = np.arange(10) # Get a list of 1-10\n",
    "Cs = [2,3,5,8,10,15,20,25,30,50] # Define a list of Cs by taking indices from 10 to -10\n",
    "# Get the f1 score arrays\n",
    "# Note that we cold be monitoring any of the other metrics to choose our model.\n",
    "# I have chosen f1 as for this case I don't think we particularly care about minimising recall or precision specifically.\n",
    "met_train_array = np.full([len(Cs)], np.nan)\n",
    "met_valid_array = np.full([len(Cs)], np.nan)\n",
    "\n",
    "#scaling not needed\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "#######################\n",
    "# Training & validation\n",
    "#######################\n",
    "\n",
    "# preprocessing\n",
    "X_train = X_train_raw\n",
    "X_valid = X_valid_raw\n",
    "\n",
    "# Loop to test C values\n",
    "for C_index, C in zip(C_indices, Cs):\n",
    "  # train\n",
    "  model = DecisionTreeClassifier(max_depth = C) # fits our Decision tree model with a different max depth\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred_train = model.predict(X_train)\n",
    "  met_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "\n",
    "  # Validation\n",
    "  y_pred_valid = model.predict(X_valid)\n",
    "  met_valid = f1_score(y_valid, y_pred_valid, average='macro')\n",
    "\n",
    "  # Store metric values for this iteration\n",
    "  met_train_array[C_index] = met_train\n",
    "  met_valid_array[C_index] = met_valid\n",
    "  print(f'C: {C}, f1 score: {f1_valid}.')\n",
    "\n",
    "# Plot our graph of f1 for training and validation\n",
    "plt.plot(Cs, met_train_array, label='f1 score on training dataset')\n",
    "plt.plot(Cs, met_valid_array, label='f1 Score on validation dataset')\n",
    "plt.xlabel(r'Max Tree Depth $C$')\n",
    "plt.ylabel('f1 score')\n",
    "plt.title(r'Max Tree Depth $C$ and f1 Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Select our best performing C (highest f1)\n",
    "best_c_index = np.nanargmax(met_valid_array)\n",
    "best_C = Cs[best_c_index]\n",
    "print(f'\\nThe best C:', best_C)\n",
    "\n",
    "##########\n",
    "# Evaluate best model on the validation set\n",
    "##########\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth = best_C) # fits our Logistic Regression with a best regularisation\n",
    "model.fit(X_train, y_train)\n",
    "# Predict validation data\n",
    "y_pred_valid = model.predict(X_valid)\n",
    "# Note, because this is multivariable classification I've not included a scatter plot of the decision surface.\n",
    "\n",
    "# The accuracy score: 1 for perfect prediction\n",
    "print('Accuracy: {:.4f}'.format(accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_valid, y_pred_valid, average='macro')))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_valid, y_pred_valid, average='macro')))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='macro')))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='weighted')))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_valid, y_pred_valid, normalize='all')\n",
    "print(f'Confusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "# Note, the Precision/Recall/F1 in the report match the positive class (1.0) in the report\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qKNf09flWFo"
   },
   "outputs": [],
   "source": [
    "# Possibly we can get a better result with the decision tree with greater tuning of the parameters.\n",
    "# But this is a good case to show how seriously decision trees often overfit the training data.\n",
    "\n",
    "#feel free to explore the decision tree more, but based on these results we can clearly see we're better off with the logistic regression model\n",
    "#where C=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "af7S_O9Wl2OQ"
   },
   "outputs": [],
   "source": [
    "# now we want to evaluate our final model!\n",
    "######\n",
    "# Test\n",
    "######\n",
    "model = LogisticRegression(C=1) # fits our ridge model with a new regularisation weight each iteration\n",
    "model.fit(X_train, y_train)\n",
    "# Preprocess test data\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "# Predict test data\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "##########\n",
    "# Evaluate\n",
    "##########\n",
    "\n",
    "# Note, because this is multivariable classification I've not included a scatter plot of the decision surface.\n",
    "\n",
    "# Accuracy metrics - 1 is an perfect prediction\n",
    "print('\\nAccuracy: {:.4f}'.format(accuracy_score(y_test, y_pred_test)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_test, y_pred_test, average='macro')))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_test, y_pred_test, average='macro')))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_test, y_pred_test)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test, average='macro')))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test, average='weighted')))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_test, y_pred_test, normalize='all')\n",
    "print(f'\\nConfusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "print(f'\\nClassification Report:')\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKorOS7-LWQj"
   },
   "source": [
    "##3.3 Exercise 3 - Nonlinear Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Cr-yRqXLahq"
   },
   "source": [
    "Here I have used the `make_moons` command to create a dataset that has a very nonlinear relationship between the two features (but has clear groups visually).\n",
    "\n",
    "Can you fit a model that classifies this nonlinear data well?\n",
    "\n",
    "Extra: Can you find a way to make Logistic regression classify this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAJkkhAPosBs"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Create a synthetic dataset with two interleaving half circles\n",
    "X, y = make_moons(n_samples=300, noise=0.15, random_state=42)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k', marker='o', s=50, linewidth=1.5)\n",
    "plt.title('Synthetic Nonlinear Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFbnEZ9ul6mF"
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# Your code here\n",
    "##############################################################\n",
    "#####################\n",
    "# Load and split data\n",
    "#####################\n",
    "\n",
    "# Lets start by just using a normal logistic regression\n",
    "# We'll see that it doesn't behave very well...\n",
    "\n",
    "# Create a synthetic dataset with two interleaving half circles\n",
    "X, y = make_moons(n_samples=300, noise=0.15, random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X, y, test_size=1/5, shuffle=True, random_state=0)\n",
    "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=1/4, shuffle=True, random_state=0)\n",
    "\n",
    "\n",
    "###################################\n",
    "# Initialise Model and preprocessors\n",
    "###################################\n",
    "\n",
    "C_indices = np.arange(20) # Get a list of 1-20\n",
    "Cs = 10.0 ** ( C_indices-10) # Define a list of Cs by taking indices from 10 to -10\n",
    "# Get the F1 score arrays\n",
    "# Note that we could be monitoring any of the other metrics to choose our model.\n",
    "#I have chosen f1 here as it give a balanced penalisation of accuracy and recall, but for your case you may care about maximising another metric\n",
    "f1_train_array = np.full([len(Cs)], np.nan)\n",
    "f1_valid_array = np.full([len(Cs)], np.nan)\n",
    "\n",
    "#Initialise our scaler object - remember that for Logistic regression we should always scale features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#######################\n",
    "# Training & validation\n",
    "#######################\n",
    "\n",
    "# preprocessing\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_valid = scaler.transform(X_valid_raw)\n",
    "\n",
    "# Loop to test C values\n",
    "for C_index, C in zip(C_indices, Cs):\n",
    "  # train\n",
    "  model = LogisticRegression(C=C) # fits our Logistic Regression model with a new regularisation weight each iteration\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred_train = model.predict(X_train)\n",
    "  f1_train = f1_score(y_train, y_pred_train)\n",
    "\n",
    "  # Validation\n",
    "  y_pred_valid = model.predict(X_valid)\n",
    "  f1_valid = f1_score(y_valid, y_pred_valid)\n",
    "\n",
    "  # Store f1s for this iteration\n",
    "  f1_train_array[C_index] = f1_train\n",
    "  f1_valid_array[C_index] = f1_valid\n",
    "  print(f'C: {C}, f1 score: {f1_valid}.')\n",
    "\n",
    "# Plot our graph of F1 for training and validation\n",
    "plt.plot(Cs, f1_train_array, label='F1 score on training dataset')\n",
    "plt.plot(Cs, f1_valid_array, label='F1 Score on validation dataset')\n",
    "plt.xlabel(r'Regularization weights $C$')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title(r'Regularization weights $C$ and F1 Score')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Select our best performing C (highest f1)\n",
    "best_c_index = np.nanargmax(f1_valid_array)\n",
    "best_C = Cs[best_c_index]\n",
    "print(f'\\nThe best C:', best_C)\n",
    "\n",
    "##########\n",
    "# Evaluate best model on the validation set\n",
    "##########\n",
    "\n",
    "model = LogisticRegression(C=best_C) # fits our Logistic Regression with a best regularisation\n",
    "model.fit(X_train, y_train)\n",
    "# Predict validation data\n",
    "y_pred_valid = model.predict(X_valid)\n",
    "\n",
    "# Plot the decision boundary\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "x_min, x_max = X_valid[:, 0].min() - 1, X_valid[:, 0].max() + 1\n",
    "y_min, y_max = X_valid[:, 1].min() - 1, X_valid[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "# Plot the points\n",
    "plt.scatter(X_valid[:, 0], X_valid[:, 1], c=y_valid, cmap='viridis', edgecolors='k', marker='o', s=50, linewidth=1.5)\n",
    "plt.title('Synthetic Classification Problem using SVM')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "# The accuracy score: 1 for perfect prediction\n",
    "print('Accuracy: {:.4f}'.format(accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_valid, y_pred_valid)))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_valid, y_pred_valid)))\n",
    "print('F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid)))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='macro')))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='weighted')))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_valid, y_pred_valid, normalize='all')\n",
    "print(f'Confusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "# Note, the Precision/Recall/F1 in the report match the positive class (1.0) in the report\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdXqqqZdp2JZ"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# Load and split data\n",
    "################\n",
    "# One way around this is to bring back our polynomial features!\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create a synthetic dataset with two interleaving half circles\n",
    "X, y = make_moons(n_samples=300, noise=0.15, random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X, y, test_size=1/5, shuffle=True, random_state=0)\n",
    "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=1/4, shuffle=True, random_state=0)\n",
    "\n",
    "\n",
    "###################################\n",
    "# Initialise Model and preprocessors\n",
    "###################################\n",
    "degree = 3 # select some polynomial degree\n",
    "C_indices = np.arange(20) # Get a list of 1-20\n",
    "Cs = 10.0 ** ( C_indices-10) # Define a list of Cs by taking indices from 10 to -10\n",
    "# Get the F1 score arrays\n",
    "# Note that we could be monitoring any of the other metrics to choose our model.\n",
    "#I have chosen f1 here as it give a balanced penalisation of accuracy and recall, but for your case you may care about maximising another metric\n",
    "f1_train_array = np.full([len(Cs)], np.nan)\n",
    "f1_valid_array = np.full([len(Cs)], np.nan)\n",
    "\n",
    "#Initialise our scaler object - remember that for Logistic regression we should always scale features\n",
    "##### Add Poly object############\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "################################\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#######################\n",
    "# Training & validation\n",
    "#######################\n",
    "\n",
    "# preprocessing\n",
    "poly.fit(X_train_raw)\n",
    "X_train_poly = poly.transform(X_train_raw)\n",
    "scaler.fit(X_train_poly)\n",
    "X_train = scaler.transform(X_train_poly)\n",
    "X_valid_poly = poly.transform(X_valid_raw)\n",
    "X_valid = scaler.transform(X_valid_poly)\n",
    "\n",
    "# Loop to test C values\n",
    "for C_index, C in zip(C_indices, Cs):\n",
    "  # train\n",
    "  model = LogisticRegression(C=C) # fits our Logistic Regression model with a new regularisation weight each iteration\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred_train = model.predict(X_train)\n",
    "  f1_train = f1_score(y_train, y_pred_train)\n",
    "\n",
    "  # Validation\n",
    "  y_pred_valid = model.predict(X_valid)\n",
    "  f1_valid = f1_score(y_valid, y_pred_valid)\n",
    "\n",
    "  # Store f1s for this iteration\n",
    "  f1_train_array[C_index] = f1_train\n",
    "  f1_valid_array[C_index] = f1_valid\n",
    "  print(f'C: {C}, f1 score: {f1_valid}.')\n",
    "\n",
    "# Plot our graph of F1 for training and validation\n",
    "plt.plot(Cs, f1_train_array, label='F1 score on training dataset')\n",
    "plt.plot(Cs, f1_valid_array, label='F1 Score on validation dataset')\n",
    "plt.xlabel(r'Regularization weights $C$')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title(r'Regularization weights $C$ and F1 Score')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Select our best performing C (highest f1)\n",
    "best_c_index = np.nanargmax(f1_valid_array)\n",
    "best_C = Cs[best_c_index]\n",
    "print(f'\\nThe best C:', best_C)\n",
    "\n",
    "##########\n",
    "# Evaluate best model on the validation set\n",
    "##########\n",
    "\n",
    "\n",
    "\n",
    "model = LogisticRegression(C=best_C) # fits our Logistic Regression with a best regularisation\n",
    "model.fit(X_train, y_train)\n",
    "# Predict validation data\n",
    "y_pred_valid = model.predict(X_valid)\n",
    "\n",
    "# The accuracy score: 1 for perfect prediction\n",
    "print('Accuracy: {:.4f}'.format(accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_valid, y_pred_valid)))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_valid, y_pred_valid)))\n",
    "print('F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid)))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='macro')))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='weighted')))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_valid, y_pred_valid, normalize='all')\n",
    "print(f'Confusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "# Note, the Precision/Recall/F1 in the report match the positive class (1.0) in the report\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred_valid))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-T2QWr8suuE"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# Load and split data\n",
    "#####################\n",
    "\n",
    "# Alternate solution - a Support Vector Machine!\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create a synthetic dataset with two interleaving half circles\n",
    "X, y = make_moons(n_samples=300, noise=0.15, random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X, y, test_size=1/5, shuffle=True, random_state=0)\n",
    "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=1/4, shuffle=True, random_state=0)\n",
    "\n",
    "\n",
    "###################################\n",
    "# Initialise Model and preprocessors\n",
    "###################################\n",
    "\n",
    "C_indices = np.arange(20) # Get a list of 1-20\n",
    "Cs = 10.0 ** ( C_indices-10) # Define a list of Cs by taking indices from 10 to -10\n",
    "# Get the F1 score arrays\n",
    "# Note that we could be monitoring any of the other metrics to choose our model.\n",
    "#I have chosen f1 here as it give a balanced penalisation of accuracy and recall, but for your case you may care about maximising another metric\n",
    "f1_train_array = np.full([len(Cs)], np.nan)\n",
    "f1_valid_array = np.full([len(Cs)], np.nan)\n",
    "\n",
    "#Initialise our scaler object - remember that for SVM we should always scale features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#######################\n",
    "# Training & validation\n",
    "#######################\n",
    "\n",
    "# preprocessing\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_valid = scaler.transform(X_valid_raw)\n",
    "\n",
    "# Loop to test C values\n",
    "for C_index, C in zip(C_indices, Cs):\n",
    "  # train\n",
    "  model = SVC(C=C) # fits our SVM with a new regularisation weight each iteration\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred_train = model.predict(X_train)\n",
    "  f1_train = f1_score(y_train, y_pred_train)\n",
    "\n",
    "  # Validation\n",
    "  y_pred_valid = model.predict(X_valid)\n",
    "  f1_valid = f1_score(y_valid, y_pred_valid)\n",
    "\n",
    "  # Store f1s for this iteration\n",
    "  f1_train_array[C_index] = f1_train\n",
    "  f1_valid_array[C_index] = f1_valid\n",
    "  print(f'C: {C}, f1 score: {f1_valid}.')\n",
    "\n",
    "# Plot our graph of F1 for training and validation\n",
    "plt.plot(Cs, f1_train_array, label='F1 score on training dataset')\n",
    "plt.plot(Cs, f1_valid_array, label='F1 Score on validation dataset')\n",
    "plt.xlabel(r'Regularization weights $C$')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title(r'Regularization weights $C$ and F1 Score')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Select our best performing C (highest f1)\n",
    "best_c_index = np.nanargmax(f1_valid_array)\n",
    "best_C = Cs[best_c_index]\n",
    "print(f'\\nThe best C:', best_C)\n",
    "\n",
    "##########\n",
    "# Evaluate best model on the validation set\n",
    "##########\n",
    "\n",
    "model = SVC(C=best_C) # fits our SVM with a best regularisation\n",
    "model.fit(X_train, y_train)\n",
    "# Predict validation data\n",
    "y_pred_valid = model.predict(X_valid)\n",
    "\n",
    "# Plot the decision boundary\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "x_min, x_max = X_valid[:, 0].min() - 1, X_valid[:, 0].max() + 1\n",
    "y_min, y_max = X_valid[:, 1].min() - 1, X_valid[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "# Plot the points\n",
    "plt.scatter(X_valid[:, 0], X_valid[:, 1], c=y_valid, cmap='viridis', edgecolors='k', marker='o', s=50, linewidth=1.5)\n",
    "plt.title('Synthetic Classification Problem using SVM')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "# The accuracy score: 1 for perfect prediction\n",
    "print('Accuracy: {:.4f}'.format(accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_valid, y_pred_valid)))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_valid, y_pred_valid)))\n",
    "print('F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid)))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='macro')))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='weighted')))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_valid, y_pred_valid, normalize='all')\n",
    "print(f'Confusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "# Note, the Precision/Recall/F1 in the report match the positive class (1.0) in the report\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WIi4jMyxIzT"
   },
   "outputs": [],
   "source": [
    "# This is a case where both methods have given us identical answers! A SVM with an rbf kernel like this could be equivalent to logistic regression in some cases\n",
    "# as the rbf essentially fits infiite polynomial features to find the best comination. Given that this is quite simple data it could be that degree 3 is the optimal degree anyway.\n",
    "\n",
    "# For this case I'm going to go with our logistic regression model as we have a clearere idea of what is happening with the features, but picking the SVM is entirely justifiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zWtATAdyGS6"
   },
   "outputs": [],
   "source": [
    "# now we want to evaluate our final model!\n",
    "######\n",
    "# Test\n",
    "######\n",
    "model = LogisticRegression(C=10000000.0) # fits our ridge model with a new regularisation weight each iteration\n",
    "model.fit(X_train, y_train)\n",
    "# Preprocess test data - remember we need poly features for this case.\n",
    "poly.fit(X_train_raw)\n",
    "X_train_poly = poly.transform(X_train_raw)\n",
    "scaler.fit(X_train_poly)\n",
    "X_train = scaler.transform(X_train_poly)\n",
    "X_test_poly = poly.transform(X_test_raw)\n",
    "X_test = scaler.transform(X_test_poly)\n",
    "# Predict test data\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "##########\n",
    "# Evaluate\n",
    "##########\n",
    "\n",
    "# Note, because this is multivariable classification I've not included a scatter plot of the decision surface.\n",
    "\n",
    "# Accuracy metrics - 1 is an perfect prediction\n",
    "print('\\nAccuracy: {:.4f}'.format(accuracy_score(y_test, y_pred_test)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_test, y_pred_test)))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_test, y_pred_test)))\n",
    "print('F1 Score: {:.4f}'.format(f1_score(y_test, y_test)))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_test, y_pred_test)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test, average='macro')))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test, average='weighted')))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_test, y_pred_test, normalize='all')\n",
    "print(f'\\nConfusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat, display_labels=['benign', 'malignant']).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "print(f'\\nClassification Report:')\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4oua6p-MQKM"
   },
   "source": [
    "##3.4 Exercise 4 - Wine Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bm-4QoeMVhK"
   },
   "source": [
    "This is another multi class classification dataset. See how accurate a solution you can get to this previously unseen dataset.\n",
    "\n",
    "Guidance:\n",
    "- I would like you to try fitting at least one other type of classification model other than Logistic regression.\n",
    "- Try examining multiple hyperparameters.\n",
    "- Try including all of the data in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvWZfa95y5VT"
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# Your code here\n",
    "##############################################################\n",
    "\n",
    "#####################\n",
    "# Load and split data\n",
    "#####################\n",
    "# Load the wine dataset\n",
    "X_pd, y_pd = sklearn.datasets.load_wine(return_X_y=True, as_frame=True)\n",
    "\n",
    "# Convert the data to `np.array`\n",
    "# note here I'm selecting all of the features\n",
    "columns = X_pd.columns.tolist()\n",
    "X_raw = np.array(X_pd[columns])\n",
    "\n",
    "y = np.array(y_pd)\n",
    "\n",
    "# Split the data\n",
    "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X_raw, y, test_size=1/5, shuffle=True, random_state=1337)\n",
    "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=1/4, shuffle=True, random_state=1337)\n",
    "\n",
    "\n",
    "###################################\n",
    "# Initialise Model and preprocessors\n",
    "###################################\n",
    "\n",
    "C_indices = np.arange(20) # Get a list of 1-20\n",
    "Cs = 10.0 ** ( C_indices-10) # Define a list of Cs by taking indices from 10 to -10\n",
    "# Get the recall score arrays\n",
    "# Note that we cold be monitoring any of the other metrics to choose our model.\n",
    "# I have chosen recall as for a cancer case you really want to avoid false negatives\n",
    "met_train_array = np.full([len(Cs)], np.nan)\n",
    "met_valid_array = np.full([len(Cs)], np.nan)\n",
    "\n",
    "#Initialise our scaler object - remember that for Logistic regression we should always scale features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#######################\n",
    "# Training & validation\n",
    "#######################\n",
    "\n",
    "# preprocessing\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_valid = scaler.transform(X_valid_raw)\n",
    "\n",
    "# Loop to test C values\n",
    "for C_index, C in zip(C_indices, Cs):\n",
    "  # train\n",
    "  model = LogisticRegression(C=C) # fits our Logistic Regression model with a new regularisation weight each iteration\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred_train = model.predict(X_train)\n",
    "  met_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "\n",
    "  # Validation\n",
    "  y_pred_valid = model.predict(X_valid)\n",
    "  met_valid = f1_score(y_valid, y_pred_valid, average='macro')\n",
    "\n",
    "  # Store metric values for this iteration\n",
    "  met_train_array[C_index] = met_train\n",
    "  met_valid_array[C_index] = met_valid\n",
    "  print(f'C: {C}, f1 score: {met_valid}.')\n",
    "\n",
    "# Plot our graph of f1 for training and validation\n",
    "plt.plot(Cs, met_train_array, label='f1 score on training dataset')\n",
    "plt.plot(Cs, met_valid_array, label='f1 Score on validation dataset')\n",
    "plt.xlabel(r'Regularization weights $C$')\n",
    "plt.ylabel('f1 score')\n",
    "plt.title(r'Regularization weights $C$ and f1 Score')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Select our best performing C (highest f1)\n",
    "best_c_index = np.nanargmax(met_valid_array)\n",
    "best_C = Cs[best_c_index]\n",
    "print(f'\\nThe best C:', best_C)\n",
    "\n",
    "##########\n",
    "# Evaluate best model on the validation set\n",
    "##########\n",
    "\n",
    "model = LogisticRegression(C=best_C) # fits our Logistic Regression with a best regularisation\n",
    "model.fit(X_train, y_train)\n",
    "# Predict validation data\n",
    "y_pred_valid = model.predict(X_valid)\n",
    "# Note, because this is multivariable classification I've not included a scatter plot of the decision surface.\n",
    "\n",
    "# The accuracy score: 1 for perfect prediction\n",
    "print('Accuracy: {:.4f}'.format(accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_valid, y_pred_valid, average='macro')))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_valid, y_pred_valid, average='macro')))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_valid, y_pred_valid)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='macro')))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_valid, y_pred_valid, average='weighted')))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_valid, y_pred_valid, normalize='all')\n",
    "print(f'Confusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "# Note, the Precision/Recall/F1 in the report match the positive class (1.0) in the report\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jl3EPv6m0aFl"
   },
   "outputs": [],
   "source": [
    "# Frankly this is looking pretty good already, but let's try another model\n",
    "\n",
    "##############################################################\n",
    "# Your code here\n",
    "##############################################################\n",
    " # lets try a Radom Forest this time\n",
    " # Let's also try optimising with Grid Search instead!\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "#####################\n",
    "# Load and split data\n",
    "#####################\n",
    "# Load the breast cancer dataset\n",
    "X_pd, y_pd = sklearn.datasets.load_wine(return_X_y=True, as_frame=True)\n",
    "\n",
    "# Convert the data to `np.array`\n",
    "# note here I'm selecting all of the features\n",
    "columns = X_pd.columns.tolist()\n",
    "X_raw = np.array(X_pd[columns])\n",
    "# Note that in the dataset 0 = Malignant and 1 = Benign, but we're going to switch that around going forward\n",
    "# so that the 'positive' class Malignant = 1\n",
    "y = np.array(y_pd)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_raw, y, test_size=1/5, shuffle=True, random_state=1337)\n",
    "\n",
    "\n",
    "###################################\n",
    "# Initialise Model and preprocessors\n",
    "###################################\n",
    "\n",
    "# Hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'forest__n_estimators': [10, 30, 100],  # Values for n_estimators\n",
    "    'forest__max_depth': [5, 10, 30],  # Values for DecisionTreeRegressor max depth\n",
    "    'forest__min_samples_split': [2, 10],  # Values for DecisionTreeRegressor min_samples_split\n",
    "    'forest__min_samples_leaf': [2, 10],  # Values for DecisionTreeRegressor min_samples_split\n",
    "}\n",
    "\n",
    "# Initialize Model (no preprocessing needed)\n",
    "forest = RandomForestClassifier(random_state=59)\n",
    "\n",
    "# Create a pipeline with preprocessing and Decision Tree regression\n",
    "pipeline = Pipeline([\n",
    "    ('forest', forest)\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the model with GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best model\n",
    "print(\"Best Model:\", best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKgGqwun3DxT"
   },
   "outputs": [],
   "source": [
    "# Get the results as a DataFrame\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display the relevant columns\n",
    "relevant_columns = ['params', 'mean_test_score', 'std_test_score']\n",
    "cv_results[relevant_columns].sort_values(by='mean_test_score', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugoQ9SNB3x-U"
   },
   "outputs": [],
   "source": [
    "# now we want to evalaute our final model!\n",
    "######\n",
    "# Test\n",
    "######\n",
    "best_model = grid_search.best_estimator_\n",
    "#best_model = RandomForestClassifier(n_estimators=100, max_depth = 5, min_samples_split = 10, min_samples_leaf = 10, random_state=59)\n",
    "# Train the best model on the full training set\n",
    "best_model.fit(X_train, y_train)\n",
    "# Predict test data\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "##########\n",
    "# Evaluate\n",
    "##########\n",
    "\n",
    "# Note, because this is multivariable classification I've not included a scatter plot of the decision surface.\n",
    "\n",
    "# Accuracy metrics - 1 is an perfect prediction\n",
    "print('\\nAccuracy: {:.4f}'.format(accuracy_score(y_test, y_pred_test)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_test, y_pred_test, average='macro')))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_test, y_pred_test, average='macro')))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_test, y_pred_test)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test, average='macro')))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test, average='weighted')))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_test, y_pred_test, normalize='all')\n",
    "print(f'\\nConfusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "print(f'\\nClassification Report:')\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4AUAokH7LlV"
   },
   "outputs": [],
   "source": [
    "# While the Random Forest does quite well, the logistic regression is clearly winning.\n",
    "# This could be an issue of Decision Trees (and hence random forests) not extrapolating well\n",
    "\n",
    "# Essentially, we're using quite a quite large testing set, and our model looks like it may be overfitting our training data a bit.\n",
    "# rerunning the grid search with parameters set in ranges that will make it harder to overfit (shallower models, more datapoints to split, etc) may help\n",
    "# Also may be worth increasing the amount of training data used.\n",
    "\n",
    "# Regardless, logistic regression is going a good job with how things are set up now, so let's evaluate it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-2Xaoiq8UAG"
   },
   "outputs": [],
   "source": [
    "# now we want to evaluate our final model!\n",
    "######\n",
    "# Test\n",
    "######\n",
    "\n",
    "# Make sure our data is setup and processed properly, as the Random Forest did things a bit differently\n",
    "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X_raw, y, test_size=1/5, shuffle=True, random_state=1337)\n",
    "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=1/4, shuffle=True, random_state=1337)\n",
    "\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "\n",
    "model = LogisticRegression(C=best_C) # fits our ridge model with a new regularisation weight each iteration\n",
    "model.fit(X_train, y_train)\n",
    "# Preprocess test data\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "# Predict test data\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "##########\n",
    "# Evaluate\n",
    "##########\n",
    "\n",
    "# Note, because this is multivariable classification I've not included a scatter plot of the decision surface.\n",
    "\n",
    "# Accuracy metrics - 1 is an perfect prediction\n",
    "print('\\nAccuracy: {:.4f}'.format(accuracy_score(y_test, y_pred_test)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_test, y_pred_test, average='macro')))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_test, y_pred_test, average='macro')))\n",
    "print('Balanced Accuracy: {:.4f}'.format(balanced_accuracy_score(y_test, y_pred_test)))\n",
    "print('Macro averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test, average='macro')))\n",
    "print('Weighted averaged F1 Score: {:.4f}'.format(f1_score(y_test, y_pred_test, average='weighted')))\n",
    "# Confusion matrix\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_test, y_pred_test, normalize='all')\n",
    "print(f'\\nConfusion matrix: \\n', confusion_mat)\n",
    "# Visualize the confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay(confusion_mat).plot(cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "# The classification report, which contains accuracy, precision, recall, F1 score\n",
    "print(f'\\nClassification Report:')\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred_test))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "COMP1801-ML(GPU)",
   "language": "python",
   "name": "comp1801-ml"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
