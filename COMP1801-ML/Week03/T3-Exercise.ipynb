{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLTmRhhGjULD"
      },
      "source": [
        "# COMP1801 Tutorial Week 3 - Multivariable Regression Models\n",
        "*Dr Peter Soar - 2024/25*\n",
        "\n",
        "This tutorial will be expanding on the work we did in week two on Linear Regression: revising the vectorised formulation, showing how we can fit multivariable and polynomial models.\n",
        "\n",
        "For this tutorial, read through and try to understand the text and code examples I have provided (ask your tutor if you have any questions) and there will be a selection of exercises. Attempt these exercises on your own, but do ask your tutor for help if you get stuck.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSo3PZoZeF3M"
      },
      "source": [
        "##0. Do not forget to import all the Python Libraries being used!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIhPMWKy2ML2"
      },
      "outputs": [],
      "source": [
        "import numpy as np # A useful package for dealing with mathematical processes, we will be using it this week for vectors and matrices\n",
        "import pandas as pd # A common package for viewing tabular data\n",
        "import time # We will be using this to time the efficiency of vectorisation\n",
        "import sklearn.linear_model, sklearn.datasets # We want to be able to access the sklearn datasets again, also we are using some model evaluation\n",
        "import matplotlib.pyplot as plt # We will be using Matplotlib for our graphs\n",
        "from mpl_toolkits import mplot3d # Used to make a 3D plot used to demonstrate multidimensional relationships.\n",
        "from sklearn.preprocessing import PolynomialFeatures # A preprocessing function allowing us to include polynomial features into our model\n",
        "# from google.colab import files # We will be importing a csv file I have provided for one section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTmklCvcsdTj"
      },
      "source": [
        "# 1. Vectorization\n",
        "We can represent our algorithm more concisely using vectorization, which uses vectors, matrices and linear-algebra operations to represent the operations we need to perform when fitting and predicting with our models.\n",
        "\n",
        "Not only does this simplify the implementation of various tasks, but this vectorised form can also improve the speed of some operations.\n",
        "\n",
        "Note: if you feel very comfortable with vectors and linear algebra you might want to skip this section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN3G1eKymjPJ"
      },
      "source": [
        "## 1.1. Notation revision\n",
        "\n",
        "- Bold lower case letters (e.g. $\\boldsymbol{v}$) indicate a column vector\n",
        "$$\n",
        "\\boldsymbol{v}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "v_{0} \\\\\n",
        "v_{1} \\\\\n",
        "\\vdots \\\\\n",
        "v_{m-1} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "- Bold upper case letters(e.g. $\\boldsymbol{A}$) indicate a ($m \\times n$) matrix\n",
        "$$\n",
        "\\boldsymbol{A}\n",
        "= \\begin{bmatrix}\n",
        "a_{0,0} & a_{0,1} & \\cdots & a_{0,n-1} \\\\\n",
        "a_{1,0} & a_{1,1} & \\cdots & a_{1,n-1} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_{m-1,0} & a_{m-1,1} & \\cdots & a_{m-1,n-1} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "- The maximum rows (data points for ML) and columns (features for ML) are represented by $m$ and $n$ respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPGOVMIPvqoL"
      },
      "source": [
        "## 1.2. Vector multiplication for linear model prediction\n",
        "\n",
        "A key operation that vectorisation aids is simplifying the calculations when making predictions from our linear models.\n",
        "\n",
        "For a single observed set of feature data we can obtain a prediction using $\\hat{y} =\\boldsymbol{x} \\boldsymbol{\\theta}$ where we have\n",
        "\n",
        "$$\n",
        "\\boldsymbol{x} =\n",
        "\\begin{bmatrix}\n",
        "x_{0} &\n",
        "x_{1} &\n",
        "\\dots &\n",
        "x_{m-1} \\\\\n",
        "\\end{bmatrix},\n",
        "\\boldsymbol{\\theta} :=\n",
        "\\begin{bmatrix}\n",
        "\\theta_{0} \\\\\n",
        "\\theta_{1} \\\\\n",
        "\\vdots \\\\\n",
        "\\theta_{m-1} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Allowing us to work out the single value for the prediction by using matrix multiplication rules:\n",
        "$$\n",
        "\\hat{y} = \\boldsymbol{x} \\boldsymbol{\\theta} = \\sum_{i=0}^{m-1} x_{i} \\theta_{i} = x_{0} \\theta_{0} + x_{1} \\theta_{1} + \\cdots+ x_{m-1} \\theta_{m-1}\n",
        "$$\n",
        "\n",
        "\n",
        "Let us take a simple example where we define $\\boldsymbol{x}$ and $\\boldsymbol{\\theta}$ by\n",
        "$$\n",
        "\\boldsymbol{x} =\n",
        "\\begin{bmatrix}\n",
        "1 &\n",
        "3 \\\\\n",
        "\\end{bmatrix},\n",
        "\\boldsymbol{\\theta} =\n",
        "\\begin{bmatrix}\n",
        "-1 \\\\\n",
        "2 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Our prediction for this data point would be:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\boldsymbol{x} \\boldsymbol{\\theta} = \\sum_{i=0}^{1} x_{i} \\theta_{i} = x_{0} \\theta_{0} + x_{1} \\theta_{1} = 1 \\times -1 + 3 \\times 2 = 5\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmMoqvhGvmK7"
      },
      "source": [
        "Let's check this using the computer.\n",
        "\n",
        "In the following, we regard $\\boldsymbol{x}$ and $\\boldsymbol{\\theta}$ as 2D `np.array`s with the size of $1 \\times 2$ and $2 \\times 1$ respectively. $\\boldsymbol{x} \\boldsymbol{\\theta}$ is given by the matrix multiplication of $\\boldsymbol{x}$ and $\\boldsymbol{\\theta}$. In NumPy matrix multiplication is given by `@` (or you can use dot product)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMf8Mgfdx0nR"
      },
      "outputs": [],
      "source": [
        "x = np.array([[1, 3]])\n",
        "th = np.array([[-1],[2]])\n",
        "\n",
        "y_pred = x @ th\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smuAxN1gto4j"
      },
      "source": [
        "## 1.3. Matrix multiplication for linear model prediction\n",
        "\n",
        "Of course, usually we want to make more than one prediction at a time, which means multiplying the parameters vector by the entire feature matrix such that our predictions are now a vector defined by $\\hat{\\boldsymbol{y}} = \\boldsymbol{X} \\boldsymbol{\\theta}$.\n",
        "\n",
        "where\n",
        "$$\n",
        "\\boldsymbol{X}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 & x_{1}^{(0)} & x_{2}^{(0)} & \\cdots & x_{n-1}^{(0)} \\\\\n",
        "1 & x_{1}^{(1)} & x_{2}^{(1)} & \\cdots & x_{n-1}^{(1)} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{1}^{(m-1)} & x_{2}^{(m-1)} & \\cdots & x_{n-1}^{(m-1)} \\\\\n",
        "\\end{bmatrix},\n",
        "\\boldsymbol{\\theta}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\theta_{0} \\\\\n",
        "\\theta_{1} \\\\\n",
        "\\vdots \\\\\n",
        "\\theta_{n-1} \\\\\n",
        "\\end{bmatrix},\n",
        "\\hat{\\boldsymbol{y}}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\hat{y}^{(0)} \\\\\n",
        "\\hat{y}^{(1)} \\\\\n",
        "\\vdots \\\\\n",
        "\\hat{y}^{(m-1)} \\\\\n",
        "\\end{bmatrix},\n",
        "$$\n",
        "\n",
        "This means that for each $i = 0, 1, \\dots, m-1$ the prediction will be:\n",
        "$$\n",
        "\\hat{y}^{(i)} = \\theta_{0} + x_{1}^{(i)} \\theta_{1} + x_{2}^{(i)} \\theta_{2} + \\dots + x_{n-1}^{(i)} \\theta_{n-1}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "773ZaNXz9dnN"
      },
      "source": [
        "### 1.3.1 Exercise 1\n",
        "Define $\\boldsymbol{X}$ and $\\boldsymbol{\\theta}$ by\n",
        "$$\n",
        "\\boldsymbol{X} =\n",
        "\\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "1 & 3 \\\\\n",
        "1 & -2 \\\\\n",
        "\\end{bmatrix},\n",
        "\\boldsymbol{\\theta} =\n",
        "\\begin{bmatrix}\n",
        "-1 \\\\\n",
        "2 \\\\\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Find the prediction vector $\\hat{\\boldsymbol{y}}$ using matrix multiplication.\n",
        "\n",
        "To do this we define $\\boldsymbol{X}$ and $\\boldsymbol{\\theta}$ as 2D `np.array`s with the size of $3 \\times 2$ and $2 \\times 1$, respectively. $\\boldsymbol{X} \\boldsymbol{\\theta}$ is given by the matrix multiplication of $\\boldsymbol{X}$ and $\\boldsymbol{\\theta}$. In NumPy, the matrix multiplication is given by `@`.\n",
        "Store the results in `y_pred`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bru6TIcFEwu"
      },
      "outputs": [],
      "source": [
        "##############################################################\n",
        "# Your code here\n",
        "##############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrPCs2rFrwNz"
      },
      "source": [
        "## 1.4 Efficiency of the vectorization.\n",
        "Vectorization not only simplify the mathematical notation, but also makes calculation more efficient computationally.\n",
        "\n",
        "In the following, we compare the implementations of the linear regression using the vectorized form and `for` loops.\n",
        "\n",
        "First, we define the two implementations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP8h5X3FskGx"
      },
      "outputs": [],
      "source": [
        "def linear_prediction_by_vectorization(X, th):\n",
        "  y_pred = X @ th\n",
        "  return y_pred\n",
        "\n",
        "def linear_prediction_by_for_loops(X, th):\n",
        "  m, n = X.shape\n",
        "  y_pred = np.zeros([m, 1], dtype=float)\n",
        "  for i in range(m):\n",
        "    for j in range(n):\n",
        "      y_pred[i] += X[i, j] * th[j, 0]\n",
        "  return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t9JYjfAvlKx"
      },
      "source": [
        "In the following, we measure the elapsed time of both implementations, see that the vectorised code is almost twice as fast even on this tiny example. For larger datasets this difference should only become more pronounced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZsnJHKIte7o"
      },
      "outputs": [],
      "source": [
        "x = np.array([[1, 3]])\n",
        "th = np.arange(x.shape[-1]).reshape([-1, 1]) # An example of the parameter vector.\n",
        "\n",
        "execution_start_time = time.time()\n",
        "y_pred_vec = linear_prediction_by_vectorization(x, th)\n",
        "execution_end_time = time.time()\n",
        "\n",
        "elapsed_time_by_vectorization = execution_end_time - execution_start_time\n",
        "\n",
        "print(f'Elapsed time of linear prediction by vectorization: {elapsed_time_by_vectorization * 1000}ms.')\n",
        "\n",
        "execution_start_time = time.time()\n",
        "y_pred_for = linear_prediction_by_for_loops(x, th)\n",
        "execution_end_time = time.time()\n",
        "\n",
        "elapsed_time_by_for_loops = execution_end_time - execution_start_time\n",
        "\n",
        "print(f'Elapsed time of linear prediction by `for` loops: {elapsed_time_by_for_loops * 1000}ms.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS4VXPRD3CxD"
      },
      "source": [
        "### 1.4.1 Exercise 2\n",
        "\n",
        "Apply both methods above (vectorised and loop based prediction) with the California housing data to use the 'Median Income' to predict the 'House Value' (target). Start with a small sample (say 200) for your training data (you don't need to split it into training/new for this exercise), then increase to larger values (the dataset has just over 20000 entries) to see how the timings change.\n",
        "\n",
        "Note: you may want to use previous tutorials (or examples from later on in this tutorial) for reference for how to correctly load in and process the data for the model. As this is a timing exercise, we don't actually need to fit the model and can use any values for our hypothesis vector $\\boldsymbol{\\theta}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qseXEr5T4lsj"
      },
      "outputs": [],
      "source": [
        "##############################################################\n",
        "# Your code here\n",
        "##############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLdeb3gExU-u"
      },
      "source": [
        "#2. Multivariable regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLZ2zGH7Dcfn"
      },
      "source": [
        "In tutorial 2 we were primarily working with simple univariate linear regression where we fit a model predicting:\n",
        "$$\\hat{\\boldsymbol{y}} = \\boldsymbol{X} \\boldsymbol{\\theta}$$\n",
        "\n",
        "where the vectorised form our univariate linear model will look like this:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\hat{y}^{(0)} \\\\\n",
        "\\hat{y}^{(1)} \\\\\n",
        "\\vdots \\\\\n",
        "\\hat{y}^{(m-1)} \\\\\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "1 & x_{1}^{(0)}  \\\\\n",
        "1 & x_{1}^{(1)}  \\\\\n",
        "\\vdots & \\vdots  \\\\\n",
        "1 & x_{1}^{(m-1)} \\\\\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "\\theta_{0} \\\\\n",
        "\\theta_{1} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Where the column of 1's are needed to correspond with the constant parameter $\\theta_0$ used by the model (Note: we don't have to account for this when using sklearn `fit` model functionality).\n",
        "\n",
        "For multivariate linear regression we can still summarise our model as $\\hat{\\boldsymbol{y}} = \\boldsymbol{X} \\boldsymbol{\\theta}$ in matrix form, but when expanded the feature matrix and parameter vector expand to account for the extra features:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\hat{y}^{(0)} \\\\\n",
        "\\hat{y}^{(1)} \\\\\n",
        "\\vdots \\\\\n",
        "\\hat{y}^{(m-1)} \\\\\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "1 & x_{1}^{(0)} & x_{2}^{(0)} & \\cdots & x_{n-1}^{(0)} \\\\\n",
        "1 & x_{1}^{(1)} & x_{2}^{(1)} & \\cdots & x_{n-1}^{(1)} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{1}^{(m-1)} & x_{2}^{(m-1)} & \\cdots & x_{n-1}^{(m-1)} \\\\\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "\\theta_{0} \\\\\n",
        "\\theta_{1} \\\\\n",
        "\\vdots \\\\\n",
        "\\theta_{n-1} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "Multivariate models such as this are far more common as the accuracy that can be obtained using a single variable is usually quite limited in the real world, as there are often many relationships between many features which dictate the full behaviour observed in the target values being predicted during supervised learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDk0jRsxO99s"
      },
      "source": [
        "## 2.1. Univariate Example Revision\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd8ZEjvAPk79"
      },
      "source": [
        "Below I am going to reproduce the code I provided in week 2 showing how to fit a univariate regression model of the median income vs the house value for the california housing dataset - hopefully you all recognise this code and understand all the functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cas6Y3YmPIaj"
      },
      "outputs": [],
      "source": [
        "# Load the house price dataset\n",
        "X_pd, y_pd = sklearn.datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Combine the X and y for visualisation\n",
        "XY_pd = pd.concat([X_pd, y_pd], axis=1)\n",
        "\n",
        "# Shuffle dataset for a random sample, as we will not be using the entire dataset\n",
        "rng = np.random.default_rng(0) # This sets the random seed, meaning that we will get the SAME random sample if we rerun this cell.\n",
        "df = XY_pd.iloc[rng.permutation(len(XY_pd))].reset_index(drop=True) # Shuffle data\n",
        "\n",
        "# Use only one feature to start with for univariate linear regression - The Median Income\n",
        "Xy_df = df[['MedInc','MedHouseVal']]\n",
        "\n",
        "# So for this task we are going to be predicting the property value based on the Median Income\n",
        "\n",
        "# prepare NumPy ndarrays\n",
        "X_raw = np.array(Xy_df[['MedInc']])\n",
        "y_raw = np.array(Xy_df['MedHouseVal'])\n",
        "\n",
        "# Define how much data we want. You can try changing these later to see how it changes the model and predictions\n",
        "n = 200 # This is the data that will be used to train our model.\n",
        "\n",
        "# Split the feature data into our sample\n",
        "X = X_raw[:n] # This will select the data points from 0 to the number defined in n (200 in this case)\n",
        "\n",
        "# Split the target data into our sample\n",
        "y = y_raw[:n]\n",
        "\n",
        "# Create linear regression object\n",
        "obj = sklearn.linear_model.LinearRegression(fit_intercept=True)\n",
        "\n",
        "# Train the model using the training data\n",
        "obj.fit(X, y)\n",
        "\n",
        "# Make predictions using the training data\n",
        "y_pred = obj.predict(X)\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X, y,  color='black', label='y') # Observed y values\n",
        "plt.scatter(X, y_pred, color='blue', label='y_pred') # predicted y values\n",
        "plt.plot(np.r_[0:12:0.1], obj.predict(np.r_[0:12:0.1][:, np.newaxis]), color='blue', label='hypothesis') # Regression line\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('house price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The mean squared error loss\n",
        "print('Mean squared error loss: {:.4f}'.format(sklearn.metrics.mean_squared_error(y, y_pred)))\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe5w2bYtQCqK"
      },
      "source": [
        "## 2.2 Multivariate example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhj2ujZ-QKOp"
      },
      "source": [
        "It is actually very straightforward to expand this code to handle a multivariate model - all we need to do is add more feature columns to the `X` array and everything else will continue to work the same. Below I will add the `HouseAge` feature to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzInxOzvlF-e"
      },
      "outputs": [],
      "source": [
        "# Load the house price dataset\n",
        "X_pd, y_pd = sklearn.datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Combine the X and y for visualisation\n",
        "XY_pd = pd.concat([X_pd, y_pd], axis=1)\n",
        "\n",
        "# Shuffle dataset for a random sample, as we will not be using the entire dataset\n",
        "rng = np.random.default_rng(0) # This sets the random seed, meaning that we will get the SAME random sample if we rerun this cell.\n",
        "df = XY_pd.iloc[rng.permutation(len(XY_pd))].reset_index(drop=True) # Shuffle data\n",
        "\n",
        "# Now using two features for multivariate linear regression - The Median Income and the House Age\n",
        "Xy_df = df[['MedInc', 'HouseAge','MedHouseVal']]\n",
        "\n",
        "# So for this task we are going to be predicting the property value based on the Median Income and the house age\n",
        "\n",
        "# prepare NumPy ndarrays\n",
        "X_raw = np.array(Xy_df[['MedInc', 'HouseAge']])  # !!!!!! Make sure you put any additional features into this array !!!!!!!!!!\n",
        "y_raw = np.array(Xy_df['MedHouseVal'])\n",
        "\n",
        "# Define how much data we want. You can try changing these later to see how it changes the model and predictions\n",
        "n = 200 # This is the data that will be used to train our model.\n",
        "\n",
        "# Split the feature data into our sample\n",
        "X = X_raw[:n] # This will select the data points from 0 to the number defined in n_train_points (200 in this case)\n",
        "\n",
        "# Split the target data into our sample\n",
        "y = y_raw[:n]\n",
        "\n",
        "# Create linear regression object\n",
        "obj = sklearn.linear_model.LinearRegression(fit_intercept=True)\n",
        "\n",
        "# Train the model using the training data\n",
        "obj.fit(X, y)\n",
        "\n",
        "# Make predictions using the training data\n",
        "y_pred = obj.predict(X)\n",
        "\n",
        "# The mean squared error loss\n",
        "print('Mean squared error loss: {:.4f}'.format(sklearn.metrics.mean_squared_error(y, y_pred)))\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD_DSLViRaGn"
      },
      "source": [
        "While still not a great model, we can see that the $R^2$ score has increased by a clear amount, so our two variable model is describing more of the variation than the univariate case. You may have noticed that in the code above I didn't output any graphs. When we move into multivariate cases we begin to encounter problems when it comes to visualising our results as we can only have one feature variable on the `x` axis. I have also removed the regression line for the plot, as we cannot really draw a multivariate regrerssion line that tells us much useful information in a 2D plane like this.\n",
        "\n",
        "So we could make a graph of Median Income vs price:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNkqvhx7SSME"
      },
      "outputs": [],
      "source": [
        "X_disp = X[:,0] # We will need to make a special vector for the feature we want on the x axis, as now X is a matrix matplotlib can't use it for a scatter plot\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_disp, y,  color='black', label='y') # Observed y values\n",
        "plt.scatter(X_disp, y_pred, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('House Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXa6waOtTaZn"
      },
      "source": [
        "Or the House Age vs the price:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuUIIvuyTfV4"
      },
      "outputs": [],
      "source": [
        "X_disp = X[:,1] # We will need to make a special vector for the feature we want on the x axis, as now X is a matrix matplotlib can't use it for a scatter plot\n",
        "\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_disp, y,  color='black', label='y') # Observed y values\n",
        "plt.scatter(X_disp, y_pred, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('HouseAge')\n",
        "plt.ylabel('House Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP2sSIehTxSU"
      },
      "source": [
        "It is fine to keep an eye on how well you are predicting the target variables vs any of the features that you are using such as this, as providing your model is describing more of the variation, the predicted and true values should begin matching more closely. However, never forget that you are summarising a complex, multivariate relationship down to a simple set of 2D axes containing only one of those features, and while exploring multiple plots of different features can be a good exercise, it really isn't practical for more complex models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXRx--33WEjG"
      },
      "source": [
        "While drawing a regression line isn't that helpful for a multivariate model, for a two featured model such as this we will in fact have a regression plane (or surface) that we can actually visualise (much like with the optimisation methods last week, going higher than this means things would have to be represented in higher dimensions which we can't easily visualise).\n",
        "\n",
        "Below I have made a 3D plot which shows the true relationship for our fitted model of Income & House Age vs House Price. Try moving the plot around (the `ax.view_init` line) to get a better idea of how these three variables are related"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX21RnF2Wpd7"
      },
      "outputs": [],
      "source": [
        "# Creating dataset\n",
        "x_ax = np.array([[0],[1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11],[12],[13],[14]], dtype='f') # X value range\n",
        "y_ax = np.array([[0,5,10,15,20,25,30,35,40,45,50,55,60,65,70]], dtype='f') # y value range\n",
        "z = obj.intercept_+obj.coef_[0]*x_ax+obj.coef_[1]*y_ax # this is the function for the 2D regression surface, note that I am using the coefficients of our fitted model object\n",
        "\n",
        "# Creating figure\n",
        "fig = plt.figure(figsize =(10, 10))\n",
        "ax = plt.axes(projection ='3d')\n",
        "\n",
        "# Creating plot\n",
        "ax.plot_surface(x_ax, y_ax, z, cmap=\"plasma\", alpha=0.5) # Our regression surface\n",
        "\n",
        "ax.scatter(X[:,0],X[:,1],y, c=\"black\",s=50) # Training Data\n",
        "ax.scatter(X[:,0],X[:,1],y_pred, c=\"blue\",s=50) # Predicted Values\n",
        "\n",
        "ax.view_init(elev=20, azim=300) # Change these variables to get a different view of the 3D plot\n",
        "\n",
        "ax.set_xlabel(\"Median Income\")\n",
        "ax.set_ylabel(\"House Age\")\n",
        "ax.set_zlabel(\"House Price\")\n",
        "\n",
        "# show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn-5-My1dRkl"
      },
      "source": [
        "### 2.2.1 Exercise 3\n",
        "\n",
        "Try adding more features to the multivariate model for the California housing data.\n",
        "\n",
        "1.   Try adding all of the features at once. How does the accuracy of this model compare to the previous models with less features?\n",
        "2.   Now try adding (or taking away) features one at a time. Do they all contribute to the models accuracy?\n",
        "3. Once you've chosen a final model, try increasing the size of your sample in increments (training and testing should both be increased approximately in proportion, this dataset has just over 20000 data points). Does the accuracy of your model change much? Is there anything concerning you spot about some of your predictions?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH6glA_sd2Gp"
      },
      "outputs": [],
      "source": [
        "##############################################################\n",
        "# Your code here\n",
        "##############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMUXEFoU1zMX"
      },
      "source": [
        "# 3. polynomial features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pI9eyFo2Obq"
      },
      "source": [
        "So far we have only considered linear relationships between our variables, however that is not always the case - considering the power of a variable (say $\\theta_2^2$) or the interactions between variables (say $\\theta_2 \\theta_3$) may describe the relationship between the variables and the target more accurately than any linear combination of those variables is able too.\n",
        "\n",
        "As a group these are known as *polynomial features*, and can be easily created in sklearn by creating a `sklearn.preprocessing.PolynomialFeatures` [instance](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html?highlight=polynomial#sklearn.preprocessing.PolynomialFeatures).\n",
        "\n",
        "\n",
        "For the following examples we will continue using the California Housing Dataset, we will be going back to only considering `MedInc` as a feature to lower computational costs and make the examples clearer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFzFi7SbhHUA"
      },
      "outputs": [],
      "source": [
        "# Load the house price dataset\n",
        "X_pd, y_pd = sklearn.datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Combine the X and y for visualisation\n",
        "XY_pd = pd.concat([X_pd, y_pd], axis=1)\n",
        "\n",
        "# Shuffle dataset for a random sample, as we will not be using the entire dataset\n",
        "rng = np.random.default_rng(0) # This sets the random seed, meaning that we will get the SAME random sample if we rerun this cell.\n",
        "df = XY_pd.iloc[rng.permutation(len(XY_pd))].reset_index(drop=True) # Shuffle data\n",
        "\n",
        "# prepare NumPy ndarrays\n",
        "X_raw = np.array(df[['MedInc']])  # !!!!!! Make sure you put any additional features into this array !!!!!!!!!!\n",
        "y_raw = np.array(df['MedHouseVal'])\n",
        "\n",
        "# Define how much data we want. You can try changing these later to see how it changes the model and predictions\n",
        "n = 50 # This is the data that will be used to train our model.\n",
        "\n",
        "# Split the feature data into our sample\n",
        "X = X_raw[:n] # This will select the data points from 0 to the number defined in n (200 in this case)\n",
        "\n",
        "# Split the target data into our sample\n",
        "y = y_raw[:n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIj_zKq7YFW_"
      },
      "source": [
        "##3.1 Initialising the Polynomial features:\n",
        "\n",
        "Using [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html?highlight=polynomial#sklearn.preprocessing.PolynomialFeatures) is another preprocessing step, and is very similar to the feature scaling we looked at last week.\n",
        "\n",
        "- Initialize the polynomial features object.\n",
        "  - Method: `PolynomialFeatures(degree)`\n",
        "  - Input parameter: Maximum polynomial degree you want for your model (this is really a hyperparameter we need to choose carefully, but we will cover how to best pick this degree next week).\n",
        "- Output: Creates a new feature matrix with polynomial features.\n",
        "  - Method: `poly.fit_transform`\n",
        "  - Example: Let\n",
        "  $$\\begin{bmatrix} \\tilde{\\boldsymbol{x}}_{1} \\end{bmatrix}\n",
        "  = \\begin{bmatrix} x_{1}^{(0)} \\\\ x_{1}^{(1)} \\\\ \\vdots \\\\ x_{1}^{(m - 1)} \\end{bmatrix}$$ be the feature matrix. Then, the feature matrix created by the instance `PolynomialFeatures(3)` is\n",
        "  $$\\begin{bmatrix} \\tilde{\\boldsymbol{x}}_{1} & (\\tilde{\\boldsymbol{x}}_{1})^2 & (\\tilde{\\boldsymbol{x}}_{1})^3 \\end{bmatrix}\n",
        "  = \\begin{bmatrix} x_{1}^{(0)} & (x_{1}^{(0)})^2 & (x_{1}^{(0)})^3 \\\\ x_{1}^{(1)} & (x_{1}^{(1)})^2 & (x_{1}^{(1)})^3 \\\\ \\vdots & \\vdots & \\vdots \\\\ x_{1}^{(m - 1)} & (x_{1}^{(m - 1)})^2 & (x_{1}^{(m - 1)})^3 \\end{bmatrix}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kvTVVKcMrAJ"
      },
      "outputs": [],
      "source": [
        "degree = 3 # Define the maximum power of polynomial features you want to include\n",
        "poly = PolynomialFeatures(degree) # Create the polynomial features object\n",
        "X_poly = poly.fit_transform(X) # Fit the poly object to the training data to make a new feature matrix\n",
        "print(X_poly) # View the new feature matrix - do you understand what's happening?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr8PyCsQbm-n"
      },
      "source": [
        "## 3.2 Fit `sklearn`'s `LinearRegression` instance to the polynomial features.\n",
        "We can fit a regression model the same as we did for our multivariable models earlier.\n",
        "\n",
        "Note that this time we're using `fit_intercept=False` as fitting the polynomial features gives us a column of $1$'s by default, which will add a constant contribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrM1CkrUbX1q"
      },
      "outputs": [],
      "source": [
        "reg = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
        "reg.fit(X_poly, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkAHHq5DqKA1"
      },
      "source": [
        "## 3.3 Make prediction on the fitted object on the new data using polynomial features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Thcl0uUYqKA2"
      },
      "source": [
        "We can make predictions with our polynomial features in the same way that we used for the multivariable model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8zWiAb2qKA3"
      },
      "outputs": [],
      "source": [
        "y_pred = reg.predict(X_poly) # Store our predictions\n",
        "display(y_pred) # Display our predicted values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnQSgdxKs4xw"
      },
      "source": [
        "## 3.4 What is our cost function for the training data?\n",
        "How much of the variation in our training data is being described by our model with polynomial features? We can check this by making a prediction with our training data and calculating the mean squared error against the `y_train` and the $R^2$ score. We should see that we are describing quite a lot of variation with just this one feature now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXh0_QiNt4FS"
      },
      "outputs": [],
      "source": [
        "y_pred = reg.predict(X_poly) # Make our predictions of the training data\n",
        "mse = sklearn.metrics.mean_squared_error(y, y_pred) # calculate the MSE\n",
        "print(mse)\n",
        "R2 = sklearn.metrics.r2_score(y, y_pred) # calculate the MSE\n",
        "print(R2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvjoPERLx0It"
      },
      "source": [
        "Let's try plotting this and see what our hypothesis looks like based on the polynomial feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKJtrRK6xkMx"
      },
      "outputs": [],
      "source": [
        "# Plot outputs\n",
        "plt.scatter(X[:,0], y,  color='black', label='y_true') # Observed y values\n",
        "plt.scatter(X[:,0], y_pred, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('House Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw4Bt5udx9xP"
      },
      "source": [
        "This looks good, can we get an even better fit using a higher degree of polynomial features?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hca_XeyxjCYQ"
      },
      "outputs": [],
      "source": [
        "# Load the house price dataset\n",
        "X_pd, y_pd = sklearn.datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Combine the X and y for visualisation\n",
        "XY_pd = pd.concat([X_pd, y_pd], axis=1)\n",
        "\n",
        "# Shuffle dataset for a random sample, as we will not be using the entire dataset\n",
        "rng = np.random.default_rng(0) # This sets the random seed, meaning that we will get the SAME random sample if we rerun this cell.\n",
        "df = XY_pd.iloc[rng.permutation(len(XY_pd))].reset_index(drop=True) # Shuffle data\n",
        "\n",
        "# prepare NumPy ndarrays\n",
        "X_raw = np.array(df[['MedInc']])\n",
        "y_raw = np.array(df['MedHouseVal'])\n",
        "\n",
        "# Define how much data we want. You can try changing these later to see how it changes the model and predictions\n",
        "n = 50 # This is the data that will be used to train our model.\n",
        "\n",
        "# Split the feature data into our sample\n",
        "X = X_raw[:n] # This will select the data points from 0 to the number defined in n (200 in this case)\n",
        "\n",
        "# Split the target data into our sample\n",
        "y = y_raw[:n]\n",
        "\n",
        "degree = 9 # Define the maximum power of polynomial features you want to include\n",
        "poly = PolynomialFeatures(degree) # Create the polynomial features object\n",
        "X_poly = poly.fit_transform(X) # Fit the poly object to the training data to make a new feature matrix\n",
        "\n",
        "reg = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
        "reg.fit(X_poly, y)\n",
        "\n",
        "y_pred = reg.predict(X_poly) # Store our predictions\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X[:,0], y,  color='black', label='y_true') # Observed y values\n",
        "plt.scatter(X[:,0], y_pred, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('House Price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# The mean squared error loss\n",
        "print('Mean squared error loss: {:.4f}'.format(sklearn.metrics.mean_squared_error(y, y_pred)))\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wbzi1x8lazA"
      },
      "source": [
        "This is looking pretty good, with 72% accuracy. But with a single variable, we can only ever have a single line representing the relationships, which limits what can be done. If we make the model multi-variate we should be able to go even higher!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ_b9OTNjbjX"
      },
      "outputs": [],
      "source": [
        "# Load the house price dataset\n",
        "X_pd, y_pd = sklearn.datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Combine the X and y for visualisation\n",
        "XY_pd = pd.concat([X_pd, y_pd], axis=1)\n",
        "\n",
        "# Shuffle dataset for a random sample, as we will not be using the entire dataset\n",
        "rng = np.random.default_rng(0) # This sets the random seed, meaning that we will get the SAME random sample if we rerun this cell.\n",
        "df = XY_pd.iloc[rng.permutation(len(XY_pd))].reset_index(drop=True) # Shuffle data\n",
        "\n",
        "col = list(X_pd) # prepare NumPy ndarrays\n",
        "\n",
        "X_raw = np.array(df[col]) # !!!!!! Make sure you put any additional features into this array !!!!!!!!!!\n",
        "y_raw = np.array(df['MedHouseVal'])\n",
        "\n",
        "# Define how much data we want. You can try changing these later to see how it changes the model and predictions\n",
        "n = 50 # This is the data that will be used to train our model.\n",
        "\n",
        "# Split the feature data into our sample\n",
        "X = X_raw[:n] # This will select the data points from 0 to the number defined in n (200 in this case)\n",
        "\n",
        "# Split the target data into our sample\n",
        "y = y_raw[:n]\n",
        "\n",
        "degree = 9 # Define the maximum power of polynomial features you want to include\n",
        "poly = PolynomialFeatures(degree) # Create the polynomial features object\n",
        "X_poly = poly.fit_transform(X) # Fit the poly object to the training data to make a new feature matrix\n",
        "\n",
        "reg = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
        "reg.fit(X_poly, y)\n",
        "\n",
        "y_pred = reg.predict(X_poly) # Store our predictions\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X[:,0], y,  color='black', label='y_true') # Observed y values\n",
        "plt.scatter(X[:,0], y_pred, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('House Price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The mean squared error loss\n",
        "print('Mean squared error loss: {:.4f}'.format(sklearn.metrics.mean_squared_error(y, y_pred)))\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXasK1uR2E2j"
      },
      "source": [
        "## 3.7 **Important** - Polynomial feature drawbacks (Overfitting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n37UOEnS2Rzj"
      },
      "source": [
        "So, this looks pretty great right? Are polynomial features the key to all our problems? I was able to get a model with 100% accuracy!\n",
        "\n",
        "But remember, we have been using the **training data** to evaluate our accuracy for the last few exercises. What happens if we try and test the model above against **new data**, that the model hasn't seen yet?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfITr03626z8"
      },
      "outputs": [],
      "source": [
        "n_new = 20\n",
        "X_new = X_raw[n:n+n_new] # Pick our new data points\n",
        "X_new_poly = poly.fit_transform(X_new) # create Polynomial features for X_new\n",
        "y_true = y_raw[n:n+n_new] # Pick our new target values\n",
        "y_pred = reg.predict(X_new_poly) # Store our predictions\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_new[:,0], y_true,  color='black', label='y_true') # Observed y values\n",
        "plt.scatter(X_new[:,0], y_pred, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('House Price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The mean squared error loss\n",
        "print('Mean squared error loss: {:.4f}'.format(sklearn.metrics.mean_squared_error(y_true, y_pred)))\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y_true, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNlmqs_c3yE6"
      },
      "source": [
        "Well, that's disappointing. Our model is terrible. Really terrible. We would be much better off with univariate linear regression.\n",
        "\n",
        "We have **overfit** our data - our model has been fine tuned to perfectly pick up all of the noise in the training data, which gets impressive $R^2$ scores when making predictions for the training data, but means that our model utterly fails to make accurate predictions for new data.\n",
        "\n",
        "Do not despair, while there is no silver bullet, there are ways to make use of the power of polynomial features and multivariate models without overfitting our model. We will be exploring the methods for accurately validating your model and to prevent it from overfitting the data next week.\n",
        "\n",
        "Just remember:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fsXaiorwlPF"
      },
      "source": [
        "<font color=\"red\">A high degree of polynomial features can lower the the MSE greatly against the training data. However, a low error on the training data is not always desirable, since this does not tell us anything about the performance when making predictions using new data.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKfFh-GGmnug"
      },
      "source": [
        "# 4. Data Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoR07OmTms1e"
      },
      "source": [
        "So far in the tutorials we have exclusively been using a single set of data to fit and evaluate our models. As has been alluded to a few times so far (and should have been explained in your lecture) this is terrible practice!\n",
        "\n",
        "Thee are many ways you can get a model to fit data more and more accurately, but if this is just fitting random variation ('noise') in the data rather than the true relationships (the 'signal') then the model will be **useless** in practice, regardless of how accurate the $R^2$ score may be on the data it was fit on.\n",
        "\n",
        "So, how do we make sure our model is behaving well in reality?\n",
        "We split our data into two sets, and hold back a small set that the model never sees during training for evaluation purposes. This mimics the scenario from reality, where your model will have to make predictions from unseen data. However, unlike this scenario in production, with this held back 'test' data, we still have target data we can use to obtain a (more accurate) evaluation of our predictions.\n",
        "\n",
        "From this point forward you should always be using proper data splitting with your ML models and you should always report your model accuracy on unseen 'test' data to give the most reliable indication of the models performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doqfNqCPoTmv"
      },
      "source": [
        "##4.1 Data Splitting Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9RDVlHsoahu"
      },
      "source": [
        "To start with we want to load our data (using California housing again!) in as normal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TwyJ5qlodud"
      },
      "outputs": [],
      "source": [
        "X_pd, y_pd = sklearn.datasets.fetch_california_housing(return_X_y=True, as_frame=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eNdyUlcpO2G"
      },
      "source": [
        "But then we can use a new library from `sklearn` called `train_test_split`, which actually simplifies some things for us compared to what we were doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TigDy7PMpNFX"
      },
      "outputs": [],
      "source": [
        "# First import new library from sklearn\n",
        "from sklearn.model_selection import train_test_split # A library that can automatically perform data splitting for us\n",
        "\n",
        "# prepare NumPy ndarrays\n",
        "col = ['MedInc', 'HouseAge']\n",
        "X_raw = np.array(X_pd[col])\n",
        "y = y_pd.to_numpy()\n",
        "\n",
        "# Split the data into training/test data\n",
        "# We have 20640 pairs of a feature and target, we use 20% only for the test, with the remainder in the training set.\n",
        "# this can shuffle our data for us while we split using shuffle=True, and the random_state allows us to perform an identical pseudorandom shuffle each time.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_raw, y, test_size=0.20, shuffle=True, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzb2pX8Bqei2"
      },
      "source": [
        "Let's just have a sanity test that the arrays are the size we expect:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEmmmFSRqRTY"
      },
      "outputs": [],
      "source": [
        "print('X_train size:', X_train.shape)\n",
        "print('X_test size:', X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_MOhvaGqmzl"
      },
      "source": [
        "However, we can also use this method to split with explicit numbers if we don't want to use the entire dataset (which can be useful for big data cases, or just for testing models quickly), like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73qNbnMWqxN1"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_raw, y, train_size=500, test_size=100, shuffle=True, random_state=0)\n",
        "print('X_train size:', X_train.shape)\n",
        "print('X_test size:', X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-9vsmbOtaA-"
      },
      "source": [
        "We can create the model object and fit is as normal - but remember this time we are only fitting the model ont the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSVoeo08tokV"
      },
      "outputs": [],
      "source": [
        "# Create linear regression object\n",
        "obj = sklearn.linear_model.LinearRegression(fit_intercept=True)\n",
        "# Train the model using the training data\n",
        "obj.fit(X_train , y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1chaPL1trE9"
      },
      "source": [
        "We can evaluate this training model as normal if we like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_qn5HYmtxWl"
      },
      "outputs": [],
      "source": [
        "# Make predictions using the training data\n",
        "y_pred = obj.predict(X_train)\n",
        "\n",
        "print('Training evaluation:')\n",
        "# The mean squared error loss\n",
        "print('Mean squared error loss: {:.4f}'.format(sklearn.metrics.mean_squared_error(y_train, y_pred)))\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y_train, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SrbXW5Wt8St"
      },
      "source": [
        "But what we are really interested in is the performance on the new 'test' data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDbKL6VRrQj3"
      },
      "outputs": [],
      "source": [
        "# Make predictions using the test data\n",
        "y_pred = obj.predict(X_test)\n",
        "\n",
        "X_disp = X_test[:,0] # We will need to make a special vector for the feature we want on the x axis, as now X is a matrix matplotlib can't use it for a scatter plot\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_disp, y_test,  color='black', label='y_true') # Observed y values\n",
        "plt.scatter(X_disp, y_pred, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('House Price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print('Test data evaluation:')\n",
        "# The mean squared error loss\n",
        "print('Mean squared error loss: {:.4f}'.format(sklearn.metrics.mean_squared_error(y_test, y_pred)))\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzYAl9az9xG9"
      },
      "source": [
        "And just for interest, I've made a new 3D plot showing the relationship between house age, the median income and median house price. Black dots are the training data, blue the predicted values and red the observed 'test' values.\n",
        "\n",
        "It's a bit of a mess to look at with the whole dataset, so you may want to try rerunning the above with a smaller sample and mess around with the 'view' parameters `elev` and `azim` to move the plot in 3D space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWZIAauS7PKN"
      },
      "outputs": [],
      "source": [
        "# Creating dataset\n",
        "x_ax = np.array([[0],[1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11],[12],[13],[14],[15]], dtype='f') # X value range\n",
        "y_ax = np.array([[0,5,10,15,20,25,30,35,40,45,50,55,60,65,70]], dtype='f') # y value range\n",
        "z = obj.intercept_+obj.coef_[0]*x_ax+obj.coef_[1]*y_ax # this is the function for the 2D regression surface, note that I am using the coefficients of our fitted model object\n",
        "\n",
        "# Creating figure\n",
        "fig = plt.figure(figsize =(10, 10))\n",
        "ax = plt.axes(projection ='3d')\n",
        "\n",
        "# Creating plot\n",
        "ax.plot_surface(x_ax, y_ax, z, cmap=\"plasma\", alpha=0.5) # Our regression surface\n",
        "\n",
        "ax.scatter(X_train[:,0],X_train[:,1],y_train, c=\"black\",s=50) # Training Data\n",
        "ax.scatter(X_test[:,0],X_test[:,1],y_test, c=\"red\",s=50) # True Values\n",
        "ax.scatter(X_test[:,0],X_test[:,1],y_pred, c=\"blue\",s=50) # Predicted Values\n",
        "\n",
        "ax.view_init(elev=0, azim=250) # Change these variables to get a different view of the 3D plot\n",
        "\n",
        "ax.set_xlabel(\"Median Income\")\n",
        "ax.set_ylabel(\"House Age\")\n",
        "ax.set_zlabel(\"House Price\")\n",
        "\n",
        "# show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGV77Ws-uS6G"
      },
      "source": [
        "In this case our training model wasn't too bad, only having a minor change in accuracy. But as we saw with our polynomial example that isn't always the case!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g72rSN22-tsF"
      },
      "source": [
        "## 4.2 Exercise 4\n",
        "Try making a multi-variable regression model for the California housing dataset that uses all of the features and correct data splitting. How does the accuracy compare with the training data only prediction (Section 2.2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiagXoCa_VlE"
      },
      "outputs": [],
      "source": [
        "##############################################################\n",
        "# Your code here\n",
        "##############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMyl9Td6y8Et"
      },
      "source": [
        "##4.2 Exercise 5\n",
        "\n",
        "Try fitting some models for the diabetes dataset we used last week (`sklearn.datasets.load_diabetes`) using proper data splitting for the following models\n",
        "\n",
        "\n",
        "*   Univariate Regression\n",
        "*   Multivariate Regression\n",
        "*   Non-parametric Regression Models ([K-nearest Neighbour Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor), [Decision Tree Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor))\n",
        "\n",
        "Consider how your results compare with your results for similar problems in  [Tutorial 2](https://colab.research.google.com/drive/1gpTWdTOjxP53WUycdjIjr5rA2jlVgVOH?usp=sharing) where you used the training data only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcHYbnul2ENl"
      },
      "outputs": [],
      "source": [
        "##############################################################\n",
        "# Your code here\n",
        "##############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJbkUHqa51aC"
      },
      "source": [
        "##3.8 Exercise 6\n",
        "Try and implement a multivariate model with polynomial features on the California housing dataset that provides a good prediction but does not overfit the new data. You might also want to try changing some of the other parameters for the [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html?highlight=polynomial#sklearn.preprocessing.PolynomialFeatures) object. In particular using `interaction_only=true` will only give you only polynomial interactions (so $AB$ will be included but not $A^2$ or $B^2$).This may take a lot of experimentation with features and polynomial degrees to get anything better than just the baseline multivariate model. Don't worry next week we will see some methods that will automate this process for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjnLyHDqqBm1"
      },
      "outputs": [],
      "source": [
        "##############################################################\n",
        "# Your code here\n",
        "##############################################################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "COMP1801-ML(GPU)",
      "language": "python",
      "name": "comp1801-ml"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
