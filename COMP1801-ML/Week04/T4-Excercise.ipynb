{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH-43DBzVZgm"
      },
      "source": [
        "# COMP1801 Tutorial Week 4 - More Complicated Models, Overfitting and Regularisation\n",
        "\n",
        "*Dr Peter Soar - 2024/25*\n",
        "\n",
        "This week we will be expanding on the concept of overfitting we were introduced to last week, exploring how we can mitigate this with data splitting, hyperparameter selection, regularisation and finally revising the general model selection process we should be following when performing supervised learning. But first we will start with a quick look at how to deal with categorical features in our data.\n",
        "\n",
        "For this tutorial, read through and try to understand the text and code examples I have provided (ask your tutor if you have any questions) and there will be a selection of exercises. Attempt these exercises on your own, but do ask your tutor for help if you get stuck."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSo3PZoZeF3M"
      },
      "source": [
        "##0. Do not forget to import all the Python Libraries being used!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIhPMWKy2ML2"
      },
      "outputs": [],
      "source": [
        "import numpy as np # A useful package for dealing with mathematical processes, we will be using it this week for vectors and matrices\n",
        "import pandas as pd # A common package for viewing tabular data\n",
        "import sklearn.linear_model, sklearn.datasets # We want to be able to access the sklearn datasets again, also we are using some model evaluation\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures, LabelEncoder, OneHotEncoder # We will be using the inbuilt preprocessing functions sklearn provides\n",
        "import matplotlib.pyplot as plt # We will be using Matplotlib for our graphs\n",
        "from sklearn.model_selection import train_test_split # A library that can automatically perform data splitting for us\n",
        "from sklearn.linear_model import Ridge, Lasso, LinearRegression # Ridge & Lasso regression are types of linear model that use regularisation\n",
        "from sklearn.metrics import mean_squared_error, r2_score # Allows us to use the MSE function without calling in sklearn each time\n",
        "from google.colab import files # We will be importing a csv file I have provided for one section.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # suppresses a convergence warning we may get when testing Lasso - this is just cosmetic to stop too much output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UngIdVhz8C0"
      },
      "source": [
        "# 1. Categorical feature data\n",
        "\n",
        "So far we have only been considering feature columns that have been populated with numerical data, however it is not unusual to encounter datasets where the feature columns may include text values that have some meaning for the target predictions, however these must be converted into a numerical form somehow in order to be used by the machine learning algorithms.\n",
        "\n",
        "We would call these categorical feature data, and converting these features into numerical values (often called 'feature encoding') is another data preprocessing operation you should be aware of when using tabular data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L5pOUpk20lG"
      },
      "source": [
        "##1.1 Loading in dataset\n",
        "\n",
        "For this problem, I have created a synthetic dataset which I have uploaded to moodle. Please load this csv below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN2OaTigZOGZ"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload()    # Will prompt you to select file\n",
        "# Your file is now in the Colab filesystem on the left\n",
        "testData = pd.read_csv('testData.csv') # Save it to a pandas dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojdvKcwZzfI_"
      },
      "source": [
        "This data shows the final grade of a selection of students from 4 different schools. We have the scored from their previous two mock papers, but there are also a number of categorical feature columns that we can't currently use.\n",
        "\n",
        "Let's have a look at the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTjHI9buZv6m"
      },
      "outputs": [],
      "source": [
        "# This file has all the data elements constructed already, so we can just put the entire thing into the 'data='\n",
        "df = pd.DataFrame(data= testData)\n",
        "\n",
        "# show the data\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ybP8LLd0MF5"
      },
      "source": [
        "First, let's see how good a fit we get just using the mock scores, so data that is numerical already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLZgOolI0IlD"
      },
      "outputs": [],
      "source": [
        "col_fin = ['mock1','mock2']\n",
        "tar = ['finalGrade']\n",
        "X = np.array(testData[col_fin])\n",
        "y = np.array(testData[tar])\n",
        "\n",
        "# Define how much test and training data we want. You can try changing these later to see how it changes the model and predictions\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True, random_state=0)\n",
        "\n",
        "# Create linear regression object\n",
        "obj = sklearn.linear_model.LinearRegression(fit_intercept=True)\n",
        "\n",
        "# Train the model using the training sets\n",
        "obj.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "y_pred = obj.predict(X_test)\n",
        "\n",
        "X_test_disp = X_test[:,0] # We will need to make a special vector for the feature we want on the x axis, as now X is a matrix matplotlib can't use it for a scatter plot\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test_disp, y_test,  color='black', label='y_test') # Observed y values\n",
        "plt.scatter(X_test_disp, y_pred, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('Mock 1')\n",
        "plt.ylabel('Final Grade')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The mean squared error loss\n",
        "print('Mean squared error loss: {:.4f}'.format(sklearn.metrics.mean_squared_error(y_test, y_pred)))\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6NUbOq30JFU"
      },
      "source": [
        "Okay, so around 83% $R^2$ score is a pretty strong baseline, but the categorical columns may be the key to explaining the remaining variation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj30Uk_C1CGx"
      },
      "source": [
        "## 1.2 Label encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaTj6pw11nh_"
      },
      "source": [
        "\n",
        "Given the original dataset, it is clear we have many categorical features. All these need to be encoded. The [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) class is used to transform the categorical or string values to numerical ones (between 0 and n_classes-1).\n",
        "\n",
        "In this case we will see what an impact the school has on peoples final grades, as there are 4 schools we should expect this column to be filled with values from '0-3'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN55GrDX0jzO"
      },
      "outputs": [],
      "source": [
        "testData_school = testData.copy() # We will be making copies of the dataset as we will be making changes to the columns as we go along\n",
        "\n",
        "# create an object of the LabelEncoder class\n",
        "lblEncoder_X = LabelEncoder()\n",
        "# apply LblEncoder object to our categorical variables (columns - 'school') using the fit_transform method. This returns the column encoded.\n",
        "testData_school['school'] = lblEncoder_X.fit_transform(testData_school['school'])\n",
        "\n",
        "print(testData_school) # See that the 'school' column is now filled with integer values matching the 4 schools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms_w_ZFC2yeO"
      },
      "source": [
        "Now let's try including this new numerical feature in our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmKyjM9z_u8c"
      },
      "outputs": [],
      "source": [
        "tar='finalGrade'\n",
        "col_fin=['mock1','mock2','school']\n",
        "\n",
        "X = np.array(testData_school[col_fin])\n",
        "y = np.array(testData_school[tar])\n",
        "\n",
        "# Define how much test and training data we want. You can try changing these later to see how it changes the model and predictions\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True, random_state=0)\n",
        "\n",
        "# Create linear regression object\n",
        "obj = sklearn.linear_model.LinearRegression(fit_intercept=True)\n",
        "\n",
        "# Train the model using the training sets\n",
        "obj.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "y_pred = obj.predict(X_test)\n",
        "\n",
        "X_test_disp = X_test[:,0] # We will need to make a special vector for the feature we want on the x axis, as now X is a matrix matplotlib can't use it for a scatter plot\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test_disp, y_test,  color='black', label='y_test') # Observed y values\n",
        "plt.scatter(X_test_disp, y_pred, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('Mock 1')\n",
        "plt.ylabel('Final Grade')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The mean squared error loss\n",
        "print('Mean squared error loss: {:.4f}'.format(sklearn.metrics.mean_squared_error(y_test, y_pred)))\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCqQ6HKF3RCX"
      },
      "source": [
        "The score has improved, but not by much in this case - but there are still lots of other categorical features we haven't used yet. Let's try looking at the impact of the study time - surely that will have an impact on how well they perform on the test!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYj8KGz9I7s_"
      },
      "outputs": [],
      "source": [
        "testData_studyTime = testData.copy() # We will be making copies of the dataset as we will be making changes to the columns as we go along\n",
        "\n",
        "# create an object of the LabelEncoder class\n",
        "lblEncoder_X = LabelEncoder()\n",
        "# apply LblEncoder object to our categorical variables (columns - 'school') using the fit_transform method. This returns the column encoded.\n",
        "testData_studyTime['studyTime_enc'] = lblEncoder_X.fit_transform(testData_studyTime['studyTime'])\n",
        "\n",
        "display(testData_studyTime)\n",
        "\n",
        "tar='finalGrade'\n",
        "\n",
        "col_fin=['mock1','mock2','studyTime_enc']\n",
        "\n",
        "\n",
        "X = np.array(testData_studyTime[col_fin])\n",
        "y = np.array(testData_studyTime[tar])\n",
        "\n",
        "# Define how much test and training data we want. You can try changing these later to see how it changes the model and predictions\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True, random_state=0)\n",
        "\n",
        "# Create linear regression object\n",
        "obj = sklearn.linear_model.LinearRegression(fit_intercept=True)\n",
        "\n",
        "# Train the model using the training sets\n",
        "obj.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "y_pred = obj.predict(X_test)\n",
        "\n",
        "X_test_disp = X_test[:,0] # We will need to make a special vector for the feature we want on the x axis, as now X is a matrix matplotlib can't use it for a scatter plot\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test_disp, y_test,  color='black', label='y_test') # Observed y values\n",
        "plt.scatter(X_test_disp, y_pred, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('Mock 1')\n",
        "plt.ylabel('Final Grade')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The mean squared error loss\n",
        "print('Mean squared error loss: {:.4f}'.format(sklearn.metrics.mean_squared_error(y_test, y_pred)))\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA4xGtBv7cxp"
      },
      "source": [
        "Except it turns out that this adds even less accuracy than the 'school', with no significant improvement from just using the mocks.\n",
        "\n",
        "**Question:** Can you work out what has gone wrong to cause this feature to have no impact on the accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcHM9s21zzkH"
      },
      "source": [
        "##### **Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUUTdq-GZMBA"
      },
      "source": [
        "\n",
        "\n",
        "It's because of the ordinal nature being imposed by the label encoding. Now intuitively you would think this is the case where the label coding is appropriate - surely more hours studying is better than less?\n",
        "I would agree, however if you interrogate the table displayed above, you will notice that the label encoder hasn't assigned values that increment correctly with the increase in time (0: 0-60, 1: 121-240, 2: 241-480, 3:61-120), as the label encoder isn't able to discern the order from the string and just applies the number in the order it first sees them in the dataset, which ruins any pattern that might be in the feature and means in fitting the model this feature is effectively ignored.\n",
        "\n",
        "This highlights a weakness in label encoding, which could be mitigated by manually converting the columns into representative ordinal numbers, or by employing a different encoding strategy - such as one-hot encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-fzznYY7h2C"
      },
      "source": [
        "##1.3 One Hot Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM6SNSY476pD"
      },
      "source": [
        "The final encoding strategy I'll talk about today is [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder). For each feature, this expands the columns in the dataset to have one column per category using a 0/1 indicator to mark which category corresponds to each data point.\n",
        "\n",
        "There is no benefit to using OHE for categorical features with only two categories (as you will just end up with two columns where the 1's and 0's are inverted, which adds nothing to the model). So for binary categories always just use label encoding.\n",
        "\n",
        "Let's try using the `studyTime` feature again to see how different a solution we get when comparing to the label encoding.\n",
        "\n",
        "If you are going to be using OHE on a larger scale for your data, I would advise setting up a pipeline to streamline things, as in this example I have just manually added all of the encoded columns to our dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DYSrJwxamMd"
      },
      "outputs": [],
      "source": [
        "# create dataset copy for testing purposes\n",
        "testDataOHE = testData.copy()\n",
        "\n",
        "# create OneHotEncoder object\n",
        "oneHotEncoder = OneHotEncoder()\n",
        "# and fit the OneHotEncoder object to feature studyTime\n",
        "onehot_enc = oneHotEncoder.fit(testDataOHE[['studyTime']])\n",
        "print('The categories are: ', onehot_enc.categories_)\n",
        "\n",
        "# dum is an array of shape (391,2) containing the one-hot encoding of the feature Gender of the dataframe train_dataset_no_nans\n",
        "# we make a temporary object to be able to manipulate the extra number of columns\n",
        "OHT = onehot_enc.transform(testDataOHE[['studyTime']]).toarray()\n",
        "\n",
        "# we add to the encoded columns to our dataset\n",
        "testDataOHE['studyTime_0'] = OHT[:,0]\n",
        "testDataOHE['studyTime_1'] = OHT[:,1]\n",
        "testDataOHE['studyTime_2'] = OHT[:,2]\n",
        "testDataOHE['studyTime_3'] = OHT[:,3]\n",
        "\n",
        "# chack that the new columns correspond to the correct time range (note, they may not be split in order, you can see the order they were split in the print statement above)\n",
        "display(testDataOHE)\n",
        "\n",
        "tar='finalGrade'\n",
        "\n",
        "col_fin=['mock1','mock2','studyTime_0','studyTime_1','studyTime_2','studyTime_3']\n",
        "\n",
        "X = np.array(testDataOHE[col_fin])\n",
        "y = np.array(testDataOHE[tar])\n",
        "\n",
        "# Define how much test and training data we want. You can try changing these later to see how it changes the model and predictions\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True, random_state=0)\n",
        "\n",
        "# Create linear regression object\n",
        "obj = sklearn.linear_model.LinearRegression(fit_intercept=True)\n",
        "\n",
        "# Train the model using the training sets\n",
        "obj.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "y_pred = obj.predict(X_test)\n",
        "\n",
        "X_test_disp = X_test[:,0] # We will need to make a special vector for the feature we want on the x axis, as now X is a matrix matplotlib can't use it for a scatter plot\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test_disp, y_test,  color='black', label='y_test') # Observed y values\n",
        "plt.scatter(X_test_disp, y_pred, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('Mock 1')\n",
        "plt.ylabel('Final Grade')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The mean squared error loss\n",
        "print('Mean squared error loss: {:.4f}'.format(sklearn.metrics.mean_squared_error(y_test, y_pred)))\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QymftQLO91sj"
      },
      "source": [
        "Okay, so in this case our one-hot encoded features slightly outperform taking the range average, but it is basically the same accuracy wise.\n",
        "\n",
        "As is often the case in ML, which method will give you the best results when dealing with categorical data will vary based on your dataset, so it is always worth experimenting with different encoding methods for your categorical data when testing your initial models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWqFJZlt_ITv"
      },
      "source": [
        "##1.4 Exercise 1\n",
        "\n",
        "So far we have only looked at two of our categorical features.\n",
        "Using all of the data, see how accurate a prediction of the final grade you can obtain (I was able to get around 93% $R^2$, but maybe you can do better!)\n",
        "\n",
        "Some things to try:\n",
        "\n",
        "*   See how performance changes with different encoding methods for each categorical feature (Label Encoding, One-hot encoding, taking the range average).\n",
        "*   You can use different encoding methods for different features if appropriate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69guJJ1WHgTN"
      },
      "outputs": [],
      "source": [
        "##############################################################\n",
        "# Your code here\n",
        "##############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtFuNVNNez-e"
      },
      "source": [
        "#2. Data preprocessing and the full ML pipline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDVUME-H28hh"
      },
      "source": [
        "## 2.1 Data Pre-processing\n",
        "As described in the lecture, feature scaling has multiple benefits for our ML implementation, including:\n",
        "- Evening out the sensitivity of the different parameters to changes\n",
        "- Making the model more interpretable\n",
        "- Improving regularisation behaviour\n",
        "- Improving optimisation behaviour (explained in later weeks)\n",
        "\n",
        "The two main methods we are currently considering are Standardisation and Normalisation (MinMax Scaling), both of which are easy to apply using sklearn.\n",
        "\n",
        "**Standardization**\n",
        "\n",
        "By calculating the mean and variance for our data columns we can scale each feature so that it has a mean of 0 and a variance of 1.\n",
        "$$x_s = \\frac{x-\\mu_x}{\\sigma_x}$$\n",
        "We can standardize the data by the `sklearn.preprocessing.StandardScaler` instance.\n",
        "- Initialization: `StandardScaler` initializer.\n",
        "- Calculating the mean and variance (standard deviation) by using `fit` with our object and data.\n",
        "  - Input: the `np.array` instance of which to calculate the mean and variance.\n",
        "- Apply the standardization using `transform`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWUmxKYu1p4L"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "# calculate the mean and variance for each feature and store to attributes\n",
        "scaler.fit(X_train[:,0:2]) # We only want to perform feature scaling on the numeric (so not categorical) features\n",
        "print(f'Raw mean = \\n{scaler.mean_}, \\nRaw var = \\n{scaler.var_}') # print the calculated mean and variance for each attribute\n",
        "X_train_stded = scaler.transform(X_train[:,0:2]) # standardize X_train\n",
        "# verify that X_train_stded has mean 0 (mean isn't quite 0 due to numerical error, but is a miniscule value) and variance 1.\n",
        "print(f'Standardised mean = \\n{np.mean(X_train_stded, axis=0)}, \\nStandardised var = \\n{np.var(X_train_stded, axis=0)}')\n",
        "\n",
        "# Remember to rejoin our scaled features with our categorical features in our final feature matrix\n",
        "X_train_stded = np.c_[X_train_stded, X_train[:,2:]]\n",
        "print(X_train_stded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i41hSXU5S2VF"
      },
      "source": [
        "**Normalisation (MinMax)**\n",
        "\n",
        "By calculating the maximum and minimum values for our data columns we can scale each feature so that it takes a value between 0 and 1.\n",
        "$$x_s = \\frac{x-x_{min}}{x_{max}-x_{min}}$$\n",
        "We can standardize the data by the `sklearn.preprocessing.MinMaxScaler` instance.\n",
        "- Initialization: `MinMaxScaler` initializer.\n",
        "- Calculating the min and max of the feature by using `fit` with our object and data.\n",
        "  - Input: the `np.array` instance of which to calculate the mean and variance.\n",
        "- Apply the standardization using `transform`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M54eItLiS7zb"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "# calculate the mean and variance for each feature and store to attributes\n",
        "scaler.fit(X_train[:,0:2]) # We only want to perform feature scaling on the numeric (so not categorical) features\n",
        "print(f'Raw min = \\n{np.min(X_train[:,0:2], axis=0)}, \\nRaw max = \\n{np.max(X_train[:,0:2], axis=0)}') # print the calculated mina and max for each attribute\n",
        "X_train_stded = scaler.transform(X_train[:,0:2]) # standardize X_train\n",
        "# verify that X_train_stded has min 0 and max 1.\n",
        "print(f'Normalised Min = \\n{np.min(X_train_stded, axis=0)}, \\nNormalised Max = \\n{np.max(X_train_stded, axis=0)}')\n",
        "\n",
        "# Remember to rejoin our scaled features with our categorical features in our final feature matrix\n",
        "X_train_stded = np.c_[X_train_stded, X_train[:,2:]]\n",
        "print(X_train_stded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJNVVhQ5S787"
      },
      "source": [
        "For most of our case we will apply standardisation, which is generally a better choice for unbounded data like we are using. However MinMax scaling is a valid approach that can be useful when we have a bounded numerical column (for example if we know all values are in a range of 0-100). It is possible that you may want to perform different feature scaling operations on different features depending on the type of data being stored.\n",
        "\n",
        "Note: Remember that we should generally not perform feature scaling on our categorical features. Though this can cause practical difficulties when implementing polynomial interactions between categorical and numerical columns or big data implementation with many columns of different data types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPFT9n8PY2RP"
      },
      "source": [
        "##2.2 Data Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8WYCjU-Y7xq"
      },
      "source": [
        "So far in our models we have been splitting our data into two sections - training data and test data. However, until this week I have not really explained the rationale behind this in detail, and in fact splitting our data into three groups is generally preferable for a robust ML pipeline.\n",
        "\n",
        "To begin with, let us load in our California Housing data (staying univariate for now to keep things simple) and convert it to arrays as we usually do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1LKa06BZsEf"
      },
      "outputs": [],
      "source": [
        "# Load the house price dataset\n",
        "X_pd, y_pd = sklearn.datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "Xy_df = pd.concat([X_pd, y_pd], axis=1)\n",
        "\n",
        "# Use only one feature\n",
        "col = 'MedInc'\n",
        "Xy_df = Xy_df[['MedInc', 'MedHouseVal']]\n",
        "\n",
        "# show the data\n",
        "display(Xy_df)\n",
        "\n",
        "# Convert the data to `np.array`\n",
        "X_raw = np.array(X_pd[['MedInc']])\n",
        "y = np.array(y_pd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p88isL1YG0L"
      },
      "source": [
        "As we have been exploring over the last few weeks, when optimising parameters, selecting a model, and tuning hyperparameters, **overfitting** happens.\n",
        "Hence, we need a set of data for optimising our model parameters (training) and an additional set (validation) where we compare the performance of multiple models and hyperparameters to help us decide what our optimal Machine Learning model is for a given dataset. However, we still need to perform a final evaluation of this model using a final set of data (testing) that hasn't been used in either of these prior steps. It may seem counter-intuitive, but even though the validation dataset is not used to train the model parameters directly, we can still end up overfitting the validation data to a degree as we pick the model and hyperparameters which perfoms the best in the validation stage.\n",
        "\n",
        "In brief - we split the feature/target pairs into the following three datasets:\n",
        "- **Training data** for optimising parameters.\n",
        "- **Validation data** for selecting a model and tuning hyperparameters.\n",
        "- **Test data** for evaluating the true performance of your best model(s)\n",
        "\n",
        "The exact proportion of training:validation:testing data does not follow exact rules, and may depend on how much data you have available (if you do not have much data, you may not want to use too much test and validate as you may be missing out on information from the features). I would say that as a rule of thumb anywhere between 60%:20%:20% and 80%:10%:10% is an easily justifiable split (ideally keeping the validation and test datasets both using the same proportion).\n",
        "\n",
        "Strictly speaking, once you split the test data from others, we can adopt a more complicated way to optimise parameters and select a model and tune hyperparameters. The above splitting strategy is one of the simplest way, but widely used. Another widely used example is [**cross validation**](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation), where we split the non-test data multiple times. There is more detail on this in Appendix 1, and while not required to do well in your coursework it is a powerful and widely used approach to consistent data splitting results that you may want to consider exploring.\n",
        "\n",
        "From now on we will split the data using two rounds of the `sklearn.model_selection.train_test_split` function. A reminder, we must ensure we use the correct data shuffle setting.\n",
        "- For non time series data, usually, **we should shuffle** the data by specifying **`shuffle=True`** to avoid the bias of the data by data index.\n",
        "- For time series data, we **must NOT shuffle** the data to chronologically split the data into training/validation/test data. Specifically, the training data must be earlier data than the other two, and the validation data must be earlier than the test data. In this way, we can simulate the real application, where we handle the latest data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrqNtUztYG0Q"
      },
      "outputs": [],
      "source": [
        "# Split the data into non-test/test data\n",
        "# While we have 20640 pairs of a feature and target, we use 20% only for the testing, not so we hold back the other 80% for training and validation\n",
        "# `shuffle=True` for non-time series case. You should set `shuffle=False` for time series data to avoid future data being contaminated in the training data.\n",
        "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X_raw, y, test_size=0.20, shuffle=True, random_state=0)\n",
        "\n",
        "# Split the non-test data into non-test/test data\n",
        "# We use 25% of the remaining non-test data only for the validation set, leaving the rest for training\n",
        "# In the end, the splitting ratio will be 6:2:2 for the training, validation, and test data.\n",
        "# `shuffle=True` for non-time series case. You should set `shuffle=False` for time series data to avoid future data being contaminated in the training data.\n",
        "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=0.25, shuffle=True, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOA6lncPfCik"
      },
      "source": [
        "Preprocessing reminder (e.g. polynomial features and standardisation/normalisation):\n",
        "\n",
        "Validation data and test data are both used to simulate new data we are using in the prediction stage. Hence, similar to what we learnt in previous weeks, machine learning prediction models require the features in the training data, validation data, test data (the data on which we predict) to all be preprocessed **the same way**.\n",
        "\n",
        "For example, we must use the mean and standard deviation **of the training data** for standardizing **both the training data and the new data**.\n",
        "If we fit a new preprocessing object on our new data then we would obtain different values for the mean an standard deviation such that after preprocessing the same value in the training data and new data would become different in both datasets after preprocessing, confusing our ML model and almost certainly leading to useless predictions.\n",
        "\n",
        "Hence, we fit the preprocessor to **the training data only**, and use this to transform **both the training data and the new data** (where by *new* data we mean our validation data, test data and any truly *new* data encountered when deploying our ML solution in the real world)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jktJ0CGHsKmz"
      },
      "source": [
        "## 2.3 Full ML pipline\n",
        "\n",
        "I have discussed this in the lecture, but below I would just like to repeat the full Machine Learning pipeline we should be following when fitting any supervised learning models for your easy reference (remember that this all also applies to classification models and even the Neural Network models we will be using in later weeks).\n",
        "\n",
        "Note: Ensure you understand the distinction between a `fit` command using the preprocessor (e.g., `sklearn.preprocessing.StandardScaler`) vs your predictive model (e.g., `sklearn.linear_model.LinearRegression`).\n",
        "\n",
        "General pipeline:\n",
        "- Data loading\n",
        "  - Ensure you perform and Exploratory Data Analysis (EDA) and clean your data before going any further when dealing with real data.\n",
        "  - Convert your data into arrays that your ML model can interface with.\n",
        "  - Split your data into the Train:Validation:Test sets.\n",
        "- Initialization\n",
        "  - Initialize the preprocessor(s) and the machine learning model objects.\n",
        "- Model Selection\n",
        "  - Identify the different solutions you want to try: this could involve different model types (e.g. Linear Regression, Lasso, Ridge, Decision tree, Neural Network, etc), preprocessing methods, feature selection/engineering and hyperparameter selection.\n",
        "  - Training (do this for **all** models and hyperparameters)\n",
        "    - **Fit the preprocessor(s)** to the raw training feature matrix.\n",
        "    - **Transform (Preprocess)** the raw training feature matrix into the preprocessed training feature matrix.\n",
        "    - **Fit (Train) the prediction model** using the data pair of the preprocessed training feature matrix and target column vector.\n",
        "    - Recording the performance of the prediction on training data is often also helpful for identifying overfitting.\n",
        "\n",
        "  - Validation (for **all** models and hyperparameters)\n",
        "    - **Transform (Preprocess)** the raw validation feature matrix into the preprocessed validation feature matrix using **the preprocessor(s)** (**Do NOT fit any preprocessor to the validation feature matrix**).\n",
        "    - **Predict** using the preprocessed validation feature matrix to get a predicted target column.\n",
        "    - **Evaluate** the prediction performance on the validation data by a loss and/or score function.\n",
        "\n",
        "  - Continue training and validation until you have a model (or selection of models) you feel are potentially good enough to offer a robust solution to the provided prediction problem.\n",
        "\n",
        "- Test (for the model and hyperparameters selected by the validation process)\n",
        "  - **Transform (Preprocess)** the raw test feature matrix into the preprocessed test feature matrix by **the preprocessor(s)** (**Do NOT fit any preprocessor to the test feature matrix**).\n",
        "  - **Predict** on the preprocessed test feature matrix to get a predicted target column.\n",
        "  - **Evaluate** the prediction performance on the test data by a loss and/or score function.\n",
        "\n",
        "If after the test step your model is not providing good enough solutions on the test data, then you will have to go back to rethink how you can create a more robust solution less prone to overfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImMjTk9IlPbk"
      },
      "source": [
        "#3. Hyperparameter selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPIg97e4lT2D"
      },
      "source": [
        "Choosing appropriate hyperparameters is one of the reasons we need to include the validation step.\n",
        "\n",
        "A hyperparameter are parameters that dictates how the model behaves, but are not directly related to any of our features in the data. The main hyperparameter we have encountered so far is the degree of our polynomial regression, but the regularisation weight I have talked about in the lecture (and we will see in the next section) is another example. When I talk to you about Neural Networks in a few weeks we will have even more of these - such as the learning rate, number of epochs and even the number of hidden layers.\n",
        "\n",
        "Systemically selecting hyperparmaters is very important to obtaining a good ML solution, and while we could always just try varying these by hand this is slow and you need to make sure you keep detailed records of the results. It is far more robust to set up a pipeline to test a range of values for us and provide us with the setup that gives us the lowest Mean Squared Error.\n",
        "\n",
        "Experimentation is key to obtaining the best results, so ensure you demonstrate you have setup a framework for selecting the best hyperparamters whenever you are making a model recommendation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI-gXVS7iGRp"
      },
      "source": [
        "##3.1 Hyperparameter Selection Example: Polynomial Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJjjpVAvoEQB"
      },
      "source": [
        "Reminder from last week:\n",
        "\n",
        "Using [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html?highlight=polynomial#sklearn.preprocessing.PolynomialFeatures) is another preprocessing step, and is very similar to the feature scaling in implementation.\n",
        "\n",
        "- Initialize the polynomial features object.\n",
        "  - Method: `PolynomialFeatures(degree)`\n",
        "  - Input parameter: Maximum polynomial degree you want for your model (this is really a hyperparameter we need to choose carefully, but we will cover how to best pick this degree next week).\n",
        "- Output: Creates a new feature matrix with polynomial features.\n",
        "  - Method: `poly.fit_transform`\n",
        "\n",
        "In this section I will show you how we can select the best degree of polynomial features for the California housing data we loaded in and split above.\n",
        "\n",
        "Note: The order of the preprocessing steps matters and will change the results - generally it is better to create the polynomial features first then apply standardisation, especially when we have multiple features (see Rule #2 at [this link](https://samchaaa.medium.com/preprocessing-why-you-should-generate-polynomial-features-first-before-standardizing-892b4326a91d)).\n",
        "\n",
        "First, let's just fit a basic model of a fixed degree to remind you of the full process (make sure you read the comments to understand what we are doing and how this fits into our ML pipeline):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzzi2H8LpE6I"
      },
      "outputs": [],
      "source": [
        "################\n",
        "# Initialisation\n",
        "################\n",
        "model = sklearn.linear_model.LinearRegression() # Create our prediction model object\n",
        "poly = PolynomialFeatures(degree=3) # Create our Polynomial Features object, here taking a fixed degree of 3\n",
        "scaler = StandardScaler() # Here is our standardisation object\n",
        "\n",
        "##########\n",
        "# Training\n",
        "##########\n",
        "# Create Polynomial features BEFORE standardisation\n",
        "poly.fit(X_train_raw) # Fit polynomial features on our raw training data\n",
        "X_train_poly = poly.transform(X_train_raw) # transform our raw data using the poly object\n",
        "\n",
        "scaler.fit(X_train_poly) # Fit the standardisation object on the polynomial feature matrix\n",
        "X_train = scaler.transform(X_train_poly) # Standardise the feature matrix to get our 'final' training data feature matrix\n",
        "\n",
        "model.fit(X_train, y_train) # fit our linear regression model - 'train' the model\n",
        "\n",
        "# Evaluate performance of the training data\n",
        "# This does NOT reflect the true performance of the model, but keeping track of the training data performance is helpful for checking how much the model is suffering from overfitting.\n",
        "y_pred_train = model.predict(X_train) # Use our fitted linear regression model to make a prediction\n",
        "mse_train = mean_squared_error(y_train, y_pred_train) # Calculate the Mean Squared Error for our training data predictions\n",
        "print('MSE on training data:', mse_train)\n",
        "\n",
        "############\n",
        "# Validation\n",
        "############\n",
        "\n",
        "# Preprocess validation dataset using preprocessor objects for on the training data\n",
        "# Do NOT refit the preprocessors\n",
        "# Create Polynomial features BEFORE standardisation\n",
        "X_valid_poly = poly.transform(X_valid_raw) # Add polynomial features to the validation data\n",
        "X_valid = scaler.transform(X_valid_poly) # Standardise the validation data\n",
        "\n",
        "# We select the best hyperparameters based on the performance of our model on the validation dataset\n",
        "y_pred_valid = model.predict(X_valid) # Use our fitted linear regression model to make a prediction based on the validation dataset\n",
        "mse_valid = mean_squared_error(y_valid, y_pred_valid)  # Calculate the Mean Squared Error for our validation data predictions\n",
        "print('MSE on validation data:', mse_valid)\n",
        "\n",
        "# Let's just plot our outputs to see how our model is looking\n",
        "plt.scatter(X_valid_raw, y_valid,  color='black', label='y_true') # Observed y values\n",
        "plt.scatter(X_valid_raw, y_pred_valid, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('house price')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhKNrSO9vSmR"
      },
      "source": [
        "Okay, so from this we've seen that a polynomial degree of 3 behaves similarly for both our training and validation data - but is this the best degrees to pick?\n",
        "\n",
        "Rather than varying this hyperparameter manually, we want to set up a framework to automate this for us.\n",
        "\n",
        "In the following we run a `for` loop incrementing the hyperparameter variable `degree`, applying polynomial regression and storing the MSE for the training and validation data in an array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auBdK_03wxvB"
      },
      "outputs": [],
      "source": [
        "max_degree = 30 #Define the max degree to test\n",
        "# Initialise the MSE arrays, filling them with NaN's\n",
        "mse_train_array = np.full([max_degree + 1], np.nan)\n",
        "mse_valid_array = np.full([max_degree + 1], np.nan)\n",
        "degrees = range(1, max_degree+1) # create list of degree values being iterated through\n",
        "\n",
        "for degree in degrees:\n",
        "\n",
        "  # Initialise\n",
        "  model = sklearn.linear_model.LinearRegression()\n",
        "  poly = PolynomialFeatures(degree=degree)\n",
        "  scaler = StandardScaler()\n",
        "  # Train\n",
        "  poly.fit(X_train_raw)\n",
        "  X_train_poly = poly.transform(X_train_raw)\n",
        "  scaler.fit(X_train_poly)\n",
        "  X_train = scaler.transform(X_train_poly)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred_train = model.predict(X_train)\n",
        "  mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "\n",
        "  # Validate\n",
        "  X_valid_poly = poly.transform(X_valid_raw)\n",
        "  X_valid = scaler.transform(X_valid_poly)\n",
        "  y_pred_valid = model.predict(X_valid)\n",
        "  mse_valid = mean_squared_error(y_valid, y_pred_valid)\n",
        "\n",
        "  # Store MSE for this degree value\n",
        "  mse_train_array[degree] = mse_train\n",
        "  mse_valid_array[degree] = mse_valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68-9YfNHxoyw"
      },
      "source": [
        "Run the cell below to visualise the learning curves from fitting all of these models. You can see that the MSE on the training data monotonously decreases as we use higher degree polynomials (i.e. when using more complex models). As we saw last week, the more complex a model is, the better it can fit training data.\n",
        "\n",
        "However, the MSE on the validation data starts to increase around `degree=9`  onwards. This shows the overfitting effect. At the beginning of the plot where we have a small `degree` and both the training MSE and validation MSE are high (around `degree = 0-6`) we are seeing underfitting.\n",
        "\n",
        "We now have the data we need to choose the best degree for our data, as we want to use whichever degree minimises the **validation** MSE, in this case taking `degree = 8`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXt7orLGxl3p"
      },
      "outputs": [],
      "source": [
        "plt.plot(degrees, mse_train_array[1:], label='MSE on training dataset')\n",
        "plt.plot(degrees, mse_valid_array[1:], label='MSE on validation dataset')\n",
        "plt.xlabel('Degree of polynomial features')\n",
        "plt.ylabel('Mean squared error')\n",
        "plt.title('Degree of polynomial features and mean squared error')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "best_degree = np.nanargmin(mse_valid_array) # Finds the smallest VALIDATION MSE in the array (ignoring any NaN values).\n",
        "print('The best degree of polynomials:', best_degree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IojLxDyR0RPS"
      },
      "source": [
        "Finally, now we have selected the best hyperparameter (degree) on the validation data we now think we have our best model for the data. However, it is possible that the model with the selected hyperparameter is actually overfitting the validation data. To check the true performance of our ML model, we must measure the MSE on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LL7fCbNy93f"
      },
      "outputs": [],
      "source": [
        "# Initialise the test instances\n",
        "model = sklearn.linear_model.LinearRegression()\n",
        "poly = PolynomialFeatures(degree=best_degree) # use the degree we selected above\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Remember we have to fit our preprocessors and model on the training data again, as we were using many other degrees in our parametric test above\n",
        "poly.fit(X_train_raw)\n",
        "X_train_poly = poly.transform(X_train_raw)\n",
        "scaler.fit(X_train_poly)\n",
        "X_train = scaler.transform(X_train_poly)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "mse_train = mean_squared_error(y_train, model.predict(X_train))\n",
        "print('MSE on the train data:', mse_train) # Get our MSE for the training Data for comparison\n",
        "\n",
        "\n",
        "X_valid_poly = poly.transform(X_valid_raw)\n",
        "X_valid = scaler.transform(X_valid_poly)\n",
        "mse_valid = mean_squared_error(y_valid, model.predict(X_valid))\n",
        "print('MSE on the valid data:', mse_valid) # Get our MSE for the validation Data for comparison\n",
        "\n",
        "# Now we have fit our preprocessors using the correct degree, we can process our test data and make a prdiction\n",
        "X_test_poly = poly.transform(X_test_raw)\n",
        "X_test = scaler.transform(X_test_poly)\n",
        "y_pred_test = model.predict(X_test)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "print('MSE on the test data:', mse_test) # check our testing MSE, is it signficantly different from our validation MSE?\n",
        "\n",
        "# Let's just plot our outputs to see how our model is looking\n",
        "plt.scatter(X_test_raw, y_test,  color='black', label='y_true') # Observed y values\n",
        "plt.scatter(X_test_raw, y_pred_test, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('house price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Finally, let's also get the R2 score for our final model\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y_test, y_pred_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyhg6sLK1neC"
      },
      "source": [
        "So, for this case we can be confident we are not overfitting the validation dataset, with a very similar MSE value to what we observed for `degree=8` during validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fk0lEX72Kci"
      },
      "source": [
        "##3.2 Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4hfOSsK7o4A"
      },
      "source": [
        "Use the pipeline outlined above for `degree` selection to find the best polynomial degree for multivariable model for the California housing dataset to see how accurate a final model you can obtain. What do you notice as you include more features and higher polynomial degrees?\n",
        "\n",
        "Note:\n",
        "*   Try to use the automated degree selection from above to avoid picking degree values manually like we did last week!\n",
        "*   Experiment with adding/removing features one at a time by hand, how does this change the result? (we will address removing this chore in the next section). What about changing the amount of training data?\n",
        "*   Be careful if you are using the full dataset with many features - google Colab may take a long time fitting models with large numbers of polynomial features, if your code is taking too long to run consider lowering the `max_degree` or lowering the size of the sample/number of features being used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg5DVJIn2Oc_"
      },
      "outputs": [],
      "source": [
        "##############################################################\n",
        "# Your code here\n",
        "##############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eTJcB4VcIdW"
      },
      "source": [
        "#4. Regularization\n",
        "Instead of manually selecting a model from numerous candidates, regularization can be used to help control the model complexity by lowering the parameter values automatically to weigh against unhelpful parameters. While this process isn't perfect, and can still lead to overfitting the validation data, it does present an easy way of including large numbers of features in your model without painstakingly adding and removing individual features manually.\n",
        "\n",
        "The strength of the regularization can be regarded as a hyperparameter. Hence, we apply the same pipeline as we did for polynomial degree selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL_yQObXdFQ-"
      },
      "source": [
        "##4.1 Example: Ridge regression (linear regression with $\\ell^2$ regularization)\n",
        "Linear regression with the $\\ell^2$ regularization term is called Ridge regression. This is just one basic form of a model utilising regularisation, essentially following the same process as our other linear models, but with a differently defined cost function being minimised.\n",
        "\n",
        "The cost function of the ridge regression is given as follows:\n",
        "$$\n",
        "J_{\\alpha} (\\boldsymbol{\\theta}) = L (\\boldsymbol{\\theta}) + R_{\\alpha} (\\boldsymbol{\\theta}),\n",
        "$$\n",
        "where\n",
        "$$\n",
        "L (\\boldsymbol{\\theta})\n",
        "=\n",
        "\\frac{1}{m} \\sum_{i=0}^{m-1} (y^{(i)} - {\\boldsymbol{x}^{(i)}}^{\\top} \\boldsymbol{\\theta})^{2},\n",
        "$$\n",
        "and\n",
        "$$\n",
        "R_{\\alpha} (\\boldsymbol{\\theta})\n",
        "=\n",
        "\\frac{\\alpha}{m} \\sum_{j=1}^{n-1} (\\theta_{j})^2.\n",
        "$$\n",
        "\n",
        "Here,\n",
        "- $L$ is the loss function that gives the mean squared error,\n",
        "- $R_{\\alpha}$ is the regularization function that gives a $\\ell^2$ regularization term,\n",
        "- $\\alpha > 0$ is the regularization weight.\n",
        "\n",
        "  - If $\\alpha$ is large, the regularization is strong, and overfitting is strongly avoided, but may cause underfitting.\n",
        "  - If $\\alpha$ is small, the regularization is weak. In this case the model is close to the original model, so it will not prevent any overfitting.\n",
        "\n",
        "We can implement the ridge regression model by a `sklearn.linear_model.RidgeRegression` instance.\n",
        "The regularization weight is specified by the parameter `alpha`, which corresponds to $\\alpha$, in the initializer of the `RidgeRegression` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vCOET_VjdZX"
      },
      "source": [
        "### 4.1.1 Ridge regression with a fixed $\\alpha$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Qfa546AgTF"
      },
      "source": [
        "First, let's just fit a basic Ridge regression instance with a regularisation weight $\\alpha$ we have chosen. Let's use a polynomial degree of 30 for this case. This follows the same process as earlier models, but now we are defining out `Ridge` model and `alpha` values as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnvdLWFfjwjP"
      },
      "outputs": [],
      "source": [
        "# Load the house price dataset\n",
        "X_pd, y_pd = sklearn.datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Convert the data to `np.array`\n",
        "X_raw = np.array(X_pd[['MedInc']])\n",
        "y = np.array(y_pd)\n",
        "\n",
        "# Split the data\n",
        "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X_raw, y, test_size=0.20, shuffle=True, random_state=0)\n",
        "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=0.25, shuffle=True, random_state=0)\n",
        "\n",
        "# Initialise\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 1\n",
        "degree = 10\n",
        "\n",
        "# Initialise Model and preprocessors\n",
        "model = Ridge(alpha=alpha)\n",
        "poly = PolynomialFeatures(degree=degree)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Train\n",
        "poly.fit(X_train_raw)\n",
        "X_train_poly = poly.transform(X_train_raw)\n",
        "scaler.fit(X_train_poly)\n",
        "X_train = scaler.transform(X_train_poly)\n",
        "model.fit(X_train, y_train) # note, we fit our ridge model exactly the same as our linear models\n",
        "y_pred_train = model.predict(X_train)\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "\n",
        "# Validate\n",
        "X_valid_poly = poly.transform(X_valid_raw)\n",
        "X_valid = scaler.transform(X_valid_poly)\n",
        "y_pred_valid = model.predict(X_valid)\n",
        "mse_valid = mean_squared_error(y_valid, y_pred_valid)\n",
        "\n",
        "print('MSE on training data:', mse_train)\n",
        "print('MSE on validation data:', mse_valid)\n",
        "\n",
        "# Preprocess test data\n",
        "X_test_poly = poly.transform(X_test_raw)\n",
        "X_test = scaler.transform(X_test_poly)\n",
        "\n",
        "# Predict test data\n",
        "y_pred_test = model.predict(X_test)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "print('MSE on the test data:', mse_test)\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test_raw[:,0], y_test,  color='black', label='y_true') # Observed y values\n",
        "plt.scatter(X_test_raw[:,0], y_pred_test, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('house price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y_test, y_pred_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJRl0bc6Dk_5"
      },
      "source": [
        "This seems to be giving us a decent fit to the data already without overfitting, but this is just from arbitrarily picking a regularisation rate. We can actually treat this as a hyperparameter, which means we can implement the same framework used above for polynomial degree to pick the best value for $\\alpha$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBDQF6QTEK4T"
      },
      "source": [
        "###4.1.2 Choosing best $\\alpha$ for ridge regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YoUvs4kESsF"
      },
      "source": [
        "This is a very similar pipeline to that done above for polynomial degree.\n",
        "However, unlike the from the previous polynomial case, the preprocessing is the same regardless of the regularization weight (our feature matrix never changes regardless of the regularisation). Hence, we preprocess the training and validation data before the `for` loop. You can focus on the training and evaluating MSE in the `for` loop.\n",
        "\n",
        "Note: When doing regularisation, we don't need to select the best polynomial degree per se - the regularisation term will in effect account for that to some extent by weighting against unhelpful parameters. However, this does mean that we may have many features in our dataset that are useless, yet are still taking up space/slowing down the fitting processes. I would recommend starting with a relatively small degree and try increasing it if the fitting isn't taking too long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8b6fbgFFRQx"
      },
      "outputs": [],
      "source": [
        "#####################\n",
        "# Load and split data\n",
        "#####################\n",
        "\n",
        "# Load the house price dataset\n",
        "X_pd, y_pd = sklearn.datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Convert the data to `np.array`\n",
        "X_raw = np.array(X_pd[['MedInc']])\n",
        "y = np.array(y_pd)\n",
        "\n",
        "# Split the data\n",
        "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X_raw, y, test_size=0.20, shuffle=True, random_state=0)\n",
        "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=0.25, shuffle=True, random_state=0)\n",
        "\n",
        "###################################\n",
        "# Initialise Model and preprocessors\n",
        "###################################\n",
        "\n",
        "degree = 10 # Choose your degree. Regularisation will eliminate any values that hurt our model, but do beware about going to big, or python may not be able to cope with the number of features.\n",
        "alpha_indices = np.arange(20) # Get a list of 1-20\n",
        "alphas = 10.0 ** (alpha_indices -10) # Define a list of alphas by taking indices from 10 to -10\n",
        "# Get the MSE arrays\n",
        "mse_train_array = np.full([len(alphas)], np.nan)\n",
        "mse_valid_array = np.full([len(alphas)], np.nan)\n",
        "\n",
        "poly = PolynomialFeatures(degree=degree)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#######################\n",
        "# Training & validation\n",
        "#######################\n",
        "\n",
        "# preprocessing\n",
        "poly.fit(X_train_raw)\n",
        "X_train_poly = poly.transform(X_train_raw)\n",
        "scaler.fit(X_train_poly)\n",
        "X_train = scaler.transform(X_train_poly)\n",
        "X_valid_poly = poly.transform(X_valid_raw)\n",
        "X_valid = scaler.transform(X_valid_poly)\n",
        "\n",
        "# Loop to test alpha values\n",
        "for alpha_index, alpha in zip(alpha_indices, alphas):\n",
        "  # train\n",
        "  model = Ridge(alpha=alpha) # fits our ridge model with a new regularisation weight each iteration\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred_train = model.predict(X_train)\n",
        "  mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "\n",
        "  # Validation\n",
        "  y_pred_valid = model.predict(X_valid)\n",
        "  mse_valid = mean_squared_error(y_valid, y_pred_valid)\n",
        "\n",
        "  # Store MSEs for this iteration\n",
        "  mse_train_array[alpha_index] = mse_train\n",
        "  mse_valid_array[alpha_index] = mse_valid\n",
        "  print(f'alpha: {alpha}, Validation mean squared error: {mse_valid}.')\n",
        "\n",
        "# Plot our graph of MSE for training and validation\n",
        "plt.plot(alphas, mse_train_array, label='MSE on training dataset')\n",
        "plt.plot(alphas, mse_valid_array, label='MSE on validation dataset')\n",
        "plt.xlabel(r'Regularization weights $\\alpha$')\n",
        "plt.ylabel('Mean squared error')\n",
        "plt.title(r'Regularization weights $\\alpha$ and mean squared error')\n",
        "plt.xscale('log')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Select our best performing alpha (lowest MSE)\n",
        "best_alpha_index = np.argmin(mse_valid_array)\n",
        "best_alpha = alphas[best_alpha_index]\n",
        "print('The best alpha:', best_alpha)\n",
        "\n",
        "######\n",
        "# Test\n",
        "######\n",
        "\n",
        "#refit our model with the best alpha\n",
        "model = Ridge(alpha=best_alpha) # fits our ridge model with a new regularisation weight each iteration\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Preprocess test data\n",
        "X_test_poly = poly.transform(X_test_raw)\n",
        "X_test = scaler.transform(X_test_poly)\n",
        "\n",
        "# Predict test data\n",
        "y_pred_test = model.predict(X_test)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "print('MSE on the test data:', mse_test)\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test_raw[:,0], y_test,  color='black', label='y_true') # Observed y values\n",
        "plt.scatter(X_test_raw[:,0], y_pred_test, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('house price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y_test, y_pred_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTdkLX0rLJ5r"
      },
      "source": [
        "Looking at this result, we can see that the pipeline chose a different value for $\\alpha$, providing a slightly better $R^2$ score and lower MSE than we obtained above (in this case the improvement is marginal, but this may not always be the case!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4R6Y2JVtBH6"
      },
      "source": [
        "##4.2 Example: Lasso regression (linear regression with $\\ell^1$ regularization)\n",
        "Linear regression with the $\\ell^1$ regularization term is called Lasso regression, compared to the ridge method we used above which utilises $\\ell^2$ regularisation. This also works by changing the cost function.\n",
        "\n",
        "The cost function of the lasso regression is given as follows:\n",
        "$$\n",
        "J_{\\alpha} (\\boldsymbol{\\theta}) = L (\\boldsymbol{\\theta}) + R_{\\alpha} (\\boldsymbol{\\theta}),\n",
        "$$\n",
        "where\n",
        "$$\n",
        "L (\\boldsymbol{\\theta})\n",
        "=\n",
        "\\frac{1}{m} \\sum_{i=0}^{m-1} (y^{(i)} - {\\boldsymbol{x}^{(i)}}^{\\top} \\boldsymbol{\\theta})^{2},\n",
        "$$\n",
        "and\n",
        "$$\n",
        "R_{\\alpha} (\\boldsymbol{\\theta})\n",
        "=\n",
        "\\frac{\\alpha}{m} \\sum_{j=1}^{n-1} (\\theta_{j}).\n",
        "$$\n",
        "\n",
        "Here,\n",
        "- $L$ is the loss function that gives the mean squared error,\n",
        "- $R_{\\alpha}$ is the regularization function that gives a $\\ell^1$ regularization term,\n",
        "- $\\alpha > 0$ is the regularization weight.\n",
        "\n",
        "  - If $\\alpha$ is large, the regularization is strong, and overfitting is strongly avoided, but may cause underfitting.\n",
        "  - If $\\alpha$ is small, the regularization is weak. In this case the model is close to the original model, so it will not prevent overfitting.\n",
        "\n",
        "We can implement the lasso regression model by a `sklearn.linear_model.Lasso` instance.\n",
        "The regularization weight is specified by the parameter `alpha`, which corresponds to $\\alpha$, in the initializer of the `Lasso` class.\n",
        "\n",
        "Below I have implemented Lasso Regression for a univariate case. The implementation is almost identical, so I have skipped straight to the automatic $\\alpha$ selection. Here it seems to be performing similarly to our Ridge regression, but this may not always be the case.\n",
        "\n",
        "Note: Our sweep through $\\alpha$ values may cause some convergence warning to appear for the Lasso, we don't need to worry about this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G3tOgiktTnU"
      },
      "outputs": [],
      "source": [
        "#####################\n",
        "# Load and split data\n",
        "#####################\n",
        "\n",
        "# Load the house price dataset\n",
        "X_pd, y_pd = sklearn.datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Convert the data to `np.array`\n",
        "X_raw = np.array(X_pd[['MedInc']])\n",
        "y = np.array(y_pd)\n",
        "\n",
        "# Split the data\n",
        "X_non_test_raw, X_test_raw, y_non_test, y_test = train_test_split(X_raw, y, test_size=0.20, shuffle=True, random_state=0)\n",
        "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X_non_test_raw, y_non_test, test_size=0.25, shuffle=True, random_state=0)\n",
        "\n",
        "###################################\n",
        "# Initialise Model and preprocessors\n",
        "###################################\n",
        "\n",
        "degree = 10 # Choose your degree. Regularisation will eliminate any values that hurt our model, but do beware about going to big, or python may not be able to cope with the number of features.\n",
        "alpha_indices = np.arange(20) # Get a list of 1-20\n",
        "alphas = 10.0 ** (alpha_indices -10) # Define a list of alphas by taking indices from 10 to -10\n",
        "# Get the MSE arrays\n",
        "mse_train_array = np.full([len(alphas)], np.nan)\n",
        "mse_valid_array = np.full([len(alphas)], np.nan)\n",
        "\n",
        "poly = PolynomialFeatures(degree=degree)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#######################\n",
        "# Training & validation\n",
        "#######################\n",
        "\n",
        "# preprocessing\n",
        "poly.fit(X_train_raw)\n",
        "X_train_poly = poly.transform(X_train_raw)\n",
        "scaler.fit(X_train_poly)\n",
        "X_train = scaler.transform(X_train_poly)\n",
        "X_valid_poly = poly.transform(X_valid_raw)\n",
        "X_valid = scaler.transform(X_valid_poly)\n",
        "\n",
        "# Loop to test alpha values\n",
        "for alpha_index, alpha in zip(alpha_indices, alphas):\n",
        "  # train\n",
        "  model = Lasso(alpha=alpha, max_iter=10000) # fits our ridge model with a new regularisation weight each iteration\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred_train = model.predict(X_train)\n",
        "  mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "\n",
        "  # Validation\n",
        "  y_pred_valid = model.predict(X_valid)\n",
        "  mse_valid = mean_squared_error(y_valid, y_pred_valid)\n",
        "\n",
        "  # Store MSEs for this iteration\n",
        "  mse_train_array[alpha_index] = mse_train\n",
        "  mse_valid_array[alpha_index] = mse_valid\n",
        "  print(f'alpha: {alpha}, Validation mean squared error: {mse_valid}.')\n",
        "\n",
        "# Plot our graph of MSE for training and validation\n",
        "plt.plot(alphas, mse_train_array, label='MSE on training dataset')\n",
        "plt.plot(alphas, mse_valid_array, label='MSE on validation dataset')\n",
        "plt.xlabel(r'Regularization weights $\\alpha$')\n",
        "plt.ylabel('Mean squared error')\n",
        "plt.title(r'Regularization weights $\\alpha$ and mean squared error')\n",
        "plt.xscale('log')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Select our best performing alpha (lowest MSE)\n",
        "best_alpha_index = np.argmin(mse_valid_array)\n",
        "best_alpha = alphas[best_alpha_index]\n",
        "print('The best alpha:', best_alpha)\n",
        "\n",
        "######\n",
        "# Test\n",
        "######\n",
        "\n",
        "#refit our model with the best alpha\n",
        "model = Lasso(alpha=best_alpha, max_iter=1000, tol=1e-1) # fits our lasso model with a new regularisation weight each iteration\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Preprocess test data\n",
        "X_test_poly = poly.transform(X_test_raw)\n",
        "X_test = scaler.transform(X_test_poly)\n",
        "\n",
        "# Predict test data\n",
        "y_pred_test = model.predict(X_test)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "print('MSE on the test data:', mse_test)\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test_raw[:,0], y_test,  color='black', label='y_test') # Observed y values\n",
        "plt.scatter(X_test_raw[:,0], y_pred_test, color='blue', label='y_test') # predicted y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('house price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y_test, y_pred_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp741GSu1jQl"
      },
      "source": [
        "## 4.3 Exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSKNFdC118dm"
      },
      "source": [
        "\n",
        "Try experimenting with the regularised regression models (Lasso and Ridge) to fit the best multivariable model you can on the California housing dataset.\n",
        "\n",
        "1.   To begin with, try sample size of `1000`, degree `5` and using all of the features, as this small sample should speed up testing.\n",
        "2.   Extra: Try manually removing some of the original features (so the features we have before fitting our polynomial features object) one at a time. How does this change the speed and the model accuracy?\n",
        "3.   Extra: Try changing the degree and sample size. How does the solution speed and best regularisation parameter change as the sample both increases and decreases?\n",
        "4.   Extra: Check the `model.coef_` values for your best model for Lasso and Ridge regression. What do you notice?\n",
        "\n",
        "\n",
        "Note:\n",
        "*   We want to use the automated $\\alpha$ selection from above, don't pick degree values manually!\n",
        "*   Above we were looking at small regularisation rates - we might want stronger regularisation in this case, so you may want to explore a different range of $\\alpha$ indices.\n",
        "*   Be careful if you are using the full dataset with many features - google colab may take a long time fitting models with large numbers of polynomial features, if your code is taking too long to run consider lowering the `degree`, lowering the size of the sample or removing the less impactful features (check the `model.coef_` values to identify which of the coefficients are contributing more to the prediction. Because our features are standardised, we can directly compare the magnitude of our coefficients this way).\n",
        "-  Extra: Consider using [cross validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) and/or [Grid Search](https://scikit-learn.org/0.17/modules/grid_search.html#grid-search) (see appendices) during your model selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H2gJn2plUCR"
      },
      "outputs": [],
      "source": [
        "##############################################################\n",
        "# Your code here\n",
        "##############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nafGN7ZT3_na"
      },
      "source": [
        "##4.4 Exercise 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnJGk_x34JvD"
      },
      "source": [
        "You should now have all the fundamental tools you need to systematically choose the best regression model for a given dataset for a traditional Machine Learning problem. (Though by all means do your own research into other models and methods you can apply to supervised learning problems!)\n",
        "\n",
        "For this exercise, use the dataset on the compressive strength of concrete(`Concrete_Data.csv`) which I have uploaded to moodle and see how good a regression model you can implement using all the tools we have discussed over the last two weeks. With a little experimentation I was able to get an $R^2$ score of about 85% - can you beat me?\n",
        "\n",
        "Notes:\n",
        "- This Concrete dataset is also available at the following link donated by Prof. I-Cheng Yeh: https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength\n",
        "- With only 1000 datapoints, you can probably work with the whole dataset, but if you want to explore high polynomial features (say above 10) in a reasonable time you might have to work with a smaller sample and/or remove features to test things in a reasonable time.\n",
        "-  Extra: Consider using [cross validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) and/or [Grid Search](https://scikit-learn.org/0.17/modules/grid_search.html#grid-search) (see appendices) during your model selection.\n",
        "- Extra: You could see how non parametric methods perform (e.g. Decision Trees & K-nearest neighbours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvT9uXak549M"
      },
      "outputs": [],
      "source": [
        "##############################################################\n",
        "# Your code here\n",
        "##############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nzpOWJcIon1"
      },
      "source": [
        "##4.5 Exercise 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUjPPUqZXfwY"
      },
      "source": [
        "If you have completed everything else, you could go back to our example of school test results from **Exercise 1**  (`testData.csv`) with the categorical features. Take the answer you got for this exercise, but now try applying all of the other things we've seen in this week's tutorial to see if we can improve our accuracy! I was able to get nearly 98%, can we do any better?\n",
        "\n",
        "Some things to try:\n",
        "\n",
        "*   Perform more robust data splitting with train/validation/test sets.\n",
        "*   Perform pre-processing on the numerical features\n",
        "*  See if polynomial features have an impact.\n",
        "*  You can use Ridge/Lasso regularisation (or even try a non parametric approach like a [decision tree](https://scikit-learn.org/stable/modules/tree.html#tree)).\n",
        "*  Extra: Consider using cross validation and/or grid searches (see appendices) during your model selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mfx6jvMJLWaj"
      },
      "outputs": [],
      "source": [
        "##############################################################\n",
        "# Your code here\n",
        "##############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zsQgR1wDpsZ"
      },
      "source": [
        "# Appendix 1: Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r8BVYfoDws4"
      },
      "source": [
        "Cross validation is an alternative way to evaluate your models during the model selection process.\n",
        "\n",
        "Rather than splitting your data into train/validation/test sets, initially you only split it into train/test sets.\n",
        "\n",
        "Then the training date is split into `k` folds (essentially subsamples of the training data). You then fit your model `k` times, using `k-1` of the folds for training and `1` fold for validation (changing the validation fold each time). You then take the average of the performance metrics (MSE/$R^2$), which gives you a more thorough idea of if your model is robust as by the end of this process it has been tested on all of the training data. If it passes the k-fold cross validation and you are happy with the average performance, you can then fit the model again on the ***entire*** training dataset and evaluate this final model using the test data.\n",
        "\n",
        "Issues with this is of course it is more computationally expensive as you are fitting `k-1` more models (which isn't a big deal with small datasets, but makes this approach impractical for larger datasets). Also it complicates the process for testing for hyperparameters as we would then have to include the cross validation in that wider loop (often if you want to perform a study with cross validation you would use some sort of pre-made grid search method - see the next appendix).\n",
        "\n",
        "You are not required to implement Cross validation to do well in your coursework, but you may implement it if you wish as it is a widely used and thorough way to evaluate your model prior to testing.\n",
        "\n",
        "See the sklearn documentation [here](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baeNpTFUQhAb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Load the house price dataset\n",
        "X_pd, y_pd = sklearn.datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Extract the Median Income feature\n",
        "X_raw = np.array(X_pd)\n",
        "y_raw = np.array(y_pd)\n",
        "\n",
        "# Split the data into training/test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.20, shuffle=True, random_state=0)\n",
        "\n",
        "# Create linear regression object\n",
        "model = LinearRegression()\n",
        "\n",
        "# Define K-fold Cross Validation object\n",
        "\n",
        "# n_splits - how many 'folds' you want to use to evalaute your model.\n",
        "# 5 folds is pretty standard, I wouldn't advise going much smaller than that, and unless you have a good rationale I would generally not go above 10.\n",
        "# While generally you want the k-fold splitting to give you folds of a similar size to your test data, this doesn't have to be too closely monitored.\n",
        "# You want to have a balance between what is computationally sensible with having enough folds to ensure you are happy with the testing thoroughness.\n",
        "\n",
        "# shuffle - shuffles your data while performing the k-fold splitting. Generally you want to do this as it just helps make the process even more robust.\n",
        "# random state - give the random seed for the shuffling. This means you can generate the same pseudorandom sample each time to aid comparison.\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "# Create empty lists to fill with out MSE and R2 values\n",
        "mse_list = []\n",
        "r2_list = []\n",
        "\n",
        "for train_index, valid_index in kf.split(X_train):\n",
        "    print('Validation points:', valid_index)\n",
        "    print('Training points:',train_index)\n",
        "\n",
        "    # split our full training data into the CV train and validation for this fold\n",
        "    X_train_cv, X_valid_cv = X_train[train_index], X_train[valid_index]\n",
        "    y_train_cv, y_valid_cv = y_train[train_index], y_train[valid_index]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train_cv, y_train_cv)\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred_valid = model.predict(X_valid_cv)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse_valid = mean_squared_error(y_valid_cv, y_pred_valid)\n",
        "    r2_valid = r2_score(y_valid_cv, y_pred_valid)\n",
        "\n",
        "    print(mse_valid)\n",
        "    print(r2_valid)\n",
        "    print(' ')\n",
        "\n",
        "    # Store results for this fold\n",
        "    mse_list.append(mse_valid)\n",
        "    r2_list.append(r2_valid)\n",
        "\n",
        "# Calculate average MSE and R2 across folds\n",
        "avg_mse = np.mean(mse_list)\n",
        "avg_r2 = np.mean(r2_list)\n",
        "\n",
        "print('Average MSE on validation data:', avg_mse)\n",
        "print('Average R2 score on validation data:', avg_r2)\n",
        "\n",
        "# Now, let's train the model on the full training set and evaluate on the test set\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test[:, 0], y_test, color='black', label='y_test')  # Observed y values\n",
        "plt.scatter(X_test[:, 0], y_pred_test, color='blue', label='y_pred')  # Predicted y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('House Price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The mean squared error loss on the test set\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "print('Mean squared error loss on test data: {:.4f}'.format(mse_test))\n",
        "\n",
        "# The R2 score on the test set: 1 is perfect prediction\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "print('R2 score on test data: {:.4f}'.format(r2_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8oj0OKVGzzO"
      },
      "source": [
        "Below is an expanded example where we are using cross validation to help us chose an optimal regularisation strength. While certainly workable, it significantly increases the computational load and does complicate our code further - so depending on how much we want to test a grid search may be a better choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqFyGbh94bPf"
      },
      "outputs": [],
      "source": [
        "#####################\n",
        "# Load and split data\n",
        "#####################\n",
        "\n",
        "# Load the house price dataset\n",
        "X_pd, y_pd = sklearn.datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Convert the data to `np.array`\n",
        "X_raw = np.array(X_pd[['MedInc']])\n",
        "y = np.array(y_pd)\n",
        "\n",
        "# Split the data\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X_raw, y, test_size=0.20, shuffle=True, random_state=0)\n",
        "\n",
        "###################################\n",
        "# Initialise Model and preprocessors\n",
        "###################################\n",
        "\n",
        "degree = 5 # Choose your degree. Regularisation will eliminate any values that hurt our model, but do beware about going to big, or python may not be able to cope with the number of features.\n",
        "alpha_indices = np.arange(10) # Get a list of 1-10\n",
        "alphas = 10.0 ** (alpha_indices -5) # Define a list of alphas by taking indices from 5 to -5\n",
        "# Get the MSE arrays\n",
        "mse_train_array = np.full([len(alphas)], np.nan)\n",
        "mse_valid_array = np.full([len(alphas)], np.nan)\n",
        "\n",
        "poly = PolynomialFeatures(degree=degree)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#######################\n",
        "# Training & validation\n",
        "#######################\n",
        "\n",
        "# preprocessing\n",
        "poly.fit(X_train_raw)\n",
        "X_train_poly = poly.transform(X_train_raw)\n",
        "scaler.fit(X_train_poly)\n",
        "X_train = scaler.transform(X_train_poly)\n",
        "\n",
        "# Loop to test alpha values\n",
        "for alpha_index, alpha in zip(alpha_indices, alphas):\n",
        "  # train\n",
        "  model = Ridge(alpha=alpha) # fits our ridge model with a new regularisation weight each iteration\n",
        "  mse_list_tr = []\n",
        "  mse_list_val = []\n",
        "\n",
        "  for train_index, valid_index in kf.split(X_train):\n",
        "    # split our full training data into the CV train and validation for this fold\n",
        "    X_train_cv, X_valid_cv = X_train[train_index], X_train[valid_index]\n",
        "    y_train_cv, y_valid_cv = y_train[train_index], y_train[valid_index]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train_cv, y_train_cv)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_train = model.predict(X_train_cv)\n",
        "    y_pred_valid = model.predict(X_valid_cv)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse_train = mean_squared_error(y_train_cv, y_pred_train)\n",
        "    mse_valid = mean_squared_error(y_valid_cv, y_pred_valid)\n",
        "\n",
        "    # Store results for this fold\n",
        "    mse_list_tr.append(mse_train)\n",
        "    mse_list_val.append(mse_valid)\n",
        "\n",
        "    # Calculate average MSE across folds\n",
        "    avg_mse_val = np.mean(mse_list_tr)\n",
        "    avg_mse_train = np.mean(mse_list_val)\n",
        "\n",
        "  # Store MSEs for this iteration\n",
        "  mse_train_array[alpha_index] = avg_mse_train\n",
        "  mse_valid_array[alpha_index] = avg_mse_val\n",
        "  print(f'alpha: {alpha}, Validation average mean squared error: {mse_valid}.')\n",
        "\n",
        "# Plot our graph of MSE for training and validation\n",
        "plt.plot(alphas, mse_train_array, label='MSE on training dataset')\n",
        "plt.plot(alphas, mse_valid_array, label='MSE on validation dataset')\n",
        "plt.xlabel(r'Regularization weights $\\alpha$')\n",
        "plt.ylabel('Mean squared error')\n",
        "plt.title(r'Regularization weights $\\alpha$ and mean squared error')\n",
        "plt.xscale('log')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Select our best performing alpha (lowest MSE)\n",
        "best_alpha_index = np.argmin(mse_valid_array)\n",
        "best_alpha = alphas[best_alpha_index]\n",
        "print('The best alpha:', best_alpha)\n",
        "\n",
        "######\n",
        "# Test\n",
        "######\n",
        "\n",
        "#refit our model with the best alpha\n",
        "model = Ridge(alpha=best_alpha) # fits our ridge model with a new regularisation weight each iteration\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Preprocess test data\n",
        "X_test_poly = poly.transform(X_test_raw)\n",
        "X_test = scaler.transform(X_test_poly)\n",
        "\n",
        "# Predict test data\n",
        "y_pred_test = model.predict(X_test)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "print('MSE on the test data:', mse_test)\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test_raw[:,0], y_test,  color='black', label='y_true') # Observed y values\n",
        "plt.scatter(X_test_raw[:,0], y_pred_test, color='blue', label='y_pred') # predicted y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('house price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y_test, y_pred_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zYiLfbyDxjI"
      },
      "source": [
        "#Appendix 2: Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WuL1JVLEBLf"
      },
      "source": [
        "Using grid search is an alternative to setting up your own studies (like we have been doing so far). This is a predefined framework in `sklearn` that can be test models using a variety of user defined hyperparameters to find the combination that provides the best results. Of course, this can be a very slow process and many models are being fitted and evaluated - so while grid search does allow you to test a lot of combinations, it is useful if you have the knowledge to provide it with smaller lists of options that you believe are most likely to obtain good results - as python risks timing out and/or running out of memory if you provide too large a list.\n",
        "\n",
        "You are free to use Grid Searches in your coursework (but this is not required for a top grade), but if you do so some things I want you to bear in mind:\n",
        "- Grid search does not magically give you the 'best' answer. You as the developer are choosing what hyperparameters to test and what values to use, along with other choices such as preprocessing and data splitting that can have a massive impact on the results.\n",
        "- It's also impossible to test every permutation of hyperparameter values, so it's always possible a different permutation you never tested will give you a better model.\n",
        "- You don't have to end your model selection with a single grid search. You can investigate the results and this may give you an idea of different parameters to try testing (either manually now or with further grid searches).\n",
        "- You don't have to accept the top result of a GridSeach as your final model choice - there are reasons other than the final score function for why you may choose one model over another.\n",
        "\n",
        "if you use grid search in your coursework I would expect you to clearly show what hyperparameters and hyperparameter values have been chosen to test, with some justifications give for these choices. Further interrogation of the grid search results and potentially even further testing (manual or using more grid searches) before selecting a final model will be looked on favourably.\n",
        "\n",
        "I'm going to be using `GridSearchCV`, which includes cross validation in it's selection process, but there are [other inbuilt search methods](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) in sklearn you can experiment with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjAQhCU-EBVy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the house price dataset\n",
        "X_pd, y_pd = fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Convert the data to `np.array`\n",
        "X_raw = np.array(X_pd)\n",
        "y = np.array(y_pd)\n",
        "\n",
        "# Split the data\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X_raw, y, test_size=0.1, shuffle=True, random_state=0)\n",
        "\n",
        "# Hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'ridge__alpha': [0.01, 0.1, 1, 10],  # Values for Ridge alpha\n",
        "    'poly__degree': [1, 2, 3, 4, 5],  # Values for PolynomialFeatures degree\n",
        "}\n",
        "\n",
        "# Initialize Model and preprocessors\n",
        "ridge = Ridge()\n",
        "poly = PolynomialFeatures()  # Apply PolynomialFeatures only to the selected feature\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create a pipeline with preprocessing and Ridge regression\n",
        "# Pipelines can be used outside of grid searchs to give you a tidier process\n",
        "# essentially they roll up multiple preprocesing operations with the model so it only requires 'fitting' once\n",
        "pipeline = Pipeline([\n",
        "    ('poly', poly),\n",
        "    ('scaler', scaler),\n",
        "    ('ridge', ridge)\n",
        "])\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# Pipeline - provide teh model (or pipeline) that you want to test teh grid search on\n",
        "# param_grid - the hyperparameter values you wish to test. Must make sure these\n",
        "# align with the hyperparameters of the pipeline being fitted or there will be an error.\n",
        "# cv - cross validation folds to use\n",
        "# scoring - how to evaluate the different models, many options, but for our regression problem I have chosen R2.\n",
        "# n_jobs - this allows parallel processing to be use multiple processors to run the search quicker.\n",
        "# Unless you have a good reason, I would leave as '-1' so all available processors are used.\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "\n",
        "# Fit the model with GridSearchCV\n",
        "grid_search.fit(X_train_raw, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Print the best model\n",
        "print(\"Best Model:\", best_model)\n",
        "\n",
        "# Train the best model on the full training set\n",
        "best_model.fit(X_train_raw, y_train)\n",
        "\n",
        "# Predict test data\n",
        "y_pred_test = best_model.predict(X_test_raw)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "print('MSE on the test data:', mse_test)\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test_raw[:, 0], y_test,  color='black', label='y_true')  # Observed y values\n",
        "plt.scatter(X_test_raw[:, 0], y_pred_test, color='blue', label='y_pred')  # Predicted y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('house price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(r2_score(y_test, y_pred_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc6Ppv0MJM6P"
      },
      "source": [
        "If you are using grid search properly, you should also be looking into all of the results for the entire set of combinations tested. This is good for giving you a better idea of what is and isn't working well (that may be used for further experimentation). You may also look at these results and decide that the 'best' model isn't actually the one you want to use - for example you may have a simpler model that only marginally comes in second place, but will be much more robust when deployed.\n",
        "\n",
        "One way of doing this is shown below, where all of the results have been put into a sorted table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv-yeijWG2-q"
      },
      "outputs": [],
      "source": [
        "# Get the results as a DataFrame\n",
        "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
        "\n",
        "# Display the relevant columns\n",
        "relevant_columns = ['params', 'mean_test_score', 'std_test_score']\n",
        "cv_results[relevant_columns].sort_values(by='mean_test_score', ascending=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1-VYU3jQPqP"
      },
      "source": [
        "Grid searches can potentially be used with almost any model (it is possible, but more complicated with Neural Networks...). So as an additional example, here is a Decision tree model fitted using grid search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C-4VpmkH8NT"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Load the house price dataset\n",
        "X_pd, y_pd = fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Convert the data to `np.array`\n",
        "X_raw = np.array(X_pd)\n",
        "y = np.array(y_pd)\n",
        "\n",
        "# Split the data\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X_raw, y, test_size=0.1, shuffle=True, random_state=0)\n",
        "\n",
        "# Hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'poly__degree': [1, 2],  # Values for PolynomialFeatures degree\n",
        "    'tree__max_depth': [None, 5, 10],  # Values for DecisionTreeRegressor max depth\n",
        "    'tree__min_samples_split': [None, 2, 5],  # Values for DecisionTreeRegressor min_samples_split\n",
        "}\n",
        "\n",
        "# Initialize Model and preprocessors\n",
        "tree = DecisionTreeRegressor()\n",
        "poly = PolynomialFeatures()  # Apply PolynomialFeatures only to the selected feature\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create a pipeline with preprocessing and Decision Tree regression\n",
        "pipeline = Pipeline([\n",
        "    ('poly', poly),\n",
        "    ('scaler', scaler),\n",
        "    ('tree', tree)\n",
        "])\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "\n",
        "# Fit the model with GridSearchCV\n",
        "grid_search.fit(X_train_raw, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Print the best model\n",
        "print(\"Best Model:\", best_model)\n",
        "\n",
        "# Train the best model on the full training set\n",
        "best_model.fit(X_train_raw, y_train)\n",
        "\n",
        "# Predict test data\n",
        "y_pred_test = best_model.predict(X_test_raw)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "print('MSE on the test data:', mse_test)\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test_raw[:, 0], y_test,  color='black', label='y_true')  # Observed y values\n",
        "plt.scatter(X_test_raw[:, 0], y_pred_test, color='blue', label='y_pred')  # Predicted y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('house price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(r2_score(y_test, y_pred_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbM-yh-VSBE5"
      },
      "outputs": [],
      "source": [
        "# Get the results as a DataFrame\n",
        "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
        "\n",
        "# Display the relevant columns\n",
        "relevant_columns = ['params', 'mean_test_score', 'std_test_score']\n",
        "cv_results[relevant_columns].sort_values(by='mean_test_score', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPQWQTCJQvgo"
      },
      "source": [
        "#Appendix 3: Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm3FXlK0Q0JH"
      },
      "source": [
        "As mentioned in week 1, on this module we're not having too big a focus on the data preparation side of Data Science / Machine Learning, however it is worth noting at this point how we can deal with missing data using imputation methods, which are another preprocessing method mentioned in the lecture. This provides you with an another option of what to do if your data contains missing values other than dropping those entries (Note: there is no missing data in your coursework, so you don't have to worry about doing this).\n",
        "\n",
        "These work in a similar way to our other preprocessing functions - you fit them on the training data and transform the training and validation/test data.\n",
        "\n",
        "The type of imputation you use depends on if you are missing a numeric or categorical data, but all can be accessed using the [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) object in sklearn.\n",
        "\n",
        "Numeric:\n",
        "- Mean - The average value in the column (generally the default choice for numerical columns)\n",
        "- Median - The middle value when ordering the column (can be better than the average when your data has outliers or a significant skew)\n",
        "- Mode - The most commonly occurring value in the column (unusual for numeric)\n",
        "- Constant - Replaces all missing values with a constant value defined by you (can be useful if you have some domain knowledge telling you how to treat these missing values)\n",
        "\n",
        "Categorical:\n",
        "- Mode - The most commonly occurring category in the column (Generally the default choice for a categorical column)\n",
        "- Constant - Replaces all missing values with a constant category value defined by you (can be useful if you have some domain knowledge telling you how to treat these missing values)\n",
        "\n",
        "Below I have an example of the school test dataset from exercise 1, but with missing data (`testData_missing.csv` on moodle). Try exploring how different imputation strategies impact the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNaPVPFyUm4n"
      },
      "outputs": [],
      "source": [
        "# Load new data (make sure to manually load into your python environment)\n",
        "testData = pd.read_csv('testData_missing.csv') # Save it to a pandas dataframe\n",
        "\n",
        "# Display only rows with missing data\n",
        "missing_data_rows = testData[testData.isna().any(axis=1)]\n",
        "display(missing_data_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQWrfCSFVQBI"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the data\n",
        "testData = pd.read_csv('testData_missing.csv')\n",
        "\n",
        "# Save it to a pandas dataframe\n",
        "tar = 'finalGrade'\n",
        "\n",
        "# create an object of the LabelEncoder class\n",
        "lblEncoder_X = LabelEncoder()\n",
        "\n",
        "# Impute missing values for numeric columns with mean\n",
        "numeric_cols = testData.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "#Below are the differnt approaches you could use for a numeric column - try uncommenting the others to see their impact.\n",
        "numeric_imputer = SimpleImputer(strategy='mean')\n",
        "#numeric_imputer = SimpleImputer(strategy='most_frequent')\n",
        "#numeric_imputer = SimpleImputer(strategy='median')\n",
        "#numeric_imputer = SimpleImputer(strategy='constant', fill_value = 0) # try messing around with other constant values!\n",
        "\n",
        "testData[numeric_cols] = numeric_imputer.fit_transform(testData[numeric_cols])\n",
        "\n",
        "# Impute missing values for categorical columns with mode\n",
        "categorical_cols = testData.select_dtypes(include=['object']).columns\n",
        "\n",
        "\n",
        "# Below are the differnt approaches you could use for a categorical column\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "  # Blanketly applying the constant this way means that all columns will have the same value imputed\n",
        "  # If you wanted to do anythin other than make a group for missing categories like\n",
        "  # I have done, you would have to start appling the constant imputation to\n",
        "  # individual columns so you can manually set what you want done for each feature.\n",
        "#categorical_imputer = SimpleImputer(strategy='constant', fill_value = 'Missing')\n",
        "\n",
        "testData[categorical_cols] = categorical_imputer.fit_transform(testData[categorical_cols])\n",
        "\n",
        "# Encode our categorical columns\n",
        "testData['studyTime'] = lblEncoder_X.fit_transform(testData['studyTime'])\n",
        "testData['travelTime'] = lblEncoder_X.fit_transform(testData['travelTime'])\n",
        "testData['absence'] = lblEncoder_X.fit_transform(testData['absence'])\n",
        "testData['school'] = lblEncoder_X.fit_transform(testData['school'])\n",
        "\n",
        "col_fin = ['mock1', 'mock2', 'studyTime', 'travelTime', 'absence', 'school']\n",
        "\n",
        "X = np.array(testData[col_fin])\n",
        "y = np.array(testData[tar])\n",
        "\n",
        "# Define how much test and training data we want. You can try changing these later to see how it changes the model and predictions\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True, random_state=0)\n",
        "\n",
        "# Create linear regression object\n",
        "obj = sklearn.linear_model.LinearRegression()\n",
        "\n",
        "# Train the model using the training sets\n",
        "obj.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "y_pred = obj.predict(X_test)\n",
        "\n",
        "X_test_disp = X_test[:, 0]  # We will need to make a special vector for the feature we want on the x axis, as now X is a matrix matplotlib can't use it for a scatter plot\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test_disp, y_test, color='black', label='y_test')  # Observed y values\n",
        "plt.scatter(X_test_disp, y_pred, color='blue', label='y_pred')  # Predicted y values\n",
        "plt.xlabel('Mock 1')\n",
        "plt.ylabel('Final Grade')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The mean squared error loss\n",
        "print('Mean squared error loss: {:.4f}'.format(sklearn.metrics.mean_squared_error(y_test, y_pred)))\n",
        "# The R2 score: 1 is perfect prediction\n",
        "print('R2 score: {:.4f}'.format(sklearn.metrics.r2_score(y_test, y_pred)))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JcHM9s21zzkH"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "COMP1801-ML(GPU)",
      "language": "python",
      "name": "comp1801-ml"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
