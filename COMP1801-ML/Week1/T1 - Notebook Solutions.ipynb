{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# COMP1801 Tutorial Week 1 - Python Introduction\n",
        "*Dr Peter Soar - 2024/25*\n",
        "\n",
        "This opening tutorial is going to be a bit of a whistlestop tour of various things I think it would be helpful to ensure that you to know for the rest of the module. If you are a January starter or have just have lots of experience in these topics, then I'm afraid this tutorial may be a bit dull.\n",
        "Apologies, but we need to make sure everyone is up to speed on the basics before we can move onto the more interesting topics next week!\n",
        "\n",
        "On the other hand, if you find yourself struggling with anything in this tutorial it will be worth while putting in the work now to do some more practice, otherwise you will likely find yourself having more problems as term goes on.\n",
        "\n"
      ],
      "metadata": {
        "id": "SMGwEzD_cPyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Colab basics"
      ],
      "metadata": {
        "id": "EBlR1BnddJqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section I just want to quickly introduce you to the colab environment we will be using for this module.\n",
        "\n",
        "A google Colab instance is essentially a Jupyter notebook hosted remotely, so this means the file you are currently reading is a notebook full of markdown text cells and cells filled with code that you can edit and execute. Here are a few things that you should remember:\n",
        "\n",
        "*   In order to edit this notebook if you came here directly from the link, you must first make a copy to your own google drive (and log into google if you have not already).\n",
        "*   Cells can be edited by clicking inside them.\n",
        "*   To execute a cell, press SHIFT + ENTER or hit the 'play' button on the left of the cell\n",
        "*   You can add new cells with the `+Code` and `+Text` button near the top of the screen.\n",
        "*  There are some useful commands under `Runtime` that allow you to do things such as restart the workbook and run all the cells in order.\n",
        "*   It's generally a good idea to run all cells in a workbook in order, as there will likely be dependencies that will cause errors if run out of order. If something isn't working and you are not sure why, a good first step is generally to restart the runtime and run all cells in the workbook in order again.\n",
        "* On the left are various buttons you can click to expand, showing things such as: Table of contents, a find and replace utility, view all of the variables in use and view the Colab file system.\n",
        "*  It's good practice to start the workbook with a cell containing the packages you will be using rather than importing them ad hoc as you need them in the workbook, like we have below:\n"
      ],
      "metadata": {
        "id": "tLAexU7p_sGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell or a lot of the code in the workbook will not work!\n",
        "import numpy as np # A useful package for dealing with mathematical processes\n",
        "import sklearn.datasets # sklearn is an important package for much of the ML we will be doing, but this time we are only interested in its datasets\n",
        "import pandas as pd # a common package for viewing tabular data\n",
        "import matplotlib.pyplot as plt # A popular package for visualisation in python. While quite easy to use for basic functions, you can also do some quite impressive things using it."
      ],
      "metadata": {
        "id": "G3zUJLDuR53a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully that will make things clear if you have never used Colab (or a notebook) before.\n",
        "\n",
        "If you are struggling with using python more generally, there are many free online tutorials which will introduce you to all the basic python commands and operations that you should know."
      ],
      "metadata": {
        "id": "zt8_F9jj_Cm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Linear Algebra Recap"
      ],
      "metadata": {
        "id": "5kzBMzxqdUx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the lectures, when describing how models work it will often be helpful to describe the models using mathematical notation, which means you will need to understand the basics of how vectors and matrices work in both theory and in Python so we can understand how they are related to our ML algorithms.\n",
        "\n",
        "Hopefully this section will just be a refresh of some mathematical concepts that you have encountered in the past, but if all of this is new to you I would recommend doing some more practice in your own time with free online tutorials (for Linear Algebra / Matrices) or by checking out some books on the reading list such as 'Mathematics for Machine Learning', which you can also obtain free online copies of."
      ],
      "metadata": {
        "id": "EFc29Hm27CEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Matrices and Vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "b9xHMp0retUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A matrix is a rectangular array of numbers written inside of square brackets, often represented as a bold upper case letter in cases where you do not want to write out the entire Matrix, for example:\n",
        "$$\n",
        "\\boldsymbol{A}\n",
        "= \\begin{bmatrix}\n",
        "2 & 11  & -12 \\\\\n",
        "6 & 13  & -1 \\\\\n",
        "7 & 19  & -6 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "or for the general case we can take the following matrix $\\boldsymbol{B} $  where *m* is the number of rows and *n* is the number of columns for the matrix populated with real numbers $a_{ij}$:\n",
        "$$\n",
        "\\boldsymbol{B}\n",
        "= \\begin{bmatrix}\n",
        "a_{0,0} & a_{0,1} & \\cdots & a_{0,n-1} \\\\\n",
        "a_{1,0} & a_{1,1} & \\cdots & a_{1,n-1} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_{m-1,0} & a_{m-1,1} & \\cdots & a_{m-1,n-1} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Each row or column of the Matrix can also be represented using a vector. A vector is a 1D array of numbers written inside square brackets and usually represented with a lowercase letter when not writing out the full vector:\n",
        "\n",
        "\n",
        "$$\\boldsymbol{a}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "2\\\\\n",
        "6 \\\\\n",
        "7  \\\\\n",
        "\\end{bmatrix}\n",
        ".\n",
        "$$\n",
        "or for the general case we can take the following vector $\\boldsymbol{b} $  where *m* is the number of rows for the vector populated with real numbers $v_{i}$:\n",
        "$$\\boldsymbol{b}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "v_{0} \\\\\n",
        "v_{1} \\\\\n",
        "\\vdots \\\\\n",
        "v_{m-1} \\\\\n",
        "\\end{bmatrix}\n",
        ".\n",
        "$$\n"
      ],
      "metadata": {
        "id": "VHTDwMIFfq0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Working with matrices"
      ],
      "metadata": {
        "id": "zm0y-bUclCv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following are some important things you should know about working with matrices (and vectors, which are just a specific type of 1D matrix):\n",
        "\n",
        "\n",
        "*   You can add (or subtract) matrices, but the matrices must have the same dimensions, where the corresponding elements of the matrix are added to create a new Matrix:\n",
        "$$\n",
        "\\boldsymbol{A} + \\boldsymbol{B}\n",
        "= \\begin{bmatrix}\n",
        "2 & 11 \\\\\n",
        "6 & 13 \\\\\n",
        "\\end{bmatrix} + \\begin{bmatrix}\n",
        "8 & 17 \\\\\n",
        "7 & 11 \\\\\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "2+8 & 11+17 \\\\\n",
        "6+7 & 13+11 \\\\\n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "10 & 28 \\\\\n",
        "13 & 24 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "*   You can multiply (or divide) a matrix by any scalar, which acts upon every element of the matrix:\n",
        "$$\n",
        "3 \\times \\boldsymbol{A}\n",
        "= 3 \\times \\begin{bmatrix}\n",
        "2 & 11 \\\\\n",
        "6 & 13 \\\\\n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        "3 \\times 2 & 3 \\times 11 \\\\\n",
        "3 \\times 6 & 3 \\times 13 \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "6 & 33 \\\\\n",
        "18 & 39 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "*   Matrix multiplication is a more complex process, where to obtain the elements of the new matrix, for each column of the left matrix, you step through each column of the right matrix, multiplying the corresponding elements and adding together these products to get the new value for each element of the matrix. This is probably easiest to understand by seeing it in practice, visualised here by this simple, general case:\n",
        "$$\n",
        "\\boldsymbol{B} \\times\n",
        "\\boldsymbol{C}  \n",
        "= \\begin{bmatrix}\n",
        "b_{0,0} & b_{0,1} \\\\\n",
        "b_{1,0} & b_{1,1} \\\\\n",
        "\\end{bmatrix} \\times \\begin{bmatrix}\n",
        "c_{0,0} & c_{0,1} \\\\\n",
        "c_{1,0} & c_{1,1} \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "b_{0,0} \\times c_{0,0} +  b_{0,1} \\times c_{1,0}  & b_{0,0} \\times c_{0,1} +  b_{0,1} \\times c_{1,1} \\\\\n",
        "b_{1,0} \\times c_{0,0} +  b_{1,1} \\times c_{1,0}  & b_{1,0} \\times c_{0,1} +  b_{1,1} \\times c_{1,1} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "and using real values:\n",
        "$$\n",
        "\\boldsymbol{B} \\times\n",
        "\\boldsymbol{C}  \n",
        "= \\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4 \\\\\n",
        "\\end{bmatrix} \\times \\begin{bmatrix}\n",
        "5 & 6 \\\\\n",
        "7 & 8 \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 \\times 5 +  2 \\times 7  & 1 \\times 6 +  2 \\times 8 \\\\\n",
        "3 \\times 5 +  4 \\times 7 & 3 \\times 6 + 4 \\times 8 \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "5 +  14  & 6 +  16 \\\\\n",
        "15 +  28 & 18 + 32 \\\\\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "19  & 22 \\\\\n",
        "43 & 50 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "*   Matrix multiplication has some properties we need to keep in mind:\n",
        " - Matrix multiplication is not commutative, this means that the order of your matrices matters and so $\\boldsymbol{B} \\times\n",
        "\\boldsymbol{C}  \\neq \\boldsymbol{C} \\times\n",
        "\\boldsymbol{B}$\n",
        "  - Matrix multiplication is associative, meaning that $(\\boldsymbol{A} \\times\\boldsymbol{B}) \\times\n",
        "\\boldsymbol{C}  =  \\boldsymbol{A} \\times (\\boldsymbol{B} \\times\n",
        "\\boldsymbol{C})$\n",
        "  - We have an identity matrix where the diagonal is all '1', and any matrix multiplied by the identity matrix remains unchanged:\n",
        " $$\\boldsymbol{I} \\times \\boldsymbol{B} = \\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1 \\\\\n",
        "\\end{bmatrix} \\times\n",
        "\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4 \\\\\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "  - You can multiply differently sized matrices, but their dimensions must correspond (and this can lead a matrix with different dimensions to either of the initial matrices). You can multiply a vector and a matrix because of this.\n",
        "  $$\n",
        "\\boldsymbol{B} \\times\n",
        "\\boldsymbol{v}  \n",
        "= \\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4 \\\\\n",
        "\\end{bmatrix}\n",
        "\\times \\begin{bmatrix}\n",
        "5 \\\\\n",
        "7 \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 \\times 5 +  2 \\times 7   \\\\\n",
        "3 \\times 5 +  4 \\times 7  \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "5 +  14   \\\\\n",
        "15 +  28  \\\\\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "19  \\\\\n",
        "43  \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "* You can work out the inverse of a matrix such that $\\boldsymbol{B} \\times \\boldsymbol{B}^{-1} = \\boldsymbol{I}$. I won't go into the details of working out a matrix inverse here as it is quite involved and we won't need to manually calculate the matrix inverse in this module.\n",
        "* The transpose of a matrix is an operation that swaps the rows and columns of a matrix, such that for matrix:\n",
        "$$\\boldsymbol{A}\n",
        "= \\begin{bmatrix}\n",
        "24 & 17 \\\\\n",
        "5 & 64 \\\\\n",
        "34 & 55 \\\\\n",
        "\\end{bmatrix}$$\n",
        "The transpose (denoted $\\boldsymbol{A}^{T}$) would be:\n",
        "$$\n",
        "\\boldsymbol{A}^{T}\n",
        "= \\begin{bmatrix}\n",
        "24 & 5 & 34 \\\\\n",
        "17 & 64 & 55 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "Bf6Z8bOslNah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Matrices in Python\n",
        "  - A matrix (e.g. $\\boldsymbol{A}$) is represented by 2D `np.array` (e.g. `A`) in NumPy.\n",
        "  - `A.shape`: the shape of `A`. If `A` represents an $m \\times n$ matrix, then `A.shape == (m, n)`.\n",
        "  - $a_{i,j}$ (`A[i, j]`): the element in the $i$-th row and $j$-th column of $\\boldsymbol{A}$ (`A`).\n",
        "  - `A[i, :]`: the 1D `np.array` that contains the $i$-th row of $\\boldsymbol{A}$\\\n",
        "`A[i, :]` $=$ `[A[i, 0], A[i, 1], ..., A[i, n-1]]`\n",
        "$=\n",
        "    \\begin{bmatrix}\n",
        "    a_{i,0} & a_{i,1} & \\cdots & a_{i,n-1} \\\\\n",
        "    \\end{bmatrix}\n",
        "    .\n",
        "$\n",
        "  - `A[:, j]`: the 1D `np.array` that contains the $j$-th column of $\\boldsymbol{A}$.\\\n",
        "`A[:, j]` $=$ `[A[0, j], A[1, j], ..., A[m-1, j]]`\n",
        "$=\n",
        "    \\begin{bmatrix}\n",
        "    a_{0,j} \\\\ a_{1,j} \\\\ \\vdots \\\\ a_{m-1,j} \\\\\n",
        "    \\end{bmatrix}\n",
        "    .\n",
        "$\n",
        "  - A 2D `np.array` is the stack of the 1D arrays that correspond to the **row** vectors.\\\n",
        "`A` $=$ `[A[0, :], A[1, :], ..., A[m-1, :]]`\\\n",
        " $=$\\\n",
        "`[[A[0, 0], A[0, 1], ..., A[0, n-1]],`\\\n",
        "&ensp;`[A[1, 0], A[1, 1], ..., A[1, n-1]],`\\\n",
        "&ensp;`...,`\\\n",
        "&ensp;` [A[m-1, 0], A[m-1, 1], ..., A[m-1, n-1]]]`\n",
        "  - $\\boldsymbol{A}^{\\top}$ (`A.T`): the transpose of $\\boldsymbol{A}$. If `A.shape == (m, n)` then `A.T.shape == (n, m)`.\n",
        "  - You can perform matrix multiplication by using the `.dot()` function, so $\\boldsymbol{A} \\times \\boldsymbol{B}$ would be `A.dot(B)`. Alternatively you can use the `@` symbol to get the same effect , e.g. `A@B`.\n",
        "  - Vectors:\n",
        "    - We always represent a row vector as the transpose of the column vector, e.g. $\\boldsymbol{v}^{\\top}$.\n",
        "    - A row or column vector (e.g. $\\boldsymbol{v}$ or $\\boldsymbol{v}^{\\top}$) is usually represented by an 1D `np.array` (e.g. `v`) in NumPy.\n",
        "    - Note: an 1D `np.array` does not distinguish row and column vectors.\n",
        "    - To distinguish them (e.g. matrix multiplication), we use an $m \\times 1$ 2D `np.array` (an $m$-dimensional column vector) or $1 \\times n$ 2D `np.array`  (an $n$-dimensional row vector).\n",
        "    - `v.shape`: the shape of `v`. If `v` represents a vector with the size of $m$, then `v.shape == (m, )`.\n",
        "    - `v[i]`: the `i`-th element of `v`.\n",
        "$$\n",
        "    \\texttt{v}\n",
        "    \\texttt{==}\n",
        "    \\texttt{[v[0], v[1], ..., v[m-1]]} \\\\\n",
        "    =\n",
        "    \\begin{bmatrix}\n",
        "    v_{0} \\\\\n",
        "    v_{1} \\\\\n",
        "    \\vdots \\\\\n",
        "    v_{m-1} \\\\\n",
        "    \\end{bmatrix}.\n",
        "$$\n"
      ],
      "metadata": {
        "id": "mtcnGeEG6H96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of accessing elements of vectors and matrices and performing  matrix operations"
      ],
      "metadata": {
        "id": "XYEpAq1M6lcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vector\n",
        "v = np.array( [ 1, 2**2, 3**2, 4**2, 5**2, 6**2, 7**2, 8**2] )\n",
        "print('v =', v) # See the whole vector\n",
        "# Access the elements\n",
        "print('v[0] =', v[0])\n",
        "print('v[1] =', v[1])\n",
        "print('v[4] =', v[4])\n",
        "print('v[7] =', v[7])\n",
        "\n",
        "#Create a Matrix\n",
        "A = np.array( [[ 1, 2**2, 3**2],[4**2, 5**2, 6**2],[7**2, 8**2,9**2]] )\n",
        "print('A =\\n', A, '\\n') # View the whole matrix\n",
        "print('A[2, :] =', A[2, :]) # View a row\n",
        "print('A[:, 0] =', A[:, 0]) # View a column\n",
        "print('A[1, 2] =', A[1, 2]) # View a single element\n",
        "\n",
        "# Make a new matrix\n",
        "B = np.array( [[ 1, 2, 3],[4, 5, 6],[7, 8,9]] )\n",
        "print('B =\\n', B, '\\n') # View the whole matrix\n",
        "print('Matrix Addition')\n",
        "print('A+B =\\n', A+B, '\\n')\n",
        "print('Matrix Subtraction')\n",
        "print('A-B =\\n', A-B, '\\n')\n",
        "\n",
        "print('Scalar multiplication')\n",
        "print('3*A =\\n', 3*A, '\\n')\n",
        "print('Transpose')\n",
        "print('A^T =\\n', A.T, '\\n')\n",
        "print('Matrix Multiplication')\n",
        "print('AB =\\n', A.dot(B), '\\n')"
      ],
      "metadata": {
        "id": "Vys4O5Hp6e7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "Take the matrices\n",
        "$$\n",
        "\\boldsymbol{A}\n",
        "= \\begin{bmatrix}\n",
        "6 & 3 & 5\\\\\n",
        "12 & -5 & 11\\\\\n",
        "-4 & 7 & -2\\\\\n",
        "\\end{bmatrix}, \\boldsymbol{B}\n",
        "= \\begin{bmatrix}\n",
        "4 & 6 & 6 \\\\\n",
        "9 & 12 & 9 \\\\\n",
        "3 & 21 & 1 \\\\\n",
        "\\end{bmatrix}, \\boldsymbol{v}\n",
        "=\\begin{bmatrix}\n",
        "2  \\\\\n",
        "9  \\\\\n",
        "14   \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Using Python create these matrices and try to work out the following:\n",
        "* $\\boldsymbol{A} + \\boldsymbol{B}$\n",
        "* $\\boldsymbol{B} - \\boldsymbol{A}$\n",
        "* $3 \\times \\boldsymbol{A} - 2 \\times \\boldsymbol{B}$\n",
        "* $\\boldsymbol{A} \\times \\boldsymbol{B}$\n",
        "* $\\boldsymbol{B} \\times \\boldsymbol{A}$\n",
        "* $\\boldsymbol{A} \\times \\boldsymbol{v}$\n"
      ],
      "metadata": {
        "id": "nrQf16zcx3Ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "# Your code Here\n",
        "##############################\n",
        "\n",
        "A = np.array( [[ 6, 3, 5],[12, -5, 11],[-4, 7,-2]] )\n",
        "B = np.array( [[ 4, 6, 6],[9, 12, 9],[3, 21,1]] )\n",
        "v1 = np.array( [ 2, 9, 14] ) # This is technically 1D, so does not distinguish between being a row or column\n",
        "v2 = np.array( [ [2], [9], [14]] ) # To make it explicitely a column vector you can define it with two square brackets to make it a true 2D Matrix\n",
        "#v2 = np.array( [ [2, 9, 14]] ) # And this is how you would do the same thing but make it a row vector (which won't work for this question.)\n",
        "\n",
        "print('A+B =\\n', A+B, '\\n')\n",
        "print('B+A =\\n', B-A, '\\n')\n",
        "print('3A-2B =\\n', 3*A-2*B, '\\n')\n",
        "print('AB =\\n', A.dot(B), '\\n')\n",
        "print('BA =\\n', B.dot(A), '\\n')\n",
        "print('Av =\\n', A.dot(v1), '\\n') # Note, numpy is smart enough to know what to do with your 1D vector for matrix multiplication, though the output is still 1D.\n",
        "print('Av =\\n', A.dot(v2), '\\n') # To get a 2D output you need your vector to be in the 2D form first."
      ],
      "metadata": {
        "id": "6Gbw7nkb6Vgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Loading & Manipulation"
      ],
      "metadata": {
        "id": "ZYpGrHO6dNgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can't implement Machine Learning techniques if we don't have any data to use it on!\n",
        "For the later tutorials, your coursework and likely for many other projects (either on the MSc or in the future) it will be critical that you know how to load data into python (into google Colab specifically in our case), view that data and perform various operations to manipulate that data into the form we need.\n",
        "\n",
        "This section will show you some different methods you can use to get your data into colab, and then demonstrate some of the key ways of viewing and manipulating it."
      ],
      "metadata": {
        "id": "TRSrBBdY-iGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Data Loading"
      ],
      "metadata": {
        "id": "JutWRire_8GQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method 1: Load from packages**\n",
        "\n",
        "The first case is a bit of a cheat - some python packages may come with some datasets included. Of particular interest to us is sklearn, which we will be using a lot in this module. This includes a selection of datasets that are commonly used for initial testing of ML techniques. I'm going to show one example here, but you can see what other datasets are included at:\n",
        "https://scikit-learn.org/stable/datasets.html\n",
        "\n",
        "Note, there are some very small toy datasets that you *load*, and some larger ones (real world datasets) that you *fetch*."
      ],
      "metadata": {
        "id": "jpucXKjSJnKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are actually multiple different ways we can use these `sklearn` datasets, first let's just try pulling everything into a variable:"
      ],
      "metadata": {
        "id": "FfhIz29l6dF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the California housing dataset using the following command\n",
        "house = sklearn.datasets.fetch_california_housing()\n",
        "# If you display this you can see there are multiple parts to the dataset: Data, target data, target names, feature (column) names and a description of the dataset.\n",
        "display(house)"
      ],
      "metadata": {
        "id": "gZV6xgcmR8om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " We can see there are a lot of elements to this data library. We will go into 'target' and 'feature' aspects of the data in more detail next week, for now let's combine all this data into a nice dataframe."
      ],
      "metadata": {
        "id": "1RKwYNTvUEWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see that the 'Data' is just that, the predictive attributes with no names or the house values.\n",
        "df = pd.DataFrame(data= house['data'])\n",
        "display(df)"
      ],
      "metadata": {
        "id": "hWuJsCZeTXYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we stitch together the predictive and target data together using 'c_' and we combine the feature and target names that go into 'columns'\n",
        "df = pd.DataFrame(data= np.c_[house['data'], house['target']], columns= house['feature_names'] + ['target'])\n",
        "\n",
        "# show the data\n",
        "display(df)"
      ],
      "metadata": {
        "id": "k_M31EPy-hi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, while there are some benefits to loading in the data this way, it often adds some unnecessary steps to get what we want - our data in a frame, so generally in the module I will be calling in the sklearn sets like so:"
      ],
      "metadata": {
        "id": "HjBDajH866Rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This returns our features and target as separate dataframes automatically\n",
        "X_pd, y_pd = sklearn.datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "\n",
        "# show the data\n",
        "display(X_pd)"
      ],
      "metadata": {
        "id": "fD_5M8S27MLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can still easily stitch them together for display using the `concat` command:"
      ],
      "metadata": {
        "id": "uxPy4aok8gHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the X and y for visualisation\n",
        "XY_pd = pd.concat([X_pd, y_pd], axis=1)\n",
        "\n",
        "# show the data\n",
        "display(XY_pd)"
      ],
      "metadata": {
        "id": "wkMjPooy7U2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**\n",
        "\n",
        "Try loading a copy of the famous 'Iris' dataset into a dataframe, which sklearn also has preloaded as a dataset (see link given above). Note that this is a small 'Toy' dataset, so you want to *load* rather than *fetch* it from sklearn (`load_iris()`)."
      ],
      "metadata": {
        "id": "-dICxvcohDqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "# Your code Here\n",
        "##############################\n",
        "\n",
        "X_pd, y_pd = sklearn.datasets.load_iris(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Combine the X and y for visualisation\n",
        "XY_pd = pd.concat([X_pd, y_pd], axis=1)\n",
        "\n",
        "# show the data\n",
        "display(XY_pd)"
      ],
      "metadata": {
        "id": "AWErT22OhUZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method 2: Upload a .csv from your computer**\n",
        "I have created a small dataset based on the California housing data which I have uploaded to moodle. Download it to your computer and try uploading it to Colab by doing the following:\n",
        "\n",
        "\n",
        "1.   Open the Colab filesystem (the little folder on the left of your scree)\n",
        "2.   Click the upload to session storage button (the page with an arrow)\n",
        "3.   Select the .csv file from your computer\n",
        "\n",
        "You should now be able to see the .csv file in the file system.\n",
        "\n",
        "Note that files loaded into the filesystem will be removed when you close down colab. You may want to research mounting your google drive into Colab and storing your data on there if this is causing you problems.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rLgF94BTbVcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you can use the read_csv command to store the .csv in a variable\n",
        "housingData = pd.read_csv('caliHousing_exercise.csv')\n",
        "# This file has all the data elements constructed already, so we can just put the entire thing into the 'data='\n",
        "df = pd.DataFrame(data= housingData)\n",
        "\n",
        "# show the data\n",
        "display(df)"
      ],
      "metadata": {
        "id": "8S1h9LiPbTzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method 3: Load from .csv on your computer directly**\n",
        "\n",
        "You can also import local files without having to go into the file system and clicking buttons. Delete the version currently uploaded in the file system and try the following:"
      ],
      "metadata": {
        "id": "FccUF8umYsMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  You need to import these packages so that colab can load from your computer.\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()    # Will prompt you to select file\n",
        "# Your file is now in the Colab filesystem on the left (click the little folder and you should see the file caliHousing_exercise.csv)\n",
        "housingData_csv = pd.read_csv('caliHousing_exercise.csv')"
      ],
      "metadata": {
        "id": "jXJsp0W2Yni5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This file has all the data elements constructed already, so we can just put the entire thing into the 'data='\n",
        "df = pd.DataFrame(data= housingData_csv)\n",
        "\n",
        "# show the data\n",
        "display(df)"
      ],
      "metadata": {
        "id": "OnPF8J5QabP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method 4: Data stright from URL**\n",
        "\n",
        "If you have a URL direct to any .csv files then that can be read directly into the *read_csv* command. Here I found another version of the housing data in a GitHub repository."
      ],
      "metadata": {
        "id": "kuJ6h9M7eND5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housingData_url = pd.read_csv(\"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\")\n",
        "# This file has all the data elements constructed already, so we can just put the entire thing into the 'data='\n",
        "df = pd.DataFrame(data= housingData_url)\n",
        "\n",
        "# show the data\n",
        "display(df)"
      ],
      "metadata": {
        "id": "IQB2p8V1eMYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**\n",
        "\n",
        "The eagle eyed among you may have noticed that there are some subtle differences between all of these different versions of the California housing dataset. These are mostly a matter of scaling the data to different sizes (or adding/removing string columns), but it's also a good lesson to be aware of your data sources and don't just assume every version of a dataset is truly identical!"
      ],
      "metadata": {
        "id": "FS6DtvIY-FkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**\n",
        "\n",
        "Try looking on the internet to find a copy of the 'Iris' dataset and load it as a .csv file (either from a link or by downloading a local copy to your computer)."
      ],
      "metadata": {
        "id": "c93G0E__gTFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "# Your code Here\n",
        "##############################\n",
        "irisData_url = pd.read_csv(\"https://gist.githubusercontent.com/Thanatoz-1/9e7fdfb8189f0cdf5d73a494e4a6392a/raw/aaecbd14aeaa468cd749528f291aa8a30c2ea09e/iris_dataset.csv\")\n",
        "# This file has all the data elements constructed already, so we can just put the entire thing into the 'data='\n",
        "df = pd.DataFrame(data= irisData_url)\n",
        "\n",
        "# show the data\n",
        "display(df)\n",
        "# Note, the sklearn data I imported had the target as [0,1,2], while for this version the target is a string\n",
        "# for a Machine Learning application you would want to replace these string with numbers before you can fit the model\n",
        "df['target'] =  df['target'].map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}) # Many methods you could use to replace the values\n",
        "display(df)"
      ],
      "metadata": {
        "id": "FeE1x-zbgq9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some other methods of getting data into colab and of course other types of potential data sources than .csv files. However, for the purposes of this module what we have covered here should be sufficient start."
      ],
      "metadata": {
        "id": "6IJ3kzgjf6r4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Data Manipulation"
      ],
      "metadata": {
        "id": "xWjLVi6tkv05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While there can be much more to getting your data in the right format for your ML implementation. In this context, all I mean by 'data manipulation' are the simple operations such as splitting your dataset in separate frames for different columns or removing any unwanted rows/columns."
      ],
      "metadata": {
        "id": "II0cCdhxk8rO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the housing dataset:"
      ],
      "metadata": {
        "id": "QIUgmYVwl21M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_pd, y_pd = sklearn.datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "df = pd.concat([X_pd, y_pd], axis=1)\n",
        "display(df)"
      ],
      "metadata": {
        "id": "IaUAVIKRluqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we decide that the 'Latitude' and 'Longitude' values give us no predictive weight, we may decide we want to remove them from our dataset by dropping the columns."
      ],
      "metadata": {
        "id": "GwkpHk4vmLDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cut=df.copy() # Make a copy of the dataframe when dropping/updating so you have the old data still\n",
        "df_cut.drop([\"Latitude\", \"Longitude\"], axis = 1, inplace=True)\n",
        "display(df_cut)"
      ],
      "metadata": {
        "id": "17otE0PLmVZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we can also drop rows of the data is we need to (compare output with above - rows 2 and 3 are gone):"
      ],
      "metadata": {
        "id": "OOsIdFcunfaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cut.drop([2, 3], axis = 0, inplace=True)\n",
        "display(df_cut)"
      ],
      "metadata": {
        "id": "WrtBrJxeno8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But if you only want a few columns, generally it is easier to just copy the columns you want into their own new data frames. If we only want to use the 'MedInc' and 'HouseAge' as our features and we wanted to separate out the target we could do the following:"
      ],
      "metadata": {
        "id": "i-1jg3JMoMO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = df[['MedInc','HouseAge']]\n",
        "target = df[['MedHouseVal']]\n",
        "\n",
        "# This time lets print out just the top rows using 'head' so we don't have too much output\n",
        "print(features.head(5))\n",
        "print(target.head(5))\n"
      ],
      "metadata": {
        "id": "tvZjtzJfoquU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also target single cells in the frame to be updated."
      ],
      "metadata": {
        "id": "RQGn9n5D8sTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_updated=features.copy() # make a copy when dropping/updating so you have the old data still\n",
        "features_updated['HouseAge'][1]=31\n",
        "print(features_updated.head(5))"
      ],
      "metadata": {
        "id": "RsZibqgeq_5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**\n",
        "\n",
        "Load the Iris dataset again. This time, calculate a new variable called 'Petal Area (cm^2)' which is the product of the petal length and width. Create a new data frame where you have removed the 'petal length (cm)' and 'petal width (cm)' but replacing them with 'petal area (cm^2)'.\n",
        "\n",
        "Then update the target column so that all the target values of '2' are set to be '1' (the the target values can now only be '0' or '1' - so now this is a binary classification problem). Depending on your data source, your targets may be a string instead in this case you will need to convert your target variables to be numeric before anything else."
      ],
      "metadata": {
        "id": "yHj2UxgxARt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "# Your code Here\n",
        "##############################\n",
        "X_pd, y_pd = sklearn.datasets.load_iris(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Combine the X and y for visualisation\n",
        "XY_pd = pd.concat([X_pd, y_pd], axis=1)\n",
        "\n",
        "\n",
        "XY_pd['petal area (cm^2)'] = XY_pd['petal length (cm)']*XY_pd['petal width (cm)']\n",
        "\n",
        "XY_pd.drop(['petal length (cm)', 'petal width (cm)'], axis = 1, inplace=True)\n",
        "\n",
        "XY_pd['target']=np.where(XY_pd[\"target\"] > 0.0, 1.0, 0.0) # There are other ways you could do this.\n",
        "display(XY_pd)"
      ],
      "metadata": {
        "id": "mpVC48CJAyFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Data Preprocessing and EDA"
      ],
      "metadata": {
        "id": "3KMkksWcoBzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this module we are generally going to be assuming that our data is clean (no errors/duplicates), well balanced and essentially ready to use with our models as soon as it is loaded (with the exception of normalisation/standardization and data splitting that we will cover in a later week), just to speed things up and let us focus on the actual ML models.\n",
        "\n",
        "However, when dealing with real data (either for your MSc project or if you end up working with real data in Industry) you certainly can't assume this will be the case. As bad data will lead to bad models, you should get into the habit of always performing an Exploratory Data Analysis (EDA) to understand your data and using preprocessing to deal with any obvious data issues.\n",
        "\n",
        "These are topics that should be covered in a lot more detail on other modules (especially if you are a Data Science student), so there aren't any exercises in this section. This is just meant to be a short summary to make sure you know to always keep the quality of your data in mind if you are ever fitting ML models to your own data outside of the Module."
      ],
      "metadata": {
        "id": "e3EyhERxnZuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reload the .csv version of the Housing data that I have provided on moodle - this edited version has a few errors in it I want us to find."
      ],
      "metadata": {
        "id": "7aveAYDdHbRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()    # Will prompt you to select file, uncomment this if the data is no longer loaded into the filesystem for you.\n",
        "housingData_csv = pd.read_csv('caliHousing_exercise.csv')\n",
        "\n",
        "# You can find out various attributes of the dataset before even looking at it with the following commands\n",
        "\n",
        "print('Shape of the data (rows and columns):')\n",
        "print(housingData_csv.shape)\n",
        "print()\n",
        "print('List of the column names:')\n",
        "print(housingData_csv.columns)\n",
        "print()\n",
        "print('The data type of all the columns (all just floats here):')\n",
        "print(housingData_csv.dtypes)"
      ],
      "metadata": {
        "id": "UCOR6DVKdsU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You then might want to look at the actual data:"
      ],
      "metadata": {
        "id": "7zSTq6LXI6_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We've been using display a lot, which looks nice and does let us explore the data easily by converting it into an interactive table.\n",
        "display(housingData_csv)\n",
        "\n",
        "# However, a lot of the time you just want a quick sanity check that your data looks how you expect, which the head command is good for.\n",
        "print(housingData_csv.head())\n",
        "\n",
        "# Or you may want to take a random sample of your data\n",
        "print(housingData_csv.sample(10))"
      ],
      "metadata": {
        "id": "cwvXPTgAJBI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then want to check some basic statistics about our data, see if there are any obvious outliers."
      ],
      "metadata": {
        "id": "x7-ZI-PQKCOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housingData_csv.describe()"
      ],
      "metadata": {
        "id": "0yYsyprAKLVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at this there seems to be some clear issues. A minimum House value of 3, max population of 100K and the significantly different latitude all jump out as being issues."
      ],
      "metadata": {
        "id": "XKJ1QnTIK626"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You might also want to use data visualisations like this histogram to help you spot potential errors in the data. I think we can be confident that the 100K population value is an error.\n",
        "housingData_csv[\"population\"].hist()"
      ],
      "metadata": {
        "id": "_byp_EQaLr12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whether you want to remove data entries that have errors or replace them with placeholder values (such as the mean value) really depends on the nature of the data and your requirements. For this I'm going to remove the row and see how the histogram now looks."
      ],
      "metadata": {
        "id": "I2VRwdzyNdLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housingData_csv.drop([6], axis = 0, inplace=True)\n",
        "housingData_csv[\"population\"].hist()\n",
        "\n",
        "# An alternative to dropping would be to just not select the problem data when copying it to a new variable like this\n",
        "#housingData_csv=housingData_csv[housingData_csv[\"population\"] != 100000]\n",
        "#housingData_csv[\"population\"].hist()"
      ],
      "metadata": {
        "id": "ZBBmNJWWMDwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also want to check for missing data. If you just want to remove the data then you could just use the *dropna* function which removes all rows with at least one missing value."
      ],
      "metadata": {
        "id": "pGaIcUyNPwNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can check each column for the number of NA values if you are planning to replace them.\n",
        "print('Nulls in Bedroom column:')\n",
        "print(housingData_csv[\"totalBedrooms\"].isnull().sum())\n",
        "\n",
        "# However, for today we are going to keep things easy and just drop every row with missing data.\n",
        "print('Original dataset length:')\n",
        "print(len(housingData_csv))\n",
        "housingData_csv_red = housingData_csv.dropna()\n",
        "print('Dataset length after removing all rows  missing data:')\n",
        "print(len(housingData_csv_red))"
      ],
      "metadata": {
        "id": "vBPSqq9JQPe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also removed duplicated rows:"
      ],
      "metadata": {
        "id": "9uP2Z9GqQrNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original dataset length:')\n",
        "print(len(housingData_csv_red))\n",
        "housingData_csv_red_dup = housingData_csv_red.drop_duplicates()\n",
        "print('Dataset length after removing all rows with duplicates:')\n",
        "print(len(housingData_csv_red_dup))\n"
      ],
      "metadata": {
        "id": "L4QkfGZ1Py_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are also plenty of other ways of interrogating the data. For example, a correlation heatmap lets you see what variables seem to have strong relationships with each other. Here `1` implies a perfect positive relationship, `-1` a perfect negative relationship (so when x increases by `1`, y decreases by `1`). And a correlation score of zero means there is no clear linear relationship between the variables.\n",
        "What you do with this information can vary, but generally you are looking for feature variables that have a strong correlation with our target variable (the median house value in this case)."
      ],
      "metadata": {
        "id": "y9mw2sRBNgNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "correlation_matrix = housingData_csv.corr()\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Create the heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "\n",
        "# Set the title\n",
        "plt.title('Correlation Heatmap')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ww_bpnNhLKX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can also be useful to interrogate these feature pairs further using a scatter plot, which can give you a better idea of the pattern behind the data, and if there are any seeming outliers. Below I have plotted the median house value against the feature with the highest correlation - median income. We can see that while there is a strong linear relation here, there is certainly a lot of unexplained variation."
      ],
      "metadata": {
        "id": "iiE1iey7Qfge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = housingData_csv['medianIncome']\n",
        "y = housingData_csv['medianHouseValue']\n",
        "plt.scatter(X, y,  color='black', label='y') # Observed y values\n",
        "plt.xlabel('MedInc')\n",
        "plt.ylabel('house price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-IUJckXoJ6EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the other hand, you may want to remove variables from the model that have a weak correlation with the target, as they often will not contribute significantly to the overall accuracy. For example, we can see no real pattern between the population and the house value."
      ],
      "metadata": {
        "id": "kXf6WHjjRnXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = housingData_csv['population']\n",
        "y = housingData_csv['medianHouseValue']\n",
        "plt.scatter(X, y,  color='black', label='y') # Observed y values\n",
        "plt.xlabel('population')\n",
        "plt.ylabel('house price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7bVGCzkKRg_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we don't just care about the relationship with the target variable. If variables have very strong correlations (both negative or positive) with another feature variable, you often may only want to include one of them in the model (or use a method such as dimensionality reduction to combine them). Including strongly correlated variables features( called \"multicollinearity\") often will not improve the model, and sometimes can cause stability and generalisation issues where the model accuracy is reduced (among other issues).\n",
        "\n",
        "We can see in the correlation plot that total bedrooms and households have a perfect correlation with each other. And plotting them we can see that they are indeed saying almost exactly the same thing. So ideally we would only choose one of these to use in our models."
      ],
      "metadata": {
        "id": "WdVcEduHRQoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = housingData_csv['totalBedrooms']\n",
        "y = housingData_csv['households']\n",
        "plt.scatter(X, y,  color='black', label='y') # Observed y values\n",
        "plt.xlabel('totalBedrooms')\n",
        "plt.ylabel('households')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VDegXsapMIzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that above I said the correlation score is useful for identifying **linear** relationships, it is still possible for there to be important patterns in the data that are non-linear, and which may record a poor correlation score. See this example below where I have made some synthetic clustered data, this gives a fairly poor correlation score, but looking at the scatter plot there is definitely something going on the pattern of the data other than random noise!"
      ],
      "metadata": {
        "id": "FIqvOf9UORMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a library from sklearn that is useful for making fake clustered data\n",
        "# This can be useful for testing classification and clustering algorithms.\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Create synthetic data using make_blobs\n",
        "X, y = make_blobs(n_samples=3000, centers=6, random_state=666)\n",
        "\n",
        "# Calculate the correlation score (you can replace this with your correlation function)\n",
        "correlation_score = np.corrcoef(X[:, 0], X[:, 1])[0, 1]\n",
        "\n",
        "# Create a scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], edgecolor='k', s=50)\n",
        "\n",
        "# Set plot labels and title\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title(f'Correlation Score: {correlation_score:.2f}')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uyiGTnV8Ox_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final aspect of preparing your data I want to draw your attention to is the issue of data imbalances. For this we are going to load a new dataset for a categorisation problem used for identifying if someone has breast cancer."
      ],
      "metadata": {
        "id": "VY__dTbtXoCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_pd, y_pd = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n",
        "cancer = pd.concat([X_pd, y_pd], axis=1)\n",
        "display(cancer)"
      ],
      "metadata": {
        "id": "1guU9VlzUzfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to find out how many benign (1.0) or malignant (0.0) cases there are:"
      ],
      "metadata": {
        "id": "RC4hhyHnYcwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categoryCounts = cancer['target'].value_counts()\n",
        "print('Category label counts:')\n",
        "print(categoryCounts)"
      ],
      "metadata": {
        "id": "nCu59O44WL_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this output we can see that there is nearly double the amount of benign samples. This may not seem like an issue, but when fitting a model, a class imbalance like this would mean that a model that just says that every sample is benign would still have around a 63% accuracy, and this problem only gets worse as the class imbalance grows. A basic solution to this is to sample the dataset so that the number instances for each category is the same, but that is not always practical and depending on your problem there may be better solutions."
      ],
      "metadata": {
        "id": "26HRBMCyYZLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Loading images"
      ],
      "metadata": {
        "id": "M70T_46pT0Eo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will return to this later in the module when we come to computer vision using Convolutional Neural Networks (CNNs). But I just wanted to quickly familiarise yourself with the fact that images are just arrays of data as far as python is concerned.\n",
        "However, they are a bit harder to use than tabular data, as they often take up much more computer storage space and are seeing as each image case is at least a 2D array (often more!) they can be more challenging to work with than a nice table that you can load into pandas.\n",
        "\n",
        "Don't worry too much about the code at the moment, but below we are importing an image dataset direct from sklearn, then plotting a sample of the face images:"
      ],
      "metadata": {
        "id": "tDhPt45wT4xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "\n",
        "# Load the Olivetti Faces dataset\n",
        "data = fetch_olivetti_faces(shuffle=True, random_state=42)\n",
        "images = data.images\n",
        "target = data.target\n",
        "\n",
        "# Plot some sample images from the dataset\n",
        "n_samples, n_features = data.data.shape\n",
        "n_samples_h, n_samples_v = 5, 2\n",
        "image_shape = (64, 64)\n",
        "\n",
        "# Create a figure to visualize the images\n",
        "plt.figure(figsize=(2. * n_samples_h, 2.26 * n_samples_v))\n",
        "plt.suptitle(\"Olivetti Faces\")\n",
        "\n",
        "for i in range(n_samples_h * n_samples_v):\n",
        "    plt.subplot(n_samples_v, n_samples_h, i + 1)\n",
        "    plt.imshow(images[i], cmap=plt.cm.gray)\n",
        "    plt.title(f'Subject {target[i]}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zPSbuZOr06e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, these all just look like images right? But what are these actually being stored as in python?"
      ],
      "metadata": {
        "id": "DEsXfyFiVGDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Each of these images is a 64x64 array!\n",
        "images[10].shape"
      ],
      "metadata": {
        "id": "STLPpG9iVRt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And what does this actual array look like?"
      ],
      "metadata": {
        "id": "-OociN12Va72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images[10]"
      ],
      "metadata": {
        "id": "bS4YHlTTVf9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we can see this is just filled with numerical values. In this case they are the 'greyscale' values, but python can also easily deal with colour images:"
      ],
      "metadata": {
        "id": "xIEyGxSnVj3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't worry too much about these libraries, but they are required to pull and convert an image from a url into an array.\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Just taking a random image of some kittens\n",
        "image_url = 'https://shorturl.at/A4fVP'\n",
        "# Load and convert the image from the URL to a NumPy array\n",
        "response = requests.get(image_url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "img_array = np.array(img)\n",
        "plt.imshow(img_array)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "2jM_4yywVyIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, what shape does this data have now?"
      ],
      "metadata": {
        "id": "dCdNmYngWLdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_array.shape"
      ],
      "metadata": {
        "id": "3osEvhP4WQCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see this array now has three values for each point on the image. Anyone who has worked with images on a computer before might have cottoned on to the fact these are the RGB (Red Green Blue) values, which when combined can make any colour. Below I've separated the RGB channels into separate images."
      ],
      "metadata": {
        "id": "s1mEvJWhWTEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create subplots for RGB channels\n",
        "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "# Extract RGB channels\n",
        "red_channel = img_array[:, :, 0]\n",
        "green_channel = img_array[:, :, 1]\n",
        "blue_channel = img_array[:, :, 2]\n",
        "\n",
        "# Plot each channel\n",
        "axs[0].imshow(red_channel, cmap='Reds_r')\n",
        "axs[0].set_title('Red Channel')\n",
        "axs[0].axis('off')\n",
        "\n",
        "axs[1].imshow(green_channel, cmap='Greens_r')\n",
        "axs[1].set_title('Green Channel')\n",
        "axs[1].axis('off')\n",
        "\n",
        "axs[2].imshow(blue_channel, cmap='Blues_r')\n",
        "axs[2].set_title('Blue Channel')\n",
        "axs[2].axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sar3XvgQWnhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**\n",
        "Try finding an image from the internet and load it into python this way - just to prove you can do it with any url linking to an image!"
      ],
      "metadata": {
        "id": "-l5_VGC-YTbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "# Your code Here\n",
        "##############################\n",
        "# Don't worry too much about these libraries, but they are required to pull and convert an image from a url into an array.\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Just taking a random image of some kittens\n",
        "image_url = 'https://img.ifunny.co/images/a8f59420b6bf2fd19e6ea0c65317f8a99c05b7ff5bee4f944b405173ae16c442_1.webp'\n",
        "# Load and convert the image from the URL to a NumPy array\n",
        "response = requests.get(image_url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "img_array = np.array(img)\n",
        "plt.imshow(img_array)\n",
        "plt.axis('off')\n",
        "# Create subplots for RGB channels\n",
        "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "# Extract RGB channels\n",
        "red_channel = img_array[:, :, 0]\n",
        "green_channel = img_array[:, :, 1]\n",
        "blue_channel = img_array[:, :, 2]\n",
        "\n",
        "# Plot each channel\n",
        "axs[0].imshow(red_channel, cmap='Reds_r')\n",
        "axs[0].set_title('Red Channel')\n",
        "axs[0].axis('off')\n",
        "\n",
        "axs[1].imshow(green_channel, cmap='Greens_r')\n",
        "axs[1].set_title('Green Channel')\n",
        "axs[1].axis('off')\n",
        "\n",
        "axs[2].imshow(blue_channel, cmap='Blues_r')\n",
        "axs[2].set_title('Blue Channel')\n",
        "axs[2].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P5R-sjFQYgT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Fitting your first Machine Learning Model"
      ],
      "metadata": {
        "id": "y1AXbbr2A9Kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FInally, I just want to show you how easy it is to fit a Machine Learning model using things like sklearn. Again, don't worry too much about the code as we'll be going through all of this in more detail across the term. Very loosly you can split the process into three steps: Data loading/processing; fitting the ML model; and visualising the results. Admittedly Knn is a very simple model, but in this case the actual 'fitting the model' part is the shortest part of the code!\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "Try changing the value of k (the number of neighbouring points the model considers when classifying) and see how that changes the results. If you are feeling ambitious try changing the dataset to the Breast cancer dataset (`load_breast_cancer()`) I used above to show class imbalance, and fit the Knn classifier to this dataset"
      ],
      "metadata": {
        "id": "qdDRP9d6ZoOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are importing a K Nearest Neighbors clasifiation model from skearn.\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "X_pd, y_pd = sklearn.datasets.load_iris(return_X_y=True, as_frame=True)\n",
        "\n",
        "X = np.array(X_pd[['sepal length (cm)', 'sepal width (cm)']])  # Take only the first two features for visualization\n",
        "y = np.array(y_pd)\n",
        "\n",
        "# Create a KNN classifier\n",
        "k = 5  # You can choose the number of neighbors (k) here\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(X, y)\n",
        "\n",
        "# Create a meshgrid to plot the decision boundary\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
        "\n",
        "# Predict the class labels for the points in the meshgrid\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Create a scatter plot of the data points\n",
        "plt.xlabel('Sepal Length')\n",
        "plt.ylabel('Sepal Width')\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.contourf(xx, yy, Z, alpha=0.35)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='black')\n",
        "\n",
        "# Show the plot\n",
        "plt.title(f'KNN Classifier (k={k}) on Iris Dataset')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FaeDveUcCDKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are importing a K Nearest Neighbors clasifiation model from skearn.\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "X_pd, y_pd = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n",
        "\n",
        "X = np.array(X_pd[['mean radius', 'mean texture']])  # Take only the first two features for visualization\n",
        "y = np.array(y_pd)\n",
        "\n",
        "# Create a KNN classifier\n",
        "k = 20  # You can choose the number of neighbors (k) here\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(X, y)\n",
        "\n",
        "# Create a meshgrid to plot the decision boundary\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.5), np.arange(y_min, y_max, 0.5))\n",
        "\n",
        "# Predict the class labels for the points in the meshgrid\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Create a scatter plot of the data points\n",
        "plt.xlabel('mean radius')\n",
        "plt.ylabel('mean texture')\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.contourf(xx, yy, Z, alpha=0.35)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='black')\n",
        "\n",
        "# Show the plot\n",
        "plt.title(f'KNN Classifier (k={k}) on Breast Cancer Dataset')\n",
        "plt.show()\n",
        "\n",
        "# We can see that this is a BINARY classification problem with only two classes!\n",
        "# While the Iris case had three classes to predict\n"
      ],
      "metadata": {
        "id": "qLPwdwQCjbwi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}